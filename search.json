[{"path":[]},{"path":"/articles/SMC-Mallows.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Sequential Inference with the Mallows Model","text":"provide summary Bayesian Mallows model proposed Sequential Monte Carlo framework updates parameter estimates posterior time receive new observations fixed computational cost. information Bayesian Mallows model can found Vitelli et al. (2018) Liu et al. (2019), vignette BayesMallows R package can found Sørensen et al. (2020). general discussion SMC can found Del Moral, Doucet, Jasra (2006) Doucet Johansen (2009).","code":""},{"path":"/articles/SMC-Mallows.html","id":"notation","dir":"Articles","previous_headings":"Introduction","what":"Notation","title":"Sequential Inference with the Mallows Model","text":"set \\(m\\) distinct items, labelled \\(\\mathcal{} = \\{A_1, \\dots, A_m \\}\\), asked rank items order personal preference respect attribute. information can expressed ranking \\(\\boldsymbol{R} = \\{ R_1 , \\dots , R_m\\}\\), mapping \\(\\boldsymbol{R}:\\mathcal{} \\rightarrow \\mathcal{P}_m\\), \\(\\mathcal{P}_m\\) space \\(m\\)-dimensional permutations \\(\\{1, \\dots, m\\}\\). \\(R_i \\\\{1,\\dots,m\\}\\) corresponds rank item \\(A_i\\). fundamentally assume preference information receive group individuals transitive, .e., individual contradict specifying preferences. words, three distinct items \\(\\{A_i,A_j,A_k\\}\\) set, \\(A_i \\prec A_j\\) \\(A_j \\prec A_k\\), must follow \\(A_i \\prec A_k\\). Sometimes, unable provide full rankings, instead provide ranking subset items \\(\\mathcal{}\\). referred partial rankings. Partial rankings can occur either randomly individual can specify top-\\(k\\) ranked items. scenario, need perform data augmentation order estimate parameters Mallows model.","code":""},{"path":"/articles/SMC-Mallows.html","id":"the-bayesian-mallows-model","dir":"Articles","previous_headings":"Introduction","what":"The Bayesian Mallows Model","title":"Sequential Inference with the Mallows Model","text":"Mallows model (Mallows 1957) probability distribution ranking data. probability observing ranking \\(\\boldsymbol{R}\\) defined \\[p(\\boldsymbol{R}) = p(\\boldsymbol{R}|\\boldsymbol{\\rho},\\alpha) =\\frac{1}{Z_m(\\alpha)} \\exp \\lefts\\{ -\\frac{\\alpha}{m} { d(\\boldsymbol{R}, \\boldsymbol{\\rho})} \\right\\},\\] : \\(\\boldsymbol{\\rho} \\\\mathcal{P}_m\\) consensus ranking; \\(\\alpha > 0\\) scale parameter represents variability rankings within group individuals around consensus ranking; \\(Z_m(\\alpha)\\) normalisation constant. distance function, \\(d(\\cdot,\\cdot) : \\mathcal{P}_m \\times \\mathcal{P}_m \\rightarrow [0,\\infty)\\), measures ‘’closeness’’ ranking consensus ranking. Mallows literature discusses use right-invariant distance function, means distance two items unaffected relabelling items (Diaconis 1988). distance metrics BayesMallows R package currently uses : footrule, Spearman, Cayley, Kendall Hamming. also means normalisation constant independent consensus ranking. Vitelli et al. (2018) extended Mallows model incorporate Bayesian framework inference. uniform prior elicited consensus ranking \\(\\pi(\\boldsymbol{\\rho}) = (m!)^{-1} 1_{\\mathcal{P}_m} (\\boldsymbol{\\rho})\\) space \\(\\mathcal{P}_m\\), exponential prior \\(\\alpha\\), density \\(\\pi(\\alpha|\\lambda) = \\lambda \\exp \\{ -\\lambda \\alpha \\} 1_{[0,\\infty)}(\\alpha)\\). Given \\(M\\) observed complete rankings prior distributions \\(\\pi(\\boldsymbol{\\rho})\\) \\(\\pi(\\alpha)\\), assuming prior independence variables, following posterior density, known Bayesian Mallows model, \\[ \\pi(\\boldsymbol\\rho, \\alpha | \\boldsymbol{R}_1, \\dots, \\boldsymbol{R}_M) \\propto \\frac{\\pi(\\boldsymbol\\rho)\\pi(\\alpha) }{[Z(\\alpha)]^M}   \\exp \\left\\{ - \\frac{\\alpha}{m} \\sum_{j=1}^{M} d(\\boldsymbol{R}_j, \\boldsymbol\\rho   )   \\right\\}.\\] posterior estimates interest, marginal posterior \\(\\boldsymbol{\\rho}\\), obtained use Metropolis-Hastings based Markov Chain Monte Carlo (MCMC) algorithm. Full details algorithm can found Vitelli et al. (2018). iteration algorithm, new consensus ranking \\(\\boldsymbol{\\rho}'\\) proposed update \\(\\boldsymbol{\\rho}\\) according distribution centered around current rank \\(\\boldsymbol{\\rho}\\). proposal step \\(\\boldsymbol{\\rho}\\) done using leap--shift proposal algorithm Vitelli et al. (2018) new value \\(\\alpha'\\) sampled log-normal distribution update current value \\(\\alpha\\). Inference Bayesian Mallows model can sample posterior distribution unknown consensus ranking scale parameter using variety observed data including: full rankings, incomplete rankings (e.g. top-\\(k\\) rankings ranks missing random), implicit data pairwise comparisons. example, case partial rankings, can create augmented full ranking \\(\\tilde{R}_1, \\dots, \\tilde{R}_M\\) using independent sampler assessor containing set rankings already chosen. MCMC algorithm alternates sampling new value \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\) given current \\(\\tilde{R}_1, \\dots, \\tilde{R}_M\\) sampling \\(\\tilde{R}_1, \\dots, \\tilde{R}_M\\) given current values \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\). existing methods discussed Vitelli et al. (2018) provided BayesMallows R package (Sørensen et al. 2020).","code":""},{"path":"/articles/SMC-Mallows.html","id":"sequential-monte-carlo","dir":"Articles","previous_headings":"Introduction","what":"Sequential Monte Carlo","title":"Sequential Inference with the Mallows Model","text":"Sequential Monte Carlo (SMC) methods class sampling algorithms used estimate sequence target distributions given stream observations discrete time. target distribution approximated collection random samples, termed particles, time step evolve according importance sampling resampling steps. literature SMC methods vast diverse, interested using SMC alternative MCMC methods (Chopin 2002). nice summary different variants SMC given Del Moral, Doucet, Jasra (2006). SMC, aim approximate sequence target distributions \\(\\pi_t(\\boldsymbol{\\theta})\\) parameters \\(\\boldsymbol{\\theta}\\) wish estimate given set observed data \\(D_t\\) accumulated time \\(t\\). can write target distribution \\(\\pi_t\\) posterior distribution \\[\\pi_t(\\boldsymbol{\\theta})  = \\pi_t(\\boldsymbol{\\theta} | D_t) \\propto \\pi_0(\\boldsymbol{\\theta})p_t(D_t| \\boldsymbol{\\theta}).\\] SMC algorithm begins generating \\(N\\) particles using prior distributions parameter assigning particle equal weight. time step \\(t=1,\\dots,T\\), assume additional \\(p\\) independent observations \\(y_{1:p}\\) become available target distribution. reweight particles \\(\\pi(\\boldsymbol{\\theta}| D_{t-1})\\) time \\(t-1\\) \\(t\\) weighted respect \\(\\pi(\\boldsymbol{\\theta}| D_{t})\\), \\[ w^{()}_t = \\frac{\\pi(\\boldsymbol{\\theta}^{()}_{t-1}  | D_{t})}{\\pi(\\boldsymbol{\\theta}^{()}_{t-1} | D_{t-1})} \\propto \\frac{p(D_t | \\boldsymbol{\\theta}^{()}_{t-1})}{p(D_{t-1} | \\boldsymbol{\\theta}^{()}_{t-1})} = p( y_{1:p}| D_{t-1}, \\boldsymbol{\\theta}_{t-1}^{()}   ), \\ = 1,\\dots,N.\\] Next, normalise particle weights resample particles replacement replicates heavier weighted particles discard negligible weights. results set equally weighted particles \\(\\{ \\boldsymbol{\\theta}_t^{()}, w_t^{()} = \\frac{1}{N} \\}_{=1}^N\\) represent sample posterior distribution. summary possible methods resampling given Doucet Johansen (2009). final stage, move particles using MCMC kernel within SMC resampling give back diversity particle values (Berzuini Gilks 2001). particular methodology SMC often referred Resample-Move framework Berzuini Gilks (2001) Berzuini Gilks (2003). can apply MCMC kernel many times like since particles still moving within stationary distribution \\(\\pi_t\\).","code":""},{"path":"/articles/SMC-Mallows.html","id":"smc-mallows-user-guide","dir":"Articles","previous_headings":"","what":"SMC-Mallows User Guide","title":"Sequential Inference with the Mallows Model","text":"SMC-Mallows functions contain algorithms perform Resample-Move SMC framework Berzuini Gilks (2001) using single Mallows model. algorithm begins drawing \\(N\\) particles using priors \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\) using specified initial values. particle also assigned equal weight start SMC algorithm \\(\\{\\boldsymbol{\\theta}^{()}_0 = (\\boldsymbol{\\rho}_0^{()}, \\alpha_0^{()}), w^{()} \\}_{=1}^{N}\\) set particles. Next, observe ranking data, \\(D_t\\), calculate updated weights particles respect new observations contribution current estimated posterior distribution reweighting multinomial resampling. Finally, perturb particles using Metropolis-Hastings based MCMC kernel use proposal distributions described Vitelli et al. (2018) sampling values \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\).","code":""},{"path":"/articles/SMC-Mallows.html","id":"complete-rankings","dir":"Articles","previous_headings":"SMC-Mallows User Guide","what":"Complete Rankings","title":"Sequential Inference with the Mallows Model","text":"case, assume observe collection complete rankings new assessors sequence discrete time steps, \\(t=1,\\dots, T\\), time \\(t\\), observed \\(|M_t|\\) complete rankings. particles reweighted representative underlying distribution \\(|M_t|\\) complete rankings. new weights particle calculated \\[\\begin{align*}         {w}^{()}_t(\\boldsymbol{\\theta}^{()}_{t-1}, \\boldsymbol{\\theta}^{()}_{t})             &=  \\frac{  (Z_m(\\alpha^{()}_{t-1}))^{-|M_t|} \\exp \\left\\{ - \\frac{\\alpha^{()}_{t-1}}{m} \\sum_{j=1}^{|M_t|} d(\\mathbf{R}^{()}_j, \\boldsymbol{\\rho}^{()}_{t-1}   )   \\right\\}    }{ (Z_m(\\alpha^{()}_{t-1}))^{-|M_{t-1}|} \\exp \\left\\{ - \\frac{\\alpha^{()}_{t-1}}{m} \\sum_{j=1}^{|M_t|} d(\\mathbf{R}^{()}_j, \\boldsymbol{\\rho}^{()}_{t-1}   )   \\right\\}     }  \\\\             &= (Z_m(\\alpha^{()}_{t-1}))^{-(|M_t|-|M_{t-1}|)}\\exp \\left\\{ - \\frac{\\alpha^{()}_{t-1}}{m} \\sum_{j= |M_{t-1}|+1}^{|M_t|} d(\\mathbf{R}^{()}_j, \\boldsymbol{\\rho}^{()}_{t-1}   )   \\right\\} , \\end{align*}\\] \\(\\alpha^{()}_{t-1}\\) \\(\\boldsymbol{\\rho}^{()}_{t-1}, \\ =1,\\dots,N\\) current estimated parameter values Mallows posterior reweight.","code":""},{"path":"/articles/SMC-Mallows.html","id":"demonstration","dir":"Articles","previous_headings":"SMC-Mallows User Guide > Complete Rankings","what":"Demonstration","title":"Sequential Inference with the Mallows Model","text":"interested updating parameter estimates Bayesian Mallows model based existing ranking data new observations. demonstrate SMC-Mallows functions using sushi_rankings dataset (Kamishima 2003), contains 5000 rankings 10 sushi dishes. function smc_mallows_new_users type = \"complete\" shows simplest version SMC algorithm action. Many input variables, function requires, also used existing functions BayesMallows R package. However, discuss new variables refer algorithm. variable R_obs instance 2D dataset number columns n_items number rows, \\(M\\), represents number individuals dataset. begin algorithm observations artificial time step introduce batch number observations, controlled variable num_new_obs control size bath, Time specify number updates perform. limit value Time, depends number individuals dataset value num_new_obs. move stage can apply MCMC kernel many times like using variable mcmc_kernel_app specify value. example smc_test returns list variable particles values \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\) time step, can observe posterior evolves. Specifically list contains three dimensional matrix size N n_items Time+1, named rho_samples, N Time+1 matrix called alpha_samples. matrices can studied using post-processing functions visualising analysing posterior. also possible make comparative results using MCMC algorithm SMC algorithm. Unlike MCMC observe trace plots need specify non-zero value burn-. indexing R means view Time+1 slice output order view posterior 100 rankings observed. , can observe posterior probabilities \\(\\boldsymbol{\\rho}\\) selection items can observed calling function plot().  posterior distributions \\(\\boldsymbol{\\rho}\\) \\(\\alpha\\) can studied analysis tools provided SMC-Mallows. posterior intervals consensus ranking sushi item obtained calling compute_posterior_intervals. can also rank sushi items according cumulative probability (CP) consensus maximum posterior rankings (MAP). calculated calling function compute_consensus_rho. demonstrate CP consensus, default. Similarly, can observe posterior density posterior intervals scale parameter using functions plot() compute_posterior_intervals.","code":"head(sushi_rankings) ##      shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll ## [1,]      2       8   10     3          4          1   5          9         7 ## [2,]      1       8    6     4         10          9   3          5         7 ## [3,]      2       8    3     4          6          7  10          1         5 ## [4,]      4       7    5     6          1          2   8          3         9 ## [5,]      4      10    7     5          9          3   2          8         1 ## [6,]      4       6    2    10          7          5   1          9         8 ##      cucumber roll ## [1,]             6 ## [2,]             2 ## [3,]             9 ## [4,]            10 ## [5,]             6 ## [6,]             3 n_items <- ncol(sushi_rankings) metric <- \"footrule\"  logz_list <- prepare_partition_function(metric = metric, n_items = n_items)  data <- sushi_rankings[1:100, ] leap_size <- floor(n_items / 5) N <- 1000 Time <- 20 smc_test <- smc_mallows_new_users(   R_obs = data, type = \"complete\", n_items = n_items,   metric = metric, leap_size = leap_size,   N = N, Time = Time,   logz_estimate = logz_list$logz_estimate,   cardinalities = logz_list$cardinalities,   mcmc_kernel_app = 5,   num_new_obs = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6 ) plot(smc_test, colnames = colnames(sushi_rankings), parameter = \"rho\") compute_posterior_intervals(smc_test, parameter = \"rho\") ##       item parameter  mean median conf_level  hpdi central_interval ## 1   Item 1       rho 3.601      4       95 % [2,5]    [2.000,5.000] ## 2  Item 10       rho 9.982     10       95 %  [10]         [10.000] ## 3   Item 2       rho 4.235      4       95 % [2,5]    [2.000,6.000] ## 4   Item 3       rho 2.191      2       95 % [2,3]    [2.000,4.000] ## 5   Item 4       rho 6.548      6       95 % [6,8]    [5.000,8.000] ## 6   Item 5       rho 7.713      8       95 % [6,9]    [6.000,9.000] ## 7   Item 6       rho 4.048      4       95 % [3,5]    [2.000,5.000] ## 8   Item 7       rho 8.936      9       95 % [8,9]    [8.000,9.000] ## 9   Item 8       rho 1.000      1       95 %   [1]          [1.000] ## 10  Item 9       rho 6.746      7       95 % [6,8]    [6.000,8.000] compute_consensus(smc_test) ##    ranking    item cumprob ## 1        1  Item 8   1.000 ## 2        2  Item 3   0.872 ## 3        3  Item 1   0.451 ## 4        4  Item 6   0.601 ## 5        5  Item 2   0.955 ## 6        6  Item 4   0.553 ## 7        7  Item 9   0.937 ## 8        8  Item 5   0.919 ## 9        9  Item 7   0.983 ## 10      10 Item 10   1.000 plot(smc_test, nmc = N, burnin = 0, parameter = \"alpha\") compute_posterior_intervals(smc_test, parameter = \"alpha\") ##   parameter  mean median conf_level          hpdi central_interval ## 1     alpha 1.697  1.692       95 % [1.447,1.966]    [1.446,1.965]"},{"path":[]},{"path":"/articles/SMC-Mallows.html","id":"pseudolikelihood-sampler","dir":"Articles","previous_headings":"SMC-Mallows User Guide > Partial Rankings","what":"Pseudolikelihood Sampler","title":"Sequential Inference with the Mallows Model","text":"Vitelli et al. (2018), augmentation scheme partially observed rankings done using independent sampler conditioned observed component ranking. drawback approach take account existing information, current estimates consensus ranking scale parameter. option use alternative augmentation kernel , item, use univariate Mallows distribution select rank item based item’s rank estimated consensus ranking scale parameter. approach similar importance sampling approximation normalisation constant Bayesian Mallows model, full details can found Vitelli et al. (2018). particular augmentation method applies using footrule Spearman distance metric.","code":""},{"path":"/articles/SMC-Mallows.html","id":"new-assessors-with-partial-rankings","dir":"Articles","previous_headings":"SMC-Mallows User Guide > Partial Rankings","what":"New Assessors with Partial Rankings","title":"Sequential Inference with the Mallows Model","text":"augment missing item ranks \\(\\mathbf{R}\\) create complete auxiliary ranking \\(\\tilde{\\mathbf{R}}\\) order perform remaining steps SMC algorithm. reweight particles representative underlying distribution \\(|M_t|\\) augmented rankings. particle weights recalculated \\[\\begin{align*}         {w}^{()}_t(\\boldsymbol{\\theta}^{()}_{t-1}, \\boldsymbol{\\theta}^{()}_{t})             &= (Z_m(\\alpha^{()}_{t-1}))^{-(|M_t|-|M_{t-1}|)}\\exp \\left\\{ - \\frac{\\alpha^{()}_{t-1}}{m} \\sum_{j= |M_{t-1}|+1}^{|M_t|} d(\\tilde{\\mathbf{R}}^{()}_j, \\boldsymbol{\\rho}^{()}_{t-1}   )   \\right\\} \\\\             & \\times \\prod_{j = |M_{t-1}|+1}^{|M_t|} q(\\tilde{\\mathbf{R}}^{()}_j | \\mathbf{R}_j, \\boldsymbol{\\rho}_{t-1}^{()}, \\alpha_{t-1}^{()} ), \\end{align*}\\] \\(\\alpha^{()}_{t-1}\\) \\(\\boldsymbol{\\rho}^{()}_{t-1}, \\ =1,\\dots,N\\) current estimated parameter values Mallows posterior reweight. distribution \\(q()\\) represents probability creating augmented ranking \\(\\tilde{\\mathbf{R}}\\) given observed ranking \\(\\mathbf{R}\\) current estimated parameters posterior.","code":""},{"path":"/articles/SMC-Mallows.html","id":"demonstration-1","dir":"Articles","previous_headings":"SMC-Mallows User Guide > Partial Rankings","what":"Demonstration","title":"Sequential Inference with the Mallows Model","text":"demonstration shall assume can observe top-5 ranked items user first 100 rows sushi_rankings dataset. can call function smc_mallows_new_users type = \"partial\" run SMC algorithm partial ranking data. variable aug_method allows choose data augmentation method use partial ranking data. option \"pseudolikelihood\" compatible selecting distance metric either \"footrule\" \"spearman\". time call function time provide valid combination augmentation method distance metric. variable smc_test_partial contains list three dimensional matrix size N n_items Time+1, named rho_samples, N Time+1 matrix called alpha_samples. analysis performed previous demonstration can applied scenario. , can observe posterior probabilities selection items \\(\\boldsymbol{\\rho}\\) posterior density \\(\\alpha\\) part demonstration, can use post processing functions analyse output.","code":"data_partial <- sushi_rankings[1:100, ] data_partial[data_partial > 5] <- NA head(data_partial) ##      shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll ## [1,]      2      NA   NA     3          4          1   5         NA        NA ## [2,]      1      NA   NA     4         NA         NA   3          5        NA ## [3,]      2      NA    3     4         NA         NA  NA          1         5 ## [4,]      4      NA    5    NA          1          2  NA          3        NA ## [5,]      4      NA   NA     5         NA          3   2         NA         1 ## [6,]      4      NA    2    NA         NA          5   1         NA        NA ##      cucumber roll ## [1,]            NA ## [2,]             2 ## [3,]            NA ## [4,]            NA ## [5,]            NA ## [6,]             3 aug_method <- \"pseudolikelihood\" metric <- \"cayley\" # example of selecting the incorrect combination of metric and aug_method smc_partial_test <- smc_mallows_new_users(   R_obs = data_partial,   type = \"partial\",   n_items = n_items,   metric = metric,   leap_size = leap_size, N = N,   Time = Time,   logz_estimate = logz_list$logz_estimate,   cardinalities = logz_list$cardinalities,   mcmc_kernel_app = 5,   num_new_obs = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6,   aug_method = aug_method ) aug_method <- \"pseudolikelihood\" metric <- \"footrule\" smc_partial_test <- smc_mallows_new_users(   R_obs = data_partial,   type = \"partial\",   n_items = n_items,   metric = metric,   leap_size = leap_size, N = N,   Time = Time,   logz_estimate = logz_list$logz_estimate,   cardinalities = logz_list$cardinalities,   mcmc_kernel_app = 5,   num_new_obs = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6,   aug_method = aug_method ) plot(smc_partial_test, colnames = colnames(sushi_rankings), parameter = \"rho\") plot(smc_partial_test, nmc = N, burnin = 0, parameter = \"alpha\")"},{"path":"/articles/SMC-Mallows.html","id":"updated-partial-rankings","dir":"Articles","previous_headings":"SMC-Mallows User Guide","what":"Updated partial rankings","title":"Sequential Inference with the Mallows Model","text":"can view scenario observing updated partial ranking known individual, previously latent ranking unranked items becomes known. example, individual may provided partial ranking 6 items \\((1,2,3,\\texttt{NA},\\texttt{NA},\\texttt{NA})\\) SMC algorithm might obtained augmented ranking \\((1,2,3,5,6,4)\\). However, later individual may provided information ranking, e.g., \\((1,2,3,4,\\texttt{NA},\\texttt{NA})\\), ranking longer consistent augmented ranking. causes problem augmented complete ranking SMC, conditioned original observed partial ranking, happens longer consistent new observed partial ranking. means algorithm additional step reweighting stage. order reweight particles correctly representative sample current posterior, view new information arriving system existing individual (augmented ranking) leaving system completely re-entering new partial ranking extra information. individual leaving re-entering system, make two weight adjustments account actions.","code":""},{"path":"/articles/SMC-Mallows.html","id":"demonstration-2","dir":"Articles","previous_headings":"SMC-Mallows User Guide > Updated partial rankings","what":"Demonstration","title":"Sequential Inference with the Mallows Model","text":"illustrate perform SMC updated partial rankings, modify much smaller dataset called potato_visual. dataset, described Liu et al. (2019) provided BayesMallows R package, represents rankings given 12 assessors 20 potatoes based heavy potato appeared given visual appearance. create several partial subsets complete dataset removing lowest ranked item assessor previous subset, achieve several partial datasets view top 10, top 11,… top 19 highest ranked items well complete set rankings. means now 3D array containing 10 partial datasets one complete dataset. third dimension array represents artificial time. can view updated partial rankings scenario viewing several 2D slices observed dataset sequentially. can see, example, 5th time point, observe top-14 items potato_visual. can now run experiment altered potato_visual dataset calling function smc_mallows_new_item_rank. Unlike two previous demonstrations, final slice output occurs Time instead Time+1. initialised algorithm first slice test_dataset rather initialising observed data. observe posterior probabilities items \\(\\boldsymbol{\\rho}\\) posterior density \\(\\alpha\\) using post-processing functions .","code":"example_dataset <- potato_visual n_users <- 12 n_items <- 20 test_dataset <- array(0, c(n_users, n_items, (n_items / 2 + 1))) test_dataset[, , (n_items / 2 + 1)] <- potato_visual tt <- 0 for (ii in (n_items - 1):(n_items / 2)) {   tt <- tt + 1    # set n_users line with one more NA   example_dataset[example_dataset > ii] <- NA    # set as new time stamp   test_dataset[, , ((n_items / 2 + 1) - tt)] <- example_dataset } test_dataset[, , 5] ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ##  [1,]   10   NA   NA   NA    6   NA    4   NA    3     5    12     1     2 ##  [2,]   10   NA   NA   NA   11   NA    6   NA    4     3    13     1     2 ##  [3,]   12   NA   NA   NA   13   11    7   NA    6     3     8     2     1 ##  [4,]    9   NA   NA   NA   10   NA    5   NA    3     4     8     1     2 ##  [5,]   12   NA   NA   NA    7   NA    2   NA    3     9    13     1     4 ##  [6,]   10   NA   NA   NA    8   NA    6   NA    3     7    11     1     2 ##  [7,]    9   NA   NA   NA   10   NA    5   NA    3     8    11     1     2 ##  [8,]   14   NA   NA   NA   11   NA    6   NA    4     3    10     1     2 ##  [9,]    8   NA   NA   NA   12   13    6   NA    5     3     7     1     4 ## [10,]    7   NA   NA   NA    9   NA    5   NA    3    10    11     1     2 ## [11,]   12   NA   NA   NA   13   NA    7   NA    3     5    11     1     2 ## [12,]   14   NA   NA   NA   12   NA    8   NA    3     4     9     1     2 ##       [,14] [,15] [,16] [,17] [,18] [,19] [,20] ##  [1,]     9    NA     8     7    14    13    11 ##  [2,]     7    NA     8     5    12     9    14 ##  [3,]     4    NA     5     9    14    10    NA ##  [4,]     7    NA    11     6    13    14    12 ##  [5,]     5    NA    11     6     8    10    14 ##  [6,]     4    NA     9     5    13    12    14 ##  [7,]     6    NA     7     4    14    12    13 ##  [8,]     7    NA     8     5    12     9    13 ##  [9,]     2    NA    10     9    NA    14    11 ## [10,]     6    NA     8     4    13    12    14 ## [11,]     6    NA    10     4    14     8     9 ## [12,]     7    NA     6     5    13    10    11 logz_list <- prepare_partition_function(metric = metric, n_items = n_items)  Time <- dim(test_dataset)[3] N <- 1000 aug_method <- \"pseudolikelihood\" metric <- \"footrule\" smc_test_updated_partial <- smc_mallows_new_item_rank(   n_items = n_items,   R_obs = test_dataset,   metric = metric,   leap_size = leap_size, N = N,   Time = Time,   logz_estimate = NULL,   cardinalities = logz_list$cardinalities,   mcmc_kernel_app = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6,   aug_method = aug_method ) plot(smc_test_updated_partial, parameter = \"rho\", items = c(4, 6, 7)) plot(smc_test_updated_partial, parameter = \"alpha\")"},{"path":[]},{"path":"/articles/parallel_chains.html","id":"why-parallel-chains","dir":"Articles","previous_headings":"","what":"Why Parallel Chains?","title":"MCMC with Parallel Chains","text":"Modern computers multiple cores, computing clusters one can get access hundreds cores easily. running Markov Chains parallel \\(K\\) cores, ideally different starting points, achieve least following: time wait get required number post-burnin samples scales like \\(1/K\\). can check convergence comparing chains.","code":""},{"path":"/articles/parallel_chains.html","id":"parallel-chains-with-complete-rankings","dir":"Articles","previous_headings":"","what":"Parallel Chains with Complete Rankings","title":"MCMC with Parallel Chains","text":"“BayesMallows” use “parallel” package parallel computation. Parallelization obtained starting cluster providing argument. Since limit parallelism vignettes built CRAN 2, start cluster spanning two cores, real applications number typically larger (output parallel::detectCores() can good guide). Note also give one initial value dispersion parameter \\(\\alpha\\) chain. can assess convergence usual way:  can also assess convergence latent ranks \\(\\boldsymbol{\\rho}\\). Since initial value \\(\\boldsymbol{\\rho}\\) sampled uniformly, two chains automatically get different initial values.  Based convergence plots, set burnin 700. can now use tools assessing posterior distributions usual. post-burnin samples parallel chains simply combined, . plot posterior distribution \\(\\alpha\\).  Next plot posterior distribution \\(\\boldsymbol{\\rho}\\).","code":"library(parallel) cl <- makeCluster(2) fit <- compute_mallows(   rankings = potato_visual, nmc = 5000,   cl = cl ) stopCluster(cl) assess_convergence(fit) assess_convergence(fit, parameter = \"rho\", items = 1:3) fit$burnin <- 700 plot(fit) plot(fit, parameter = \"rho\", items = 4:7)"},{"path":"/articles/parallel_chains.html","id":"parallel-chains-with-pairwise-preferences","dir":"Articles","previous_headings":"","what":"Parallel Chains with Pairwise Preferences","title":"MCMC with Parallel Chains","text":"case parallel chains might strongly needed incomplete data, e.g., arising pairwise preferences. case MCMC algorithm needs perform data augmentation, tends slow sticky. illustrate beach preference data, referring Sørensen et al. (2020) thorough introduction aspects directly related parallelism. start generating transitive closure: Next run two parallel chains, letting package generate random initial rankings, providing vector initial values \\(\\alpha\\).","code":"beach_tc <- generate_transitive_closure(beach_preferences) cl <- makeCluster(2) fit <- compute_mallows(   preferences = beach_tc, nmc = 4000,   alpha_init = runif(2, 1, 4),   save_aug = TRUE, cl = cl ) #> Generating initial ranking. stopCluster(cl)"},{"path":"/articles/parallel_chains.html","id":"trace-plots","dir":"Articles","previous_headings":"Parallel Chains with Pairwise Preferences","what":"Trace Plots","title":"MCMC with Parallel Chains","text":"convergence plots shows long-range autocorrelation, otherwise seems mix relatively well.  convergence plot \\(\\boldsymbol{\\rho}\\):  avoid overplotting, ’s good idea pick low number assessors chains. look items 1-3 assessors 1 2.","code":"assess_convergence(fit) assess_convergence(fit, parameter = \"rho\", items = 4:6) assess_convergence(fit,   parameter = \"Rtilde\",   items = 1:3, assessors = 1:2 )"},{"path":"/articles/parallel_chains.html","id":"posterior-quantities","dir":"Articles","previous_headings":"Parallel Chains with Pairwise Preferences","what":"Posterior Quantities","title":"MCMC with Parallel Chains","text":"Based trace plots, chains seem mixing well. set burnin 700 . can now study posterior distributions. posterior \\(\\alpha\\). Note increasing nmc argument compute_mallows , density appear smoother. vignette kept low reduce run time.  can also look posterior \\(\\boldsymbol{\\rho}\\).  can also compute posterior intervals usual way: can compute consensus ranking: can compute probability top-\\(k\\), \\(k=4\\):","code":"fit$burnin <- 700 plot(fit) plot(fit, parameter = \"rho\", items = 6:9) compute_posterior_intervals(fit, parameter = \"alpha\") #>   parameter  mean median conf_level          hpdi central_interval #> 1     alpha 4.835   4.82       95 % [4.307,5.419]    [4.294,5.414] compute_posterior_intervals(fit, parameter = \"rho\") #>       item parameter mean median conf_level    hpdi central_interval #> 1   Item 1       rho    7      7       95 %     [7]            [6,7] #> 2   Item 2       rho   15     15       95 %    [15]             [15] #> 3   Item 3       rho    3      3       95 %   [3,4]            [3,4] #> 4   Item 4       rho   11     11       95 % [11,13]          [11,13] #> 5   Item 5       rho    9      9       95 %   [8,9]           [8,10] #> 6   Item 6       rho    2      2       95 %   [1,2]            [1,2] #> 7   Item 7       rho    8      8       95 %   [8,9]           [8,10] #> 8   Item 8       rho   12     12       95 % [11,14]          [11,14] #> 9   Item 9       rho    1      1       95 %   [1,2]            [1,2] #> 10 Item 10       rho    6      6       95 %   [5,6]            [5,6] #> 11 Item 11       rho    4      4       95 %   [3,4]            [3,5] #> 12 Item 12       rho   13     13       95 % [12,14]          [12,14] #> 13 Item 13       rho   10     10       95 %  [9,10]           [9,10] #> 14 Item 14       rho   13     13       95 % [11,14]          [11,14] #> 15 Item 15       rho    5      5       95 %   [5,6]            [4,6] compute_consensus(fit) #>    ranking    item   cumprob #> 1        1  Item 9 0.8809091 #> 2        2  Item 6 1.0000000 #> 3        3  Item 3 0.7056061 #> 4        4 Item 11 0.9718182 #> 5        5 Item 15 0.9198485 #> 6        6 Item 10 0.9760606 #> 7        7  Item 1 1.0000000 #> 8        8  Item 7 0.7218182 #> 9        9  Item 5 0.9595455 #> 10      10 Item 13 1.0000000 #> 11      11  Item 4 0.6493939 #> 12      12  Item 8 0.7337879 #> 13      13 Item 14 0.5439394 #> 14      14 Item 12 1.0000000 #> 15      15  Item 2 1.0000000 compute_consensus(fit, type = \"MAP\") #>    map_ranking    item probability #> 1            1  Item 9   0.2006061 #> 2            2  Item 6   0.2006061 #> 3            3  Item 3   0.2006061 #> 4            4 Item 11   0.2006061 #> 5            5 Item 15   0.2006061 #> 6            6 Item 10   0.2006061 #> 7            7  Item 1   0.2006061 #> 8            8  Item 7   0.2006061 #> 9            9  Item 5   0.2006061 #> 10          10 Item 13   0.2006061 #> 11          11  Item 4   0.2006061 #> 12          12  Item 8   0.2006061 #> 13          13 Item 12   0.2006061 #> 14          14 Item 14   0.2006061 #> 15          15  Item 2   0.2006061 plot_top_k(fit, k = 4)"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Oystein Sorensen. Author, maintainer. Waldir Leoncio. Contractor. Valeria Vitelli. Author. Marta Crispino. Author. Qinghua Liu. Author. Cristina Mollica. Author. Luca Tardella. Author. Anja Stein. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). “BayesMallows: R Package Bayesian Mallows Model.” R Journal, 12(1), 324–342. doi:10.32614/RJ-2020-026.","code":"@Article{,   author = {{\\O}ystein S{\\o}rensen and Marta Crispino and Qinghua Liu and Valeria Vitelli},   doi = {10.32614/RJ-2020-026},   title = {BayesMallows: An R Package for the Bayesian Mallows Model},   journal = {The R Journal},   number = {1},   pages = {324--342},   volume = {12},   year = {2020}, }"},{"path":"/index.html","id":"bayesmallows","dir":"","previous_headings":"","what":"Bayesian Preference Learning with the Mallows Rank Model","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"package provides general framework analyzing rank preference data based Bayesian Mallows model first described Vitelli et al. (2018).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"install current release, use install current development version, use","code":"install.packages(\"BayesMallows\") # install.packages(\"remotes\") remotes::install_github(\"ocbe-uio/BayesMallows\")"},{"path":"/index.html","id":"basic-usage-example","dir":"","previous_headings":"","what":"Basic Usage Example","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"get started, load package package comes several example datasets. simplest one contains 12 persons’ assessments weights 20 potatoes, either visual inspection (potato_visual) lifting potatoes comparing relative weights hand (potato_weighing). fit Bayesian Mallows model potato_visual dataset, Next, can see diagnostic plot Metropolis-Hastings algorithm assess_convergence(). plot scale parameter, measures variation individual rankings.  Setting burnin 500, obtain plot posterior distribution scale parameter :  examples, please R Journal paper, function documentation. use parallel chains described vignette.","code":"library(BayesMallows) fit <- compute_mallows(potato_visual) assess_convergence(fit) plot(fit, burnin = 500)"},{"path":[]},{"path":"/index.html","id":"methodology","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Methodology","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"BayesMallows package currently implements complete model described Vitelli et al. (2018), includes large number distance metrics, handling missing ranks pairwise comparisons, clustering users similar preferences. extension non-transitive pairwise comparisons Crispino et al. (2019) also implemented. addition, partition function Mallows model can estimated using importance sampling algorithm Vitelli et al. (2018) asymptotic approximation Mukherjee (2016). review ranking models general, see Liu, Crispino, et al. (2019). Crispino Antoniano-Villalobos (2022) outlines informative priors can used within model. Updating posterior distribution based new data, using sequential Monte Carlo methods, implemented described separate vignette. computational algorithms described detail Stein (2023).","code":""},{"path":"/index.html","id":"applications","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Applications","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"Among current applications, Liu, Reiner, et al. (2019) applied Bayesian Mallows model providing personalized recommendations based clicking data, Barrett Crispino (2018) used model Crispino et al. (2019) analyze listeners’ understanding music. Eliseussen, Fleischer, Vitelli (2022) presented extended model variable selection genome-wide transcriptomic analyses.","code":""},{"path":"/index.html","id":"future-extensions","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Future Extensions","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"Plans future extensions package include implementation variational Bayes algorithm approximation posterior distribution. sequential Monte Carlo algorithms also extended cover larger part model framework, add options specifications prior distributions.","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"using BayesMallows package academic work, please cite Sørensen et al. (2020), addition relevant methodological papers.","code":"citation(\"BayesMallows\") #> To cite package 'BayesMallows' in publications use: #>  #>   Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). \"BayesMallows: An R #>   Package for the Bayesian Mallows Model.\" _The R Journal_, *12*(1), #>   324-342. doi:10.32614/RJ-2020-026 #>   <https://doi.org/10.32614/RJ-2020-026>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Article{, #>     author = {{\\O}ystein S{\\o}rensen and Marta Crispino and Qinghua Liu and Valeria Vitelli}, #>     doi = {10.32614/RJ-2020-026}, #>     title = {BayesMallows: An R Package for the Bayesian Mallows Model}, #>     journal = {The R Journal}, #>     number = {1}, #>     pages = {324--342}, #>     volume = {12}, #>     year = {2020}, #>   }"},{"path":"/index.html","id":"contribution","dir":"","previous_headings":"","what":"Contribution","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"open source project, contributions welcome. Feel free open Issue, Pull Request, e-mail us.","code":""},{"path":[]},{"path":"/reference/BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model. — BayesMallows","title":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model. — BayesMallows","text":"BayesMallows package provides functionality fully   Bayesian analysis preference rank data. package implements   Bayesian Mallows model described   Vitelli et al. (2018) , handles complete   rankings, top-k rankings, ranks missing random, consistent pairwise   preference data, well mixtures rank models. Modeling pairwise   preferences containing inconsistencies, described   Crispino et al. (2019) , also supported. See   also Sørensen et al. (2020)  overview   methods tutorial. documentation examples following functions likely   useful get started: analysis rank preference data, see compute_mallows. computation multiple models varying numbers mixture components,  see compute_mallows_mixtures. estimation partition function (normalizing constant) using either  importance sampling algorithm Vitelli et al. (2018)   asymptotic algorithm Mukherjee (2016) , see  estimate_partition_function. sequential Monte Carlo algorithms developed  Stein (2023) , see  smc_mallows_new_users smc_mallows_new_item_rank.","code":""},{"path":"/reference/BayesMallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model. — BayesMallows","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 . Mukherjee S (2016). “Estimation exponential families permutations.” Annals Statistics, 44(2), 853--875. doi:10.1214/15-aos1389 . Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). “BayesMallows: R Package Bayesian Mallows Model.” R Journal, 12(1), 324--342. doi:10.32614/RJ-2020-026 . Stein (2023). Sequential Inference Mallows Model. Ph.D. thesis, Lancaster University. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":"/reference/assess_convergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"assess_convergence provides trace plots parameters Mallows Rank model, order study convergence Metropolis-Hastings algorithm.","code":""},{"path":"/reference/assess_convergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"","code":"assess_convergence(   model_fit,   parameter = \"alpha\",   items = NULL,   assessors = NULL )"},{"path":"/reference/assess_convergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"model_fit fitted model object class BayesMallows returned compute_mallows object class BayesMallowsMixtures returned compute_mallows_mixtures. parameter Character string specifying parameter plot. Available options \"alpha\", \"rho\", \"Rtilde\", \"cluster_probs\", \"theta\". items items study diagnostic plot rho. Either vector item names, corresponding model_fit$items vector indices. NULL, five items selected randomly. used parameter = \"rho\" parameter = \"Rtilde\". assessors Numeric vector specifying assessors study diagnostic plot \"Rtilde\".","code":""},{"path":[]},{"path":"/reference/assign_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Assessors to Clusters — assign_cluster","title":"Assign Assessors to Clusters — assign_cluster","text":"Assign assessors clusters finding cluster highest posterior probability.","code":""},{"path":"/reference/assign_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Assessors to Clusters — assign_cluster","text":"","code":"assign_cluster(   model_fit,   burnin = model_fit$burnin,   soft = TRUE,   expand = FALSE )"},{"path":"/reference/assign_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Assessors to Clusters — assign_cluster","text":"model_fit object type BayesMallows, returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. soft logical specifying whether perform soft hard clustering. soft=TRUE, cluster probabilities returned, whereas soft=FALSE, maximum posterior (MAP) cluster probability returned, per assessor. case tie two cluster assignments, random cluster taken MAP estimate. expand logical specifying whether expand rowset assessor also include clusters assessor 0 posterior assignment probability. used soft = TRUE. Defaults FALSE.","code":""},{"path":"/reference/assign_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Assessors to Clusters — assign_cluster","text":"dataframe. soft = FALSE, one row per assessor, columns assessor, probability map_cluster. soft = TRUE, n_cluster rows per assessor, additional column cluster.","code":""},{"path":[]},{"path":"/reference/asymptotic_partition_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"Compute asymptotic approximation logarithm partition function, using iteration algorithm Mukherjee (2016) .","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"","code":"asymptotic_partition_function(   alpha_vector,   n_items,   metric,   K,   n_iterations = 1000L,   tol = 1e-09 )"},{"path":"/reference/asymptotic_partition_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"alpha_vector numeric vector alpha values. n_items Integer specifying number items. metric One \"footrule\" \"spearman\". K Integer. n_iterations Integer specifying number iterations. tol Stopping criterion algorithm. previous matrix subtracted updated, maximum absolute relative difference tol, iteration stops.","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"vector, containing partition function value alpha.","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"Mukherjee S (2016). “Estimation exponential families permutations.” Annals Statistics, 44(2), 853--875. doi:10.1214/15-aos1389 .","code":""},{"path":"/reference/beach_preferences.html","id":null,"dir":"Reference","previous_headings":"","what":"Beach Preferences — beach_preferences","title":"Beach Preferences — beach_preferences","text":"Example dataset (Vitelli et al. 2018) , Section 6.2.","code":""},{"path":"/reference/beach_preferences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Beach Preferences — beach_preferences","text":"","code":"beach_preferences"},{"path":"/reference/beach_preferences.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Beach Preferences — beach_preferences","text":"object class data.frame 1442 rows 3 columns.","code":""},{"path":"/reference/beach_preferences.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Beach Preferences — beach_preferences","text":"Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/calculate_backward_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Backward Probability — calculate_backward_probability","title":"Calculate Backward Probability — calculate_backward_probability","text":"Function calculate probability assigning set specific ranks specific item given rank consensus ranking","code":""},{"path":"/reference/calculate_backward_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Backward Probability — calculate_backward_probability","text":"","code":"calculate_backward_probability(   item_ordering,   partial_ranking,   current_ranking,   remaining_set,   rho,   alpha,   n_items,   metric = \"footrule\" )"},{"path":"/reference/calculate_backward_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Backward Probability — calculate_backward_probability","text":"item_ordering vector integer values represent specified queue unranked item assign rank proposed augmented ranking partial_ranking incomplete rank sequence vector original observed incomplete ranking contains NAs current_ranking complete rank sequence vector  proposed augmented ranking obtained calculate_forward_probability function remaining_set vector integer values represent elements (ranks) missing original observed ranking rho Numeric vector specifying consensus ranking alpha Numeric value scale parameter n_items Integer number items ranking metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\".","code":""},{"path":"/reference/calculate_backward_probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Backward Probability — calculate_backward_probability","text":"backward_auxiliary_ranking_probability numerical value creating previous augmented ranking using item ordering used create new augmented ranking calculate_forward_probability function.","code":""},{"path":"/reference/calculate_forward_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Forward Probability — calculate_forward_probability","title":"Calculate Forward Probability — calculate_forward_probability","text":"Function calculate probability assigning set   specific ranks specific item given rank consensus ranking","code":""},{"path":"/reference/calculate_forward_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Forward Probability — calculate_forward_probability","text":"","code":"calculate_forward_probability(   item_ordering,   partial_ranking,   remaining_set,   rho,   alpha,   n_items,   metric = \"footrule\" )"},{"path":"/reference/calculate_forward_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Forward Probability — calculate_forward_probability","text":"item_ordering vector integer values represent specified queue unranked item assign rank proposed augmented ranking partial_ranking incomplete rank sequence vector original observed incomplete ranking contains NAs remaining_set vector integer values represent elements (ranks) missing original observed ranking rho Numeric vector specifying consensus ranking alpha Numeric value scale parameter n_items Integer number items ranking metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\".","code":""},{"path":"/reference/calculate_forward_probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Forward Probability — calculate_forward_probability","text":"List containing aug_ranking, ranking sequence vector   proposed augmented ranking forward_prob numerical value   probability creating augmented ranking using pseudolikelihood   augmentation.","code":""},{"path":"/reference/compute_consensus.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Consensus Ranking — compute_consensus.BayesMallows","title":"Compute Consensus Ranking — compute_consensus.BayesMallows","text":"Compute Consensus Ranking","code":""},{"path":"/reference/compute_consensus.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Consensus Ranking — compute_consensus.BayesMallows","text":"","code":"# S3 method for BayesMallows compute_consensus(   model_fit,   type = \"CP\",   burnin = model_fit$burnin,   parameter = \"rho\",   assessors = 1L,   ... )"},{"path":"/reference/compute_consensus.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Consensus Ranking — compute_consensus.BayesMallows","text":"model_fit Object type BayesMallows returned compute_mallows. type Character string specifying consensus compute. Either \"CP\" \"MAP\". Defaults \"CP\". burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. parameter Character string defining parameter compute consensus. Defaults \"rho\". Available options \"rho\" \"Rtilde\", latter giving consensus rankings augmented ranks. assessors parameter = \"rho\", integer vector used define assessors compute augmented ranking. ... arguments passed methods. Currently used. Defaults 1L, yields augmented rankings assessor 1.","code":""},{"path":[]},{"path":"/reference/compute_consensus.SMCMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Consensus Ranking — compute_consensus.SMCMallows","title":"Compute Consensus Ranking — compute_consensus.SMCMallows","text":"Compute consensus ranking using either cumulative probability (CP) maximum posteriori (MAP) consensus (Vitelli et al. 2018) . mixture models, consensus given mixture.","code":""},{"path":"/reference/compute_consensus.SMCMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Consensus Ranking — compute_consensus.SMCMallows","text":"","code":"# S3 method for SMCMallows compute_consensus(model_fit, type = \"CP\", ...)"},{"path":"/reference/compute_consensus.SMCMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Consensus Ranking — compute_consensus.SMCMallows","text":"model_fit object class SMCMallows, returned smc_mallows_new_item_rank smc_mallows_new_users. type Character string specifying consensus compute. Either \"CP\" \"MAP\". Defaults \"CP\". ... optional arguments passed methods. Currently used.","code":""},{"path":[]},{"path":"/reference/compute_consensus.SMCMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Consensus Ranking — compute_consensus.SMCMallows","text":"","code":"# Basic elements data <- sushi_rankings[1:100, ] n_items <- ncol(data) leap_size <- floor(n_items / 5) metric <- \"footrule\" Time <- 20 N <- 100  # Prepare exact partition function cardinalities <- prepare_partition_function(metric = metric,                                             n_items = n_items)$cardinalities  # Performing SMC smc_test <- smc_mallows_new_users(   R_obs = data, type = \"complete\", n_items = n_items,   metric = metric, leap_size = leap_size,   N = N, Time = Time,   cardinalities = cardinalities,   mcmc_kernel_app = 5,   num_new_obs = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6 )  compute_posterior_intervals(smc_test, parameter = \"rho\") #>       item parameter mean median conf_level  hpdi central_interval #> 1   Item 1       rho 3.57      4       95 % [2,5]    [2.000,5.000] #> 2  Item 10       rho 9.99     10       95 %  [10]         [10.000] #> 3   Item 2       rho 4.48      5       95 % [2,6]    [2.000,6.000] #> 4   Item 3       rho 2.24      2       95 % [2,4]    [2.000,4.000] #> 5   Item 4       rho 6.28      6       95 % [5,8]    [5.000,8.000] #> 6   Item 5       rho 7.93      8       95 % [7,9]    [6.000,9.000] #> 7   Item 6       rho 3.83      4       95 % [2,5]    [2.000,5.000] #> 8   Item 7       rho 8.91      9       95 % [8,9]    [8.000,9.000] #> 9   Item 8       rho 1.00      1       95 %   [1]          [1.000] #> 10  Item 9       rho 6.77      7       95 % [6,8]    [6.000,8.000]  compute_consensus(model_fit = smc_test, type = \"CP\") #>    ranking    item cumprob #> 1        1  Item 8    1.00 #> 2        2  Item 3    0.84 #> 3        3  Item 1    0.46 #> 4        4  Item 6    0.75 #> 5        5  Item 2    0.92 #> 6        6  Item 4    0.69 #> 7        7  Item 9    0.95 #> 8        8  Item 5    0.90 #> 9        9  Item 7    0.99 #> 10      10 Item 10    1.00 compute_consensus(model_fit = smc_test, type = \"MAP\") #>    probability    item map_ranking #> 1         0.17  Item 8           1 #> 2         0.17  Item 3           2 #> 3         0.17  Item 6           3 #> 4         0.17  Item 1           4 #> 5         0.17  Item 2           5 #> 6         0.17  Item 4           6 #> 7         0.17  Item 9           7 #> 8         0.17  Item 5           8 #> 9         0.17  Item 7           9 #> 10        0.17 Item 10          10  compute_posterior_intervals(smc_test, parameter = \"alpha\") #>   parameter  mean median conf_level          hpdi central_interval #> 1     alpha 1.694  1.688       95 % [1.451,1.892]    [1.482,1.949]"},{"path":"/reference/compute_consensus.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Consensus Ranking — compute_consensus","title":"Compute Consensus Ranking — compute_consensus","text":"Compute consensus ranking using either cumulative probability (CP) maximum posteriori (MAP) consensus (Vitelli et al. 2018) . mixture models, consensus given mixture. Consensus augmented ranks can also computed assessor, setting parameter = \"Rtilde\".","code":""},{"path":"/reference/compute_consensus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Consensus Ranking — compute_consensus","text":"","code":"compute_consensus(model_fit, ...)"},{"path":"/reference/compute_consensus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Consensus Ranking — compute_consensus","text":"model_fit model fit. ... arguments passed methods.","code":""},{"path":"/reference/compute_consensus.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Consensus Ranking — compute_consensus","text":"Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/compute_consensus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Consensus Ranking — compute_consensus","text":"","code":"# The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the Mallows # model: model_fit <- compute_mallows(potato_visual)  # Se the documentation to compute_mallows for how to assess the convergence of the algorithm # Having chosen burin = 1000, we compute posterior intervals model_fit$burnin <- 1000 # We then compute the CP consensus. compute_consensus(model_fit, type = \"CP\") #>    ranking item cumprob #> 1        1  P12   1.000 #> 2        2  P13   1.000 #> 3        3   P9   1.000 #> 4        4  P10   0.619 #> 5        5  P17   0.882 #> 6        6   P7   0.874 #> 7        7  P14   1.000 #> 8        8  P16   1.000 #> 9        9   P1   0.424 #> 10      10   P5   0.781 #> 11      11  P11   1.000 #> 12      12  P19   1.000 #> 13      13  P20   0.508 #> 14      14  P18   1.000 #> 15      15   P6   0.992 #> 16      16   P4   0.745 #> 17      17   P2   0.834 #> 18      18  P15   1.000 #> 19      19   P3   1.000 #> 20      20   P8   1.000 # And we compute the MAP consensus compute_consensus(model_fit, type = \"MAP\") #>    map_ranking item probability #> 1            1  P12       0.072 #> 2            2  P13       0.072 #> 3            3   P9       0.072 #> 4            4  P17       0.072 #> 5            5   P7       0.072 #> 6            6  P14       0.072 #> 7            7  P10       0.072 #> 8            8  P16       0.072 #> 9            9   P5       0.072 #> 10          10   P1       0.072 #> 11          11  P11       0.072 #> 12          12  P19       0.072 #> 13          13  P20       0.072 #> 14          14  P18       0.072 #> 15          15   P6       0.072 #> 16          16   P4       0.072 #> 17          17   P2       0.072 #> 18          18  P15       0.072 #> 19          19   P3       0.072 #> 20          20   P8       0.072  if (FALSE) {   # CLUSTERWISE CONSENSUS   # We can run a mixture of Mallows models, using the n_clusters argument   # We use the sushi example data. See the documentation of compute_mallows for a more elaborate   # example   model_fit <- compute_mallows(sushi_rankings, n_clusters = 5)   # Keeping the burnin at 1000, we can compute the consensus ranking per cluster   model_fit$burnin <- 1000   cp_consensus_df <- compute_consensus(model_fit, type = \"CP\")   # We can now make a table which shows the ranking in each cluster:   cp_consensus_df$cumprob <- NULL   stats::reshape(cp_consensus_df, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\",                  varying = list(sort(unique(cp_consensus_df$cluster)))) }  if (FALSE) {   # MAP CONSENSUS FOR PAIRWISE PREFENCE DATA   # We use the example dataset with beach preferences.   model_fit <- compute_mallows(preferences = beach_preferences)   # We set burnin = 1000   model_fit$burnin <- 1000   # We now compute the MAP consensus   map_consensus_df <- compute_consensus(model_fit, type = \"MAP\") }  if (FALSE) {   # CP CONSENSUS FOR AUGMENTED RANKINGS   # We use the example dataset with beach preferences.   model_fit <- compute_mallows(preferences = beach_preferences, save_aug = TRUE,                                aug_thinning = 2, seed = 123L)   # We set burnin = 1000   model_fit$burnin <- 1000   # We now compute the CP consensus of augmented ranks for assessors 1 and 3   cp_consensus_df <- compute_consensus(model_fit, type = \"CP\",                                        parameter = \"Rtilde\", assessors = c(1L, 3L))   # We can also compute the MAP consensus for assessor 2   map_consensus_df <- compute_consensus(model_fit, type = \"MAP\",                                         parameter = \"Rtilde\", assessors = 2L)    # Caution!   # With very sparse data or with too few iterations, there may be ties in the MAP consensus   # This is illustrated below for the case of only 5 post-burnin iterations. Two MAP rankings are   # equally likely in this case (and for this seed).   model_fit <- compute_mallows(preferences = beach_preferences, nmc = 1005,                                save_aug = TRUE, aug_thinning = 1, seed = 123L)   model_fit$burnin <- 1000   compute_consensus(model_fit, type = \"MAP\", parameter = \"Rtilde\", assessors = 2L) }"},{"path":"/reference/compute_importance_sampling_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute importance sampling estimates of log partition function\nfor footrule and Spearman distances. — compute_importance_sampling_estimate","title":"Compute importance sampling estimates of log partition function\nfor footrule and Spearman distances. — compute_importance_sampling_estimate","text":"Compute importance sampling estimates log partition function footrule Spearman distances.","code":""},{"path":"/reference/compute_importance_sampling_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute importance sampling estimates of log partition function\nfor footrule and Spearman distances. — compute_importance_sampling_estimate","text":"","code":"compute_importance_sampling_estimate(   alpha_vector,   n_items,   metric = \"footrule\",   nmc = 10000L )"},{"path":"/reference/compute_importance_sampling_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute importance sampling estimates of log partition function\nfor footrule and Spearman distances. — compute_importance_sampling_estimate","text":"alpha_vector Vector alpha values compute partition function. n_items Integer specifying number ranked items. metric Distance measure target Mallows distribution. Defaults footrule. nmc Number Monte Carlo samples. Defaults 1e4.","code":""},{"path":"/reference/compute_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Preference Learning with the Mallows Rank Model — compute_mallows","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"Compute posterior distributions parameters   Bayesian Mallows Rank Model, given rankings preferences stated set   assessors. BayesMallows package uses following parametrization   Mallows rank model (Mallows 1957) :   $$p(r|\\alpha,\\rho) = (1/Z_{n}(\\alpha)) \\exp{-\\alpha/n d(r,\\rho)}$$   \\(r\\) ranking, \\(\\alpha\\) scale parameter, \\(\\rho\\)   latent consensus ranking, \\(Z_{n}(\\alpha)\\) partition function   (normalizing constant), \\(d(r,\\rho)\\) distance function   measuring distance \\(r\\) \\(\\rho\\). Note   authors use Mallows model without division \\(n\\) exponent;   includes PerMallows package, whose scale parameter   \\(\\theta\\) corresponds \\(\\alpha/n\\) BayesMallows   package. refer (Vitelli et al. 2018)    details Bayesian Mallows model. compute_mallows always returns posterior distributions latent   consensus ranking \\(\\rho\\) scale parameter \\(\\alpha\\). Several   distance measures supported, preferences can take form   complete incomplete rankings, well pairwise preferences.   compute_mallows can also compute mixtures Mallows models,   clustering assessors similar preferences.","code":""},{"path":"/reference/compute_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"","code":"compute_mallows(   rankings = NULL,   preferences = NULL,   obs_freq = NULL,   metric = \"footrule\",   error_model = NULL,   n_clusters = 1L,   clus_thin = 1L,   nmc = 2000L,   leap_size = max(1L, floor(n_items/5)),   swap_leap = 1L,   rho_init = NULL,   rho_thinning = 1L,   alpha_prop_sd = 0.1,   alpha_init = 1,   alpha_jump = 1L,   lambda = 0.001,   alpha_max = 1e+06,   psi = 10L,   include_wcd = (n_clusters > 1),   save_aug = FALSE,   aug_thinning = 1L,   logz_estimate = NULL,   verbose = FALSE,   validate_rankings = TRUE,   na_action = \"augment\",   constraints = NULL,   save_ind_clus = FALSE,   seed = NULL,   cl = NULL )"},{"path":"/reference/compute_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"rankings matrix ranked items, size n_assessors x n_items. See create_ranking ordered set items need converted rankings. preferences provided, rankings optional initial value rankings, generated generate_initial_ranking. rankings column names, assumed names items. NA values rankings treated missing data automatically augmented; change behavior, see na_action argument. preferences dataframe pairwise comparisons, 3 columns, named assessor, bottom_item, top_item, one row stated preference. Given set pairwise preferences, generate transitive closure using generate_transitive_closure. give preferences class \"BayesMallowsTC\". preferences class \"BayesMallowsTC\", compute_mallows call generate_transitive_closure preferences computations done. current version, pairwise preferences assumed mutually compatible. obs_freq vector observation frequencies (weights) apply row rankings. can speed computation large number assessors share rank pattern. Defaults NULL, means row rankings multiplied 1. provided, obs_freq must number elements rows rankings, rankings NULL. See obs_freq information rank_freq_distr convenience function computing . metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". distance given metric also used compute within-cluster distances, include_wcd = TRUE. error_model Character string specifying model use inconsistent rankings. Defaults NULL, means inconsistent rankings allowed. moment, available option \"bernoulli\", means Bernoulli error model used. See Crispino et al. (2019)  definition Bernoulli model. n_clusters Integer specifying number clusters, .e., number mixture components use. Defaults 1L, means clustering performed. See compute_mallows_mixtures convenience function computing several models varying numbers mixtures. clus_thin Integer specifying thinning applied cluster assignments cluster probabilities. Defaults 1L. nmc Integer specifying number iteration Metropolis-Hastings algorithm run. Defaults 2000L. See assess_convergence tools check convergence Markov chain. leap_size Integer specifying step size leap--shift proposal distribution. Defaults floor(n_items / 5). swap_leap Integer specifying step size Swap proposal. used error_model NULL. rho_init Numeric vector specifying initial value latent consensus ranking \\(\\rho\\). Defaults NULL, means initial value set randomly. rho_init provided n_clusters > 1, mixture component \\(\\rho_{c}\\) gets initial value. rho_thinning Integer specifying thinning rho performed Metropolis- Hastings algorithm. Defaults 1L. compute_mallows save every rho_thinningth value \\(\\rho\\). alpha_prop_sd Numeric value specifying standard deviation lognormal proposal distribution used \\(\\alpha\\) Metropolis-Hastings algorithm. Defaults 0.1. alpha_init Numeric value specifying initial value scale parameter \\(\\alpha\\). Defaults 1. n_clusters > 1, mixture component \\(\\alpha_{c}\\) gets initial value. chains run parallel, providing argument cl = cl, alpha_init can vector length length(cl), element becomes initial value given chain. alpha_jump Integer specifying many times sample \\(\\rho\\) sampling \\(\\alpha\\). words, many times jump \\(\\alpha\\) sampling \\(\\rho\\), possibly parameters like augmented ranks \\(\\tilde{R}\\) cluster assignments \\(z\\). Setting alpha_jump high number can speed computation time, reducing number times partition function Mallows model needs computed. Defaults 1L. lambda Strictly positive numeric value specifying rate parameter truncated exponential prior distribution \\(\\alpha\\). Defaults 0.1. n_cluster > 1, mixture component \\(\\alpha_{c}\\) prior distribution. alpha_max Maximum value alpha truncated exponential prior distribution. psi Integer specifying concentration parameter \\(\\psi\\) Dirichlet prior distribution used cluster probabilities \\(\\tau_{1}, \\tau_{2}, \\dots, \\tau_{C}\\), \\(C\\) value n_clusters. Defaults 10L. n_clusters = 1, argument used. include_wcd Logical indicating whether store within-cluster distances computed Metropolis-Hastings algorithm. Defaults TRUE n_clusters > 1 otherwise FALSE. Setting include_wcd = TRUE useful deciding number mixture components include, required plot_elbow. save_aug Logical specifying whether save augmented rankings every aug_thinningth iteration, case missing data pairwise preferences. Defaults FALSE. Saving augmented data useful predicting rankings assessor give items yet ranked, required plot_top_k. aug_thinning Integer specifying thinning saving augmented data. used save_aug = TRUE. Defaults 1L. logz_estimate Estimate partition function, computed estimate_partition_function. aware using estimated partition function n_clusters > 1, partition function estimated whole range \\(\\alpha\\) values covered prior distribution \\(\\alpha\\) high probability. case cluster \\(\\alpha_c\\) becomes empty Metropolis-Hastings algorithm, posterior \\(\\alpha_c\\) equals prior. example, rate parameter exponential prior equals, say \\(\\lambda = 0.001\\), 37 % (exactly: 1 - pexp(1000, 0.001)) prior probability \\(\\alpha_c > 1000\\). Hence n_clusters > 1, estimated partition function cover range, \\(\\lambda\\) increased. verbose Logical specifying whether print progress Metropolis-Hastings algorithm. TRUE, notification printed every 1000th iteration. Defaults FALSE. validate_rankings Logical specifying whether rankings provided (generated preferences) validated. Defaults TRUE. Turning check reduce computing time large number items assessors. na_action Character specifying deal NA values rankings matrix, provided. Defaults \"augment\", means missing values automatically filled using Bayesian data augmentation scheme described Vitelli et al. (2018) . options argument \"fail\", means error message printed algorithm stops NAs rankings, \"omit\" simply deletes rows NAs . constraints Optional constraint set returned generate_constraints. Defaults NULL, means constraint set computed internally. repeated calls compute_mallows, large datasets, computing constraint set may time consuming. case can beneficial precompute provide separate argument. save_ind_clus Whether save individual cluster probabilities step. results csv files cluster_probs1.csv, cluster_probs2.csv, ..., saved calling directory. option may slow code considerably, necessary detecting label switching using Stephen's algorithm. See label_switching information. seed Optional integer used random number seed. cl Optional cluster.","code":""},{"path":"/reference/compute_mallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"list class BayesMallows.","code":""},{"path":"/reference/compute_mallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 . Mallows CL (1957). “Non-Null Ranking Models. .” Biometrika, 44(1/2), 114--130. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/compute_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"","code":"# ANALYSIS OF COMPLETE RANKINGS # The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the Mallows # model: model_fit <- compute_mallows(potato_visual)  # We study the trace plot of the parameters assess_convergence(model_fit, parameter = \"alpha\")  if (FALSE) assess_convergence(model_fit, parameter = \"rho\")  # Based on these plots, we set burnin = 1000. model_fit$burnin <- 1000 # Next, we use the generic plot function to study the posterior distributions # of alpha and rho plot(model_fit, parameter = \"alpha\")  if (FALSE) plot(model_fit, parameter = \"rho\", items = 10:15)  # We can also compute the CP consensus posterior ranking compute_consensus(model_fit, type = \"CP\") #>    ranking item cumprob #> 1        1  P12   1.000 #> 2        2  P13   1.000 #> 3        3   P9   1.000 #> 4        4  P10   0.988 #> 5        5  P17   0.832 #> 6        6   P7   0.680 #> 7        7  P14   1.000 #> 8        8  P16   1.000 #> 9        9   P1   0.393 #> 10      10  P11   0.654 #> 11      11   P5   0.844 #> 12      12  P19   1.000 #> 13      13  P20   0.509 #> 14      14  P18   1.000 #> 15      15   P6   1.000 #> 16      16   P4   0.578 #> 17      17   P2   0.963 #> 18      18  P15   1.000 #> 19      19   P3   1.000 #> 20      20   P8   1.000  # And we can compute the posterior intervals: # First we compute the interval for alpha compute_posterior_intervals(model_fit, parameter = \"alpha\") #>   parameter   mean median conf_level           hpdi central_interval #> 1     alpha 10.799 10.757       95 % [9.500,12.198]   [9.499,12.198] # Then we compute the interval for all the items if (FALSE) compute_posterior_intervals(model_fit, parameter = \"rho\")  # ANALYSIS OF PAIRWISE PREFERENCES if (FALSE) {   # The example dataset beach_preferences contains pairwise   # preferences between beaches stated by 60 assessors. There   # is a total of 15 beaches in the dataset.   # In order to use it, we first generate all the orderings   # implied by the pairwise preferences.   beach_tc <- generate_transitive_closure(beach_preferences)   # We also generate an inital rankings   beach_rankings <- generate_initial_ranking(beach_tc, n_items = 15)   # We then run the Bayesian Mallows rank model   # We save the augmented data for diagnostics purposes.   model_fit <- compute_mallows(rankings = beach_rankings,                                preferences = beach_tc,                                save_aug = TRUE,                                verbose = TRUE)   # We can assess the convergence of the scale parameter   assess_convergence(model_fit)   # We can assess the convergence of latent rankings. Here we   # show beaches 1-5.   assess_convergence(model_fit, parameter = \"rho\", items = 1:5)   # We can also look at the convergence of the augmented rankings for   # each assessor.   assess_convergence(model_fit, parameter = \"Rtilde\",                      items = c(2, 4), assessors = c(1, 2))   # Notice how, for assessor 1, the lines cross each other, while   # beach 2 consistently has a higher rank value (lower preference) for   # assessor 2. We can see why by looking at the implied orderings in   # beach_tc   subset(beach_tc, assessor %in% c(1, 2) &            bottom_item %in% c(2, 4) & top_item %in% c(2, 4))   # Assessor 1 has no implied ordering between beach 2 and beach 4,   # while assessor 2 has the implied ordering that beach 4 is preferred   # to beach 2. This is reflected in the trace plots. }  # CLUSTERING OF ASSESSORS WITH SIMILAR PREFERENCES if (FALSE) {   # The example dataset sushi_rankings contains 5000 complete   # rankings of 10 types of sushi   # We start with computing a 3-cluster solution   model_fit <- compute_mallows(sushi_rankings, n_clusters = 3,                                nmc = 10000, verbose = TRUE)   # We then assess convergence of the scale parameter alpha   assess_convergence(model_fit)   # Next, we assess convergence of the cluster probabilities   assess_convergence(model_fit, parameter = \"cluster_probs\")   # Based on this, we set burnin = 1000   # We now plot the posterior density of the scale parameters alpha in   # each mixture:   model_fit$burnin <- 1000   plot(model_fit, parameter = \"alpha\")   # We can also compute the posterior density of the cluster probabilities   plot(model_fit, parameter = \"cluster_probs\")   # We can also plot the posterior cluster assignment. In this case,   # the assessors are sorted according to their maximum a posteriori cluster estimate.   plot(model_fit, parameter = \"cluster_assignment\")   # We can also assign each assessor to a cluster   cluster_assignments <- assign_cluster(model_fit, soft = FALSE)   }  # DETERMINING THE NUMBER OF CLUSTERS if (FALSE) {   # Continuing with the sushi data, we can determine the number of cluster   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(n_clusters = n_clusters, rankings = sushi_rankings,                                      nmc = 6000, alpha_jump = 10, include_wcd = TRUE)   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   plot_elbow(models, burnin = 1000)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., at 6 clusters. }  # SPEEDING UP COMPUTION WITH OBSERVATION FREQUENCIES # With a large number of assessors taking on a relatively low number of unique rankings, # the obs_freq argument allows providing a rankings matrix with the unique set of rankings, # and the obs_freq vector giving the number of assessors with each ranking. # This is illustrated here for the potato_visual dataset # # assume each row of potato_visual corresponds to between 1 and 5 assessors, as # given by the obs_freq vector set.seed(1234) obs_freq <- sample.int(n = 5, size = nrow(potato_visual), replace = TRUE) m <- compute_mallows(rankings = potato_visual, obs_freq = obs_freq)  # See the separate help page for more examples, with the following code help(\"obs_freq\")"},{"path":"/reference/compute_mallows_mixtures.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"Convenience function computing Mallows models varying numbers mixtures. useful deciding number mixtures use final model.","code":""},{"path":"/reference/compute_mallows_mixtures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"","code":"compute_mallows_mixtures(n_clusters, ..., cl = NULL)"},{"path":"/reference/compute_mallows_mixtures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"n_clusters Integer vector specifying number clusters use. ... named arguments, passed compute_mallows. cl Optional computing cluster used parallelization, returned parallel::makeCluster. Defaults NULL.","code":""},{"path":"/reference/compute_mallows_mixtures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"list Mallows models class BayesMallowsMixtures, one element number mixtures computed. object can studied plot_elbow.","code":""},{"path":[]},{"path":"/reference/compute_mallows_mixtures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"","code":"# DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA if (FALSE) {   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(n_clusters = n_clusters,                                      rankings = sushi_rankings,                                      include_wcd = TRUE)   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   plot_elbow(models, burnin = 1000)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., n_clusters = 5.    # Having chosen the number of clusters, we can now study the final model   # Rerun with 5 clusters   mixture_model <- compute_mallows(rankings = sushi_rankings, n_clusters = 5,                                    include_wcd = TRUE)   # Delete the models object to free some memory   rm(models)   # Set the burnin   mixture_model$burnin <- 1000   # Plot the posterior distributions of alpha per cluster   plot(mixture_model)   # Compute the posterior interval of alpha per cluster   compute_posterior_intervals(mixture_model, parameter = \"alpha\")   # Plot the posterior distributions of cluster probabilities   plot(mixture_model, parameter = \"cluster_probs\")   # Plot the posterior probability of cluster assignment   plot(mixture_model, parameter = \"cluster_assignment\")   # Plot the posterior distribution of \"tuna roll\" in each cluster   plot(mixture_model, parameter = \"rho\", items = \"tuna roll\")   # Compute the cluster-wise CP consensus, and show one column per cluster   cp <- compute_consensus(mixture_model, type = \"CP\")   cp$cumprob <- NULL   stats::reshape(cp, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(cp$cluster))))    # Compute the MAP consensus, and show one column per cluster   map <- compute_consensus(mixture_model, type = \"MAP\")   map$probability <- NULL   stats::reshape(map, direction = \"wide\", idvar = \"map_ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(map$cluster))))    # RUNNING IN PARALLEL   # Computing Mallows models with different number of mixtures in parallel leads to   # considerably speedup   library(parallel)   cl <- makeCluster(detectCores() - 1)   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(n_clusters = n_clusters,                                      rankings = sushi_rankings,                                      include_wcd = TRUE, cl = cl)   stopCluster(cl) }"},{"path":"/reference/compute_posterior_intervals.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute posterior intervals — compute_posterior_intervals.BayesMallows","title":"Compute posterior intervals — compute_posterior_intervals.BayesMallows","text":"Compute posterior intervals","code":""},{"path":"/reference/compute_posterior_intervals.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute posterior intervals — compute_posterior_intervals.BayesMallows","text":"","code":"# S3 method for BayesMallows compute_posterior_intervals(   model_fit,   burnin = model_fit$burnin,   parameter = \"alpha\",   level = 0.95,   decimals = 3L,   ... )"},{"path":"/reference/compute_posterior_intervals.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute posterior intervals — compute_posterior_intervals.BayesMallows","text":"model_fit object class BayesMallows returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. parameter Character string defining parameter compute posterior intervals . One \"alpha\", \"rho\", \"cluster_probs\". Default \"alpha\". level Decimal number \\([0,1]\\) specifying confidence level. Defaults 0.95. decimals Integer specifying number decimals include posterior intervals mean median. Defaults 3. ... arguments. Currently used.","code":""},{"path":[]},{"path":"/reference/compute_posterior_intervals.SMCMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute posterior intervals — compute_posterior_intervals.SMCMallows","title":"Compute posterior intervals — compute_posterior_intervals.SMCMallows","text":"function computes posterior intervals based set samples last timepoint SMC algorithm.","code":""},{"path":"/reference/compute_posterior_intervals.SMCMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute posterior intervals — compute_posterior_intervals.SMCMallows","text":"","code":"# S3 method for SMCMallows compute_posterior_intervals(   model_fit,   parameter = \"alpha\",   level = 0.95,   decimals = 3L,   ... )"},{"path":"/reference/compute_posterior_intervals.SMCMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute posterior intervals — compute_posterior_intervals.SMCMallows","text":"model_fit object class SMCMallows, returned smc_mallows_new_item_rank smc_mallows_new_users. parameter Character string defining parameter compute posterior intervals . One \"alpha\" \"rho\". level Decimal number \\([0,1]\\) specifying confidence level. Defaults 0.95. decimals Integer specifying number decimals include posterior intervals mean median. Defaults 3. ... arguments. Currently used.","code":""},{"path":[]},{"path":"/reference/compute_posterior_intervals.SMCMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute posterior intervals — compute_posterior_intervals.SMCMallows","text":"","code":"# Basic elements data <- sushi_rankings[1:100, ] n_items <- ncol(data) leap_size <- floor(n_items / 5) metric <- \"footrule\" Time <- 20 N <- 100  # Prepare exact partition function cardinalities <- prepare_partition_function(metric = metric,                                             n_items = n_items)$cardinalities  # Performing SMC smc_test <- smc_mallows_new_users(   R_obs = data, type = \"complete\", n_items = n_items,   metric = metric, leap_size = leap_size,   N = N, Time = Time,   cardinalities = cardinalities,   mcmc_kernel_app = 5,   num_new_obs = 5,   alpha_prop_sd = 0.5,   lambda = 0.15,   alpha_max = 1e6 )  compute_posterior_intervals(smc_test, parameter = \"rho\") #>       item parameter mean median conf_level  hpdi central_interval #> 1   Item 1       rho 3.55      4       95 % [2,5]    [2.000,5.000] #> 2  Item 10       rho 9.98     10       95 %  [10]         [10.000] #> 3   Item 2       rho 4.28      4       95 % [2,6]    [2.000,6.000] #> 4   Item 3       rho 2.20      2       95 % [2,4]    [2.000,4.000] #> 5   Item 4       rho 6.38      6       95 % [5,8]    [5.000,8.000] #> 6   Item 5       rho 8.01      8       95 % [6,9]    [6.000,9.000] #> 7   Item 6       rho 4.05      4       95 % [3,5]    [3.000,5.000] #> 8   Item 7       rho 8.83      9       95 % [8,9]    [8.000,9.000] #> 9   Item 8       rho 1.00      1       95 %   [1]          [1.000] #> 10  Item 9       rho 6.72      7       95 % [6,7]    [6.000,7.000]  compute_consensus(model_fit = smc_test, type = \"CP\") #>    ranking    item cumprob #> 1        1  Item 8    1.00 #> 2        2  Item 3    0.87 #> 3        3  Item 1    0.48 #> 4        4  Item 6    0.60 #> 5        5  Item 2    0.93 #> 6        6  Item 4    0.65 #> 7        7  Item 9    0.98 #> 8        8  Item 5    0.82 #> 9        9  Item 7    0.99 #> 10      10 Item 10    1.00 compute_consensus(model_fit = smc_test, type = \"MAP\") #>    probability    item map_ranking #> 1         0.12  Item 8           1 #> 2         0.12  Item 3           2 #> 3         0.12  Item 6           3 #> 4         0.12  Item 1           4 #> 5         0.12  Item 2           5 #> 6         0.12  Item 4           6 #> 7         0.12  Item 9           7 #> 8         0.12  Item 5           8 #> 9         0.12  Item 7           9 #> 10        0.12 Item 10          10  compute_posterior_intervals(smc_test, parameter = \"alpha\") #>   parameter  mean median conf_level          hpdi central_interval #> 1     alpha 1.696  1.709       95 % [1.403,1.915]    [1.411,1.911]"},{"path":"/reference/compute_posterior_intervals.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Posterior Intervals — compute_posterior_intervals","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"Compute posterior intervals parameters interest.","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"","code":"compute_posterior_intervals(model_fit, ...)"},{"path":"/reference/compute_posterior_intervals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"model_fit model object. ... arguments passed methods.","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"function computes Highest Posterior Density Interval (HPDI), may discontinuous bimodal distributions, central posterior interval, simply defined quantiles posterior distribution. HPDI intervals computed using HDInterval package (Meredith Kruschke 2018) .","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"Meredith M, Kruschke J (2018). HDInterval: Highest (Posterior) Density Intervals. R package version 0.2.0, https://CRAN.R-project.org/package=HDInterval.","code":""},{"path":[]},{"path":"/reference/compute_posterior_intervals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"","code":"# The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the Mallows # model: model_fit <- compute_mallows(potato_visual)  # Se the documentation to compute_mallows for how to assess the convergence of the algorithm # Having chosen burin = 1000, we compute posterior intervals model_fit$burnin <- 1000 # First we compute the interval for alpha compute_posterior_intervals(model_fit, parameter = \"alpha\") #>   parameter   mean median conf_level           hpdi central_interval #> 1     alpha 10.801 10.783       95 % [9.484,12.101]   [9.468,12.089] # We can reduce the number decimals compute_posterior_intervals(model_fit, parameter = \"alpha\", decimals = 2) #>   parameter mean median conf_level         hpdi central_interval #> 1     alpha 10.8  10.78       95 % [9.48,12.10]     [9.47,12.09] # By default, we get a 95 % interval. We can change that to 99 %. compute_posterior_intervals(model_fit, parameter = \"alpha\", level = 0.99) #>   parameter   mean median conf_level           hpdi central_interval #> 1     alpha 10.801 10.783       99 % [9.075,12.437]   [9.177,12.582] # We can also compute the posterior interval for the latent ranks rho compute_posterior_intervals(model_fit, parameter = \"rho\") #>    item parameter mean median conf_level    hpdi central_interval #> 1    P1       rho   10     10       95 %  [9,12]           [9,12] #> 2    P2       rho   17     17       95 % [16,18]          [16,18] #> 3    P3       rho   19     19       95 %    [19]             [19] #> 4    P4       rho   16     16       95 % [16,18]          [16,18] #> 5    P5       rho   10     10       95 %  [9,12]           [9,12] #> 6    P6       rho   15     15       95 %    [15]             [15] #> 7    P7       rho    6      6       95 %   [5,7]            [5,7] #> 8    P8       rho   20     20       95 %    [20]             [20] #> 9    P9       rho    3      3       95 %     [3]              [3] #> 10  P10       rho    4      4       95 %     [4]            [4,5] #> 11  P11       rho   10     10       95 %  [9,12]           [8,12] #> 12  P12       rho    1      1       95 %     [1]              [1] #> 13  P13       rho    2      2       95 %     [2]              [2] #> 14  P14       rho    7      7       95 %   [6,7]            [6,7] #> 15  P15       rho   18     18       95 % [17,18]          [17,18] #> 16  P16       rho    8      8       95 %     [8]            [8,9] #> 17  P17       rho    5      5       95 %   [5,7]            [4,7] #> 18  P18       rho   14     14       95 % [13,14]          [13,14] #> 19  P19       rho   11     12       95 %  [9,12]           [9,12] #> 20  P20       rho   13     13       95 % [13,14]          [13,14]  if (FALSE) {   # Posterior intervals of cluster probabilities   # We can run a mixture of Mallows models, using the n_clusters argument   # We use the sushi example data. See the documentation of compute_mallows for a more elaborate   # example   model_fit <- compute_mallows(sushi_rankings, n_clusters = 5)   # Keeping the burnin at 1000, we can compute the posterior intervals of the cluster probabilities   compute_posterior_intervals(model_fit, burnin = 1000, parameter = \"cluster_probs\") }"},{"path":"/reference/compute_posterior_intervals_alpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Posterior Intervals Alpha — compute_posterior_intervals_alpha","title":"Compute Posterior Intervals Alpha — compute_posterior_intervals_alpha","text":"posterior confidence intervals","code":""},{"path":"/reference/compute_posterior_intervals_alpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Posterior Intervals Alpha — compute_posterior_intervals_alpha","text":"","code":"compute_posterior_intervals_alpha(output, nmc, burnin, verbose = FALSE)"},{"path":"/reference/compute_posterior_intervals_alpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Posterior Intervals Alpha — compute_posterior_intervals_alpha","text":"output subset SMCMallows object (though technically matrix ) nmc Number Monte Carlo samples burnin numeric value specifying number iterations discard burn-. verbose TRUE, prints final output even function assigned object. Defaults FALSE.","code":""},{"path":[]},{"path":"/reference/compute_posterior_intervals_rho.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Posterior Intervals Rho — compute_posterior_intervals_rho","title":"Compute Posterior Intervals Rho — compute_posterior_intervals_rho","text":"posterior confidence intervals rho","code":""},{"path":"/reference/compute_posterior_intervals_rho.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Posterior Intervals Rho — compute_posterior_intervals_rho","text":"","code":"compute_posterior_intervals_rho(   output,   nmc,   burnin,   colnames = NULL,   verbose = FALSE )"},{"path":"/reference/compute_posterior_intervals_rho.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Posterior Intervals Rho — compute_posterior_intervals_rho","text":"output subset SMCMallows object (though technically matrix ) nmc Number Monte Carlo samples burnin numeric value specifying number iterations discard burn-. colnames Column names. verbose TRUE, prints final output even function assigned object. Defaults FALSE.","code":""},{"path":[]},{"path":"/reference/compute_rho_consensus.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute rho consensus — compute_rho_consensus","title":"Compute rho consensus — compute_rho_consensus","text":"function deprecated. Please use   compute_consensus.SMCMallows. MAP CP consensus ranking estimates function deprecated. Please use   compute_consensus.SMCMallows instead.","code":""},{"path":"/reference/compute_rho_consensus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute rho consensus — compute_rho_consensus","text":"","code":"compute_rho_consensus(   output,   nmc,   burnin,   C,   type = \"CP\",   colnames = NULL,   verbose = FALSE )"},{"path":"/reference/compute_rho_consensus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute rho consensus — compute_rho_consensus","text":"output Matrix nmc Number Monte Carlo samples burnin Burnin C C Number clusters. type type colnames Column names verbose Logical","code":""},{"path":[]},{"path":"/reference/correction_kernel.html","id":null,"dir":"Reference","previous_headings":"","what":"Correction Kernel — correction_kernel","title":"Correction Kernel — correction_kernel","text":"Function determine augmented ranking compatible new observed partial ranking. , create new augmentation using random sampling approach calculate augmentation probability.","code":""},{"path":"/reference/correction_kernel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Correction Kernel — correction_kernel","text":"","code":"correction_kernel(observed_ranking, current_ranking, n_items)"},{"path":"/reference/correction_kernel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Correction Kernel — correction_kernel","text":"observed_ranking ranking sequence vector observed partial ranking (missing values) original incomplete partial ranking rankings data set. current_ranking ranking sequence vector current augmented ranking (missing values) n_items Integer number items ranking","code":""},{"path":"/reference/correction_kernel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Correction Kernel — correction_kernel","text":"List containing proposed 'corrected' augmented ranking compatible new observed ranking user","code":""},{"path":"/reference/correction_kernel_pseudo.html","id":null,"dir":"Reference","previous_headings":"","what":"Correction Kernel (pseudolikelihood) — correction_kernel_pseudo","title":"Correction Kernel (pseudolikelihood) — correction_kernel_pseudo","text":"Function determine augmented ranking compatible new observed partial ranking. , create new augmentation using pseudolikelihood approach calculate augmentation probability.","code":""},{"path":"/reference/correction_kernel_pseudo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Correction Kernel (pseudolikelihood) — correction_kernel_pseudo","text":"","code":"correction_kernel_pseudo(   current_ranking,   observed_ranking,   rho,   alpha,   n_items,   metric = \"footrule\" )"},{"path":"/reference/correction_kernel_pseudo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Correction Kernel (pseudolikelihood) — correction_kernel_pseudo","text":"current_ranking complete rank sequence vector  proposed augmented ranking obtained calculate_forward_probability function observed_ranking incomplete rank sequence vector original observed incomplete ranking contains NAs rho Numeric vector specifying consensus ranking alpha Numeric value scale parameter n_items Integer number items ranking metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\".","code":""},{"path":"/reference/correction_kernel_pseudo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Correction Kernel (pseudolikelihood) — correction_kernel_pseudo","text":"list containing R_obs, proposed 'corrected' augmented ranking compatible new observed ranking user,         forward_auxiliary_ranking_probability, numerical value probability correcting ranking compatible R_obs.","code":""},{"path":"/reference/estimate_partition_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Partition Function — estimate_partition_function","title":"Estimate Partition Function — estimate_partition_function","text":"Estimate logarithm partition function Mallows rank model. Choose importance sampling algorithm described (Vitelli et al. 2018)  IPFP algorithm computing asymptotic approximation described (Mukherjee 2016) .","code":""},{"path":"/reference/estimate_partition_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Partition Function — estimate_partition_function","text":"","code":"estimate_partition_function(   method = \"importance_sampling\",   alpha_vector,   n_items,   metric,   nmc,   degree,   n_iterations,   K,   cl = NULL,   seed = NULL )"},{"path":"/reference/estimate_partition_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Partition Function — estimate_partition_function","text":"method Character string specifying method use order estimate logarithm partition function. Available options \"importance_sampling\" \"asymptotic\". alpha_vector Numeric vector \\(\\alpha\\) values compute importance sampling estimate. n_items Integer specifying number items. metric Character string specifying distance measure use. Available options \"footrule\" \"spearman\" method = \"asymptotic\" addition \"cayley\", \"hamming\", \"kendall\", \"ulam\" method = \"importance_sampling\". nmc Integer specifying number Monte Carlo samples use importance sampling. used method = \"importance_sampling\". degree Integer specifying degree polynomial used estimate \\(\\log(\\alpha)\\) grid values provided importance sampling estimate. n_iterations Integer specifying number iterations use asymptotic approximation partition function. used method = \"asymptotic\". K Integer specifying parameter \\(K\\) asymptotic approximation partition function. used method = \"asymptotic\". cl Optional computing cluster used parallelization, returned parallel::makeCluster. Defaults NULL. used method = \"importance_sampling\". seed Optional random number seed.","code":""},{"path":"/reference/estimate_partition_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Partition Function — estimate_partition_function","text":"vector length degree can supplied  logz_estimate argument compute_mallows.","code":""},{"path":"/reference/estimate_partition_function.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate Partition Function — estimate_partition_function","text":"Mukherjee S (2016). “Estimation exponential families permutations.” Annals Statistics, 44(2), 853--875. doi:10.1214/15-aos1389 . Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/estimate_partition_function.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate Partition Function — estimate_partition_function","text":"","code":"if (FALSE) {   # IMPORTANCE SAMPLING   # Let us estimate logZ(alpha) for 20 items with Spearman distance   # We create a grid of alpha values from 0 to 10   alpha_vector <- seq(from = 0, to = 10, by = 0.5)   n_items <- 20   metric <- \"spearman\"   degree <- 10    # We start with 1e3 Monte Carlo samples   fit1 <- estimate_partition_function(method = \"importance_sampling\",                                         alpha_vector = alpha_vector,                                         n_items = n_items, metric = metric,                                         nmc = 1e3, degree = degree)   # A vector of polynomial regression coefficients is returned   fit1    # Now let us recompute with 1e4 Monte Carlo samples   fit2 <- estimate_partition_function(method = \"importance_sampling\",                                       alpha_vector = alpha_vector,                                       n_items = n_items, metric = metric,                                       nmc = 1e4, degree = degree)    # ASYMPTOTIC APPROXIMATION   # We can also compute an estimate using the asymptotic approximation   K <- 20   n_iterations <- 50    fit3 <- estimate_partition_function(method = \"asymptotic\",                                       alpha_vector = alpha_vector,                                       n_items = n_items, metric = metric,                                       n_iterations = n_iterations,                                       K = K, degree = degree)    # We write a little function for storing the estimates in a dataframe   powers <- seq(from = 0, to = degree, by = 1)    compute_fit <- function(fit, type){     do.call(rbind, lapply(alpha_vector, function(alpha){       data.frame(         type = type,         alpha = alpha,         logz_estimate = sum(alpha^powers * fit)       )     }))   }      estimates <- rbind(     compute_fit(fit1, type = \"Importance Sampling 1e3\"),     compute_fit(fit2, type = \"Importance Sampling 1e4\"),     compute_fit(fit3, type = \"Asymptotic\")     )    # We can now plot the two estimates side-by-side   library(ggplot2)   ggplot(estimates, aes(x = alpha, y = logz_estimate, color = type)) +     geom_line()   # We see that the two importance sampling estimates, which are unbiased,   # overlap. The asymptotic approximation seems a bit off. It can be worthwhile   # to try different values of n_iterations and K.    # When we are happy, we can provide the coefficient vector in the   # logz_estimate argument to compute_mallows   # Say we choose to use the importance sampling estimate with 1e4 Monte Carlo samples:   model_fit <- compute_mallows(potato_visual, metric = \"spearman\",                                logz_estimate = fit2)  }"},{"path":"/reference/expected_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected value of metrics under a Mallows rank model — expected_dist","title":"Expected value of metrics under a Mallows rank model — expected_dist","text":"Compute expectation several metrics Mallows rank model.","code":""},{"path":"/reference/expected_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected value of metrics under a Mallows rank model — expected_dist","text":"","code":"expected_dist(alpha, n_items, metric)"},{"path":"/reference/expected_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected value of metrics under a Mallows rank model — expected_dist","text":"alpha Non-negative scalar specifying scale (precision) parameter Mallows rank model. n_items Integer specifying number items. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\" n_items<=95, \"footrule\" n_items<=50 \"spearman\" n_items<=14.","code":""},{"path":"/reference/expected_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected value of metrics under a Mallows rank model — expected_dist","text":"scalar providing expected value metric Mallows rank model distance specified metric argument.","code":""},{"path":[]},{"path":"/reference/expected_dist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected value of metrics under a Mallows rank model — expected_dist","text":"","code":"expected_dist(1, 5, metric = \"kendall\") #> [1] 4.177277 expected_dist(2, 6, metric = \"cayley\") #> [1] 3.212053 expected_dist(1.5, 7, metric = \"hamming\") #> [1] 5.761023 expected_dist(5, 30, \"ulam\") #> [1] 21.33016 expected_dist(3.5, 45, \"footrule\") #> [1] 377.5987 expected_dist(4, 10, \"spearman\") #> [1] 7.220669"},{"path":"/reference/generate_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"function relevant compute_mallows called repeatedly data, e.g., determining number clusters. precomputes list constraints used internally MCMC algorithm, otherwise recomputed time compute_mallows called.","code":""},{"path":"/reference/generate_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"","code":"generate_constraints(preferences, n_items, cl = NULL)"},{"path":"/reference/generate_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"preferences Data frame preferences. case consistent rankings, preferences returned generate_transitive_closure. case inconsistent preferences, using error model described Crispino et al. (2019) , dataframe preferences can directly provided. n_items Integer specifying number items. cl Optional computing cluster used parallelization, returned parallel::makeCluster. Defaults NULL.","code":""},{"path":"/reference/generate_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"list used internally MCMC algorithm.","code":""},{"path":"/reference/generate_constraints.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 .","code":""},{"path":[]},{"path":"/reference/generate_constraints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Constraint Set from Pairwise Comparisons — generate_constraints","text":"","code":"# Here is an example with the beach preference data. # First, generate the transitive closure. beach_tc <- generate_transitive_closure(beach_preferences)  # Next, generate an initial ranking. beach_init_rank <- generate_initial_ranking(beach_tc)  # Then generate the constrain set used intervally by compute_mallows constr <- generate_constraints(beach_tc, n_items = 15)  # Provide all these elements to compute_mallows model_fit <- compute_mallows(rankings = beach_init_rank, preferences = beach_tc, constraints = constr)  if (FALSE) {   # The constraints can also be generated in parallel   library(parallel)   cl <- makeCluster(detectCores() - 1)   constr <- generate_constraints(beach_tc, n_items = 15, cl = cl)   stopCluster(cl) }"},{"path":"/reference/generate_initial_ranking.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Initial Ranking — generate_initial_ranking","title":"Generate Initial Ranking — generate_initial_ranking","text":"Given consistent set pairwise preferences, generate complete ranking items consistent preferences.","code":""},{"path":"/reference/generate_initial_ranking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Initial Ranking — generate_initial_ranking","text":"","code":"generate_initial_ranking(   tc,   n_items = max(tc[, c(\"bottom_item\", \"top_item\")]),   cl = NULL,   shuffle_unranked = FALSE,   random = FALSE,   random_limit = 8L )"},{"path":"/reference/generate_initial_ranking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Initial Ranking — generate_initial_ranking","text":"tc dataframe pairwise comparisons S3 subclass BayesMallowsTC, returned generate_transitive_closure. n_items total number items. provided, assumed equal largest item index found tc, .e., max(tc[, c(\"bottom_item\", \"top_item\")]). cl Optional computing cluster used parallelization, returned parallel::makeCluster. Defaults NULL. shuffle_unranked Logical specifying whether randomly permuted unranked items initial ranking. shuffle_unranked=TRUE random=FALSE, unranked items assessor randomly permuted. Otherwise, first ordering returned igraph::topo_sort() returned. random Logical specifying whether use random initial ranking. Defaults FALSE. Setting TRUE means possible orderings consistent stated pairwise preferences generated assessor, one picked random. random_limit Integer specifying maximum number items allowed possible orderings computed, .e., random=TRUE. Defaults 8L.","code":""},{"path":"/reference/generate_initial_ranking.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Initial Ranking — generate_initial_ranking","text":"matrix rankings can given rankings argument compute_mallows.","code":""},{"path":"/reference/generate_initial_ranking.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Generate Initial Ranking — generate_initial_ranking","text":"Setting random=TRUE means possible orderings   assessor's preferences generated, one picked random.   can useful experiencing convergence issues, e.g., MCMC   algorithm mixed properly. However, finding possible orderings   combinatorial problem, may computationally hard.   result may even possible fit memory, may cause R   session crash. using option, please try increase size   problem incrementally, starting smaller subsets   complete data. example given . detailed documentation generate_transitive_closure,   assumed items labeled starting 1. example, single   comparison following form provided, assumed total   30 items (n_items=30), initial ranking permutation 30   items consistent preference 29<30. reality two items, relabeled 1 2, follows:","code":""},{"path":[]},{"path":"/reference/generate_initial_ranking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Initial Ranking — generate_initial_ranking","text":"","code":"# The example dataset beach_preferences contains pairwise preferences of beach. # We must first generate the transitive closure beach_tc <- generate_transitive_closure(beach_preferences)  # Next, we generate an initial ranking beach_init <- generate_initial_ranking(beach_tc)  # Look at the first few rows: head(beach_init) #>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> 1    7   15    3   14   13    2   10   12    6     4     1     5     8    11 #> 2    5   15   14    6    2   13   12   11    1     7     4    10     9     8 #> 3    3    9    7   15   14    2    6   13    1     8    12    11    10     5 #> 4    5   15    2   14   11    8    9   10    1     4     3    13     7    12 #> 5    9   15    7    5    6    1   14   13    4     3     2    12    11     8 #> 6    9   13    5   12   15    3    7   11    1     6     4    14     8    10 #>   [,15] #> 1     9 #> 2     3 #> 3     4 #> 4     6 #> 5    10 #> 6     2  # We can add more informative column names in order # to get nicer posterior plots: colnames(beach_init) <- paste(\"Beach\", seq(from = 1, to = ncol(beach_init), by = 1)) head(beach_init) #>   Beach 1 Beach 2 Beach 3 Beach 4 Beach 5 Beach 6 Beach 7 Beach 8 Beach 9 #> 1       7      15       3      14      13       2      10      12       6 #> 2       5      15      14       6       2      13      12      11       1 #> 3       3       9       7      15      14       2       6      13       1 #> 4       5      15       2      14      11       8       9      10       1 #> 5       9      15       7       5       6       1      14      13       4 #> 6       9      13       5      12      15       3       7      11       1 #>   Beach 10 Beach 11 Beach 12 Beach 13 Beach 14 Beach 15 #> 1        4        1        5        8       11        9 #> 2        7        4       10        9        8        3 #> 3        8       12       11       10        5        4 #> 4        4        3       13        7       12        6 #> 5        3        2       12       11        8       10 #> 6        6        4       14        8       10        2  # By default, the algorithm for generating the initial ranking is deterministic. # It is possible to randomly permute the unranked items with the argument shuffle_unranked, # as demonstrated below. This algorithm is computationally efficient, but defaults to FALSE # for backward compatibility. set.seed(2233) beach_init <- generate_initial_ranking(beach_tc, shuffle_unranked = TRUE) head(beach_init) #>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] #> 1    7   15    3   14   13    2   10   12    6     4     1     5     8    11 #> 2    6   15    3    7    2   13   14   12    1     8     5    11     9    10 #> 3    4   10    8   15   14    3    7   13    2     9    12    11     1     6 #> 4    5   15    2   12   11    8    9   10    1     4     3    14     7    13 #> 5    9   12    7    5    6    1   15   14    4     3     2    13    11     8 #> 6    9   13    5   12   15    3    7   11    1     6     4    14     8    10 #>   [,15] #> 1     9 #> 2     4 #> 3     5 #> 4     6 #> 5    10 #> 6     2  # It is also possible to pick a random sample among all topological sorts. # This requires first enumerating all possible sorts, and might hence be computationally # demanding. Here is an example, where we reduce the data considerable to speed up computation. small_tc <- beach_tc[beach_tc$assessor %in% 1:6 &                        beach_tc$bottom_item %in% 1:4 & beach_tc$top_item %in% 1:4, ] set.seed(123) init_small <- generate_initial_ranking(tc = small_tc, n_items = 4, random = TRUE) # Look at the initial rankings generated init_small #>   [,1] [,2] [,3] [,4] #> 1    3    1    4    2 #> 2    1    4    3    2 #> 3    1    3    2    4 #> 4    1    4    3    2 #> 5    4    2    3    1 #> 6    3    1    4    2  # For this small dataset, we can also study the effect of setting shuffle_unranked=TRUE # in more detail. We consider assessors 1 and 2 only. # First is the deterministic ordering. This one is equal for each run. generate_initial_ranking(tc = small_tc[small_tc$assessor %in% c(1, 2), ],                          n_items = 4, shuffle_unranked = FALSE, random = FALSE) #>   [,1] [,2] [,3] [,4] #> 1    2    4    1    3 #> 2    1    4    3    2 # Next we shuffle the unranked, setting the seed for reproducibility. # For assessor 1, item 2 is unranked, and by rerunning the code multiple times, # we see that element (1, 2) indeed changes ranking randomly. # For assessor 2, item 3 is unranked, and we can also see that this item changes # ranking randomly when rerunning the function multiple times. # The ranked items also change their ranking from one random realiziation to another, # but their relative ordering is constant. set.seed(123) generate_initial_ranking(tc = small_tc[small_tc$assessor %in% c(1, 2), ],                          n_items = 4, shuffle_unranked = TRUE, random = FALSE) #>   [,1] [,2] [,3] [,4] #> 1    2    3    1    4 #> 2    1    3    4    2   if (FALSE) {   # We now give beach_init and beach_tc to compute_mallows. We tell compute_mallows   # to save the augmented data, in order to study the convergence.   model_fit <- compute_mallows(rankings = beach_init,                                preferences = beach_tc,                                nmc = 2000,                                save_aug = TRUE)    # We can study the acceptance rate of the augmented rankings   assess_convergence(model_fit, parameter = \"Rtilde\")    # We can also study the posterior distribution of the consensus rank of each beach   model_fit$burnin <- 500   plot(model_fit, parameter = \"rho\", items = 1:15) }  if (FALSE) {   # The computations can also be done in parallel   library(parallel)   cl <- makeCluster(detectCores() - 1)   beach_tc <- generate_transitive_closure(beach_preferences, cl = cl)   beach_init <- generate_initial_ranking(beach_tc, cl = cl)   stopCluster(cl) }"},{"path":"/reference/generate_transitive_closure.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Transitive Closure — generate_transitive_closure","title":"Generate Transitive Closure — generate_transitive_closure","text":"Generate transitive closure set consistent pairwise comparisons. result can given preferences argument compute_mallows.","code":""},{"path":"/reference/generate_transitive_closure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Transitive Closure — generate_transitive_closure","text":"","code":"generate_transitive_closure(df, cl = NULL)"},{"path":"/reference/generate_transitive_closure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Transitive Closure — generate_transitive_closure","text":"df data frame one row per pairwise comparison, columns assessor, top_item, bottom_item. column contains following: assessor numeric vector containing assessor index, character       vector containing (unique) name assessor. bottom_item numeric vector containing index item       disfavored pairwise comparison. top_item numeric vector containing index item       preferred pairwise comparison. two assessors five items, assessor 1 prefers item 1 item 2 item 1 item 5, assessor 2 prefers item 3 item 5, following df: cl Optional computing cluster used parallelization, returned parallel::makeCluster. Defaults NULL.","code":""},{"path":"/reference/generate_transitive_closure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Transitive Closure — generate_transitive_closure","text":"dataframe columns df, set rows expanded include pairwise preferences implied ones stated df. returned object S3 subclass BayesMallowsTC, indicate transitive closure.","code":""},{"path":[]},{"path":"/reference/generate_transitive_closure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Transitive Closure — generate_transitive_closure","text":"","code":"# Let us first consider a simple case with two assessors, where assessor 1 # prefers item 1 to item 2, and item 1 to item 5, while assessor 2 prefers # item 3 to item 5. We then have the following dataframe of pairwise # comparisons: pair_comp <- data.frame(   assessor = c(1, 1, 2),   bottom_item = c(2, 5, 5),   top_item = c(1, 1, 3)) # We then generate the transitive closure of these preferences: (pair_comp_tc <- generate_transitive_closure(pair_comp)) #>   assessor bottom_item top_item #> 1        1           2        1 #> 2        1           5        1 #> 3        2           5        3 # In this case, no additional relations we implied by the ones # stated in pair_comp, so pair_comp_tc has exactly the same rows # as pair_comp.  # Now assume that assessor 1 also preferred item 5 to item 3, and # that assessor 2 preferred item 4 to item 3. pair_comp <- data.frame(   assessor = c(1, 1, 1, 2, 2),   bottom_item = c(2, 5, 3, 5, 3),   top_item = c(1, 1, 5, 3, 4))  # We generate the transitive closure again: (pair_comp_tc <- generate_transitive_closure(pair_comp)) #>   assessor bottom_item top_item #> 1        1           2        1 #> 2        1           3        1 #> 3        1           5        1 #> 4        1           3        5 #> 5        2           5        3 #> 6        2           3        4 #> 7        2           5        4 # We now have one implied relation for each assessor. # For assessor 1, it is implied that 1 is preferred to 3. # For assessor 2, it is implied that 4 is preferred to 5.  if (FALSE) {   # If assessor 1 in addition preferred item 3 to item 1,   # the preferences would not be consistent. This is not yet supported by compute_mallows,   # so it causes an error message. It will be supported in a future release of the package.   # First, we add the inconsistent row to pair_comp   pair_comp <- rbind(     pair_comp,     data.frame(assessor = 1, bottom_item = 1, top_item = 3))    # This causes an error message and prints out the problematic rankings:   (pair_comp_tc <- generate_transitive_closure(pair_comp)) }   if (FALSE) {   # The computations can also be done in parallel   library(parallel)   cl <- makeCluster(detectCores() - 1)   beach_tc <- generate_transitive_closure(beach_preferences, cl = cl)   stopCluster(cl) }"},{"path":"/reference/get_mallows_loglik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"Compute either likelihood log-likelihood value   Mallows mixture model parameters dataset complete rankings.","code":""},{"path":"/reference/get_mallows_loglik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"","code":"get_mallows_loglik(   rho,   alpha,   weights,   metric,   rankings,   obs_freq = NULL,   log = TRUE )"},{"path":"/reference/get_mallows_loglik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"rho matrix size n_clusters x n_items whose rows permutations first n_items integers corresponding modal rankings Mallows mixture components. alpha vector n_clusters non-negative scalar specifying scale (precision) parameters Mallows mixture components. weights vector n_clusters non-negative scalars specifying mixture weights. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\" n_items<=95, \"footrule\" n_items<=50 \"spearman\" n_items<=14. rankings matrix observed rankings row. obs_freq vector observation frequencies (weights) apply row rankings. can speed computation large number assessors share rank pattern. Defaults NULL, means row rankings multiplied 1. provided, obs_freq must number elements rows rankings, rankings NULL. log logical; TRUE, log-likelihood value returned, otherwise exponential. Default TRUE.","code":""},{"path":"/reference/get_mallows_loglik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"likelihood log-likelihood value corresponding one   observed complete rankings Mallows mixture rank model   distance specified metric argument.","code":""},{"path":[]},{"path":"/reference/get_mallows_loglik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"","code":"# Simulate a sample from a Mallows model with the Kendall distance  n_items <- 5 mydata <- sample_mallows(   n_samples = 100,   rho0 = 1:n_items,   alpha0 = 10,   metric = \"kendall\")  # Compute the likelihood and log-likelihood values under the true model... get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = mydata,   log = FALSE   ) #> [1] 1.274607e-67  get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = mydata,   log = TRUE   ) #> [1] -154.0306  # or equivalently, by using the frequency distribution freq_distr <- rank_freq_distr(mydata) get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = freq_distr[, 1:n_items],   obs_freq = freq_distr[, n_items + 1],   log = FALSE   ) #> [1] 1.274607e-67  get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = freq_distr[, 1:n_items],   obs_freq = freq_distr[, n_items + 1],   log = TRUE   ) #> [1] -154.0306"},{"path":"/reference/get_rank_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the Distance between two rankings — get_rank_distance","title":"Compute the Distance between two rankings — get_rank_distance","text":"Compute distance two rankings according several metrics.","code":""},{"path":"/reference/get_rank_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the Distance between two rankings — get_rank_distance","text":"","code":"get_rank_distance(r1, r2, metric)"},{"path":"/reference/get_rank_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the Distance between two rankings — get_rank_distance","text":"r1 vector ranks. r2 vector ranks. metric string. Available options \"footrule\", \"kendall\", \"cayley\", \"hamming\", \"spearman\", \"ulam\".","code":""},{"path":"/reference/get_rank_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the Distance between two rankings — get_rank_distance","text":"scalar.","code":""},{"path":"/reference/get_rank_distance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the Distance between two rankings — get_rank_distance","text":"Note Spearman distance squared L2 norm, whereas footrule distance L1 norm. Ulam distance uses SUBSET library developed John Burkardt, available http://people.sc.fsu.edu/~jburkardt/cpp_src/subset/subset.html. implementation Cayley distance based C++ translation Rankcluster::distCayley (Grimonprez Jacques 2016) .","code":""},{"path":"/reference/get_rank_distance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute the Distance between two rankings — get_rank_distance","text":"Grimonprez Q, Jacques J (2016). Rankcluster: Model-Based Clustering Multivariate Partial Ranking Data. R package version 0.94, https://CRAN.R-project.org/package=Rankcluster.","code":""},{"path":"/reference/heat_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Heat plot of posterior probabilities — heat_plot","title":"Heat plot of posterior probabilities — heat_plot","text":"Generates heat plot items consensus ordering along horizontal axis ranking along vertical axis. color denotes posterior probability.","code":""},{"path":"/reference/heat_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heat plot of posterior probabilities — heat_plot","text":"","code":"heat_plot(model_fit, burnin = model_fit$burnin, ...)"},{"path":"/reference/heat_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heat plot of posterior probabilities — heat_plot","text":"model_fit object type BayesMallows, returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults x$burnin, must provided x$burnin exist. See assess_convergence. ... Additional arguments passed methods. particular, type = \"CP\" type = \"MAP\" can passed compute_consensus determine order items along horizontal axis.","code":""},{"path":"/reference/heat_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heat plot of posterior probabilities — heat_plot","text":"ggplot object.","code":""},{"path":[]},{"path":"/reference/heat_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Heat plot of posterior probabilities — heat_plot","text":"","code":"# Setting the number of Monte Carlo samples very low for the example to run fast. # A real application should run much longer, and have a large burnin. model_fit <- compute_mallows(potato_visual, nmc = 500, seed = 1) model_fit$burnin <- 100  heat_plot(model_fit)   # Items are ordered along the horizontal axis according to the ordering # returned by compute_consensus, whose default argument is type=\"CP\".  heat_plot(model_fit, type = \"MAP\")"},{"path":"/reference/label_switching.html","id":null,"dir":"Reference","previous_headings":"","what":"Checking for Label Switching in the Mallows Mixture Model — label_switching","title":"Checking for Label Switching in the Mallows Mixture Model — label_switching","text":"Label switching may sometimes problem running mixture models. algorithm Stephens (Stephens 2000) , implemented label.switching package (Papastamoulis 2016) , allows assessment label switching MCMC. moment, available option BayesMallows package. Stephens algorithms requires individual cluster probabilities assessor saved iteration MCMC algorithm. potentially requires much memory, current implementation compute_mallows saves cluster probabilities csv file iteration. example shows perform check label switching practice. Beware functionality development. Later releases might let user determine directory filenames csv files.","code":""},{"path":"/reference/label_switching.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Checking for Label Switching in the Mallows Mixture Model — label_switching","text":"Papastamoulis P (2016). “label.switching: R Package Dealing Label Switching Problem MCMC Outputs.” Journal Statistical Software, Code Snippets, 69(1), 1--24. ISSN 1548-7660, doi:10.18637/jss.v069.c01 . Stephens M (2000). “Dealing label switching mixture models.” Journal Royal Statistical Society: Series B (Statistical Methodology), 62(4), 795-809. doi:10.1111/1467-9868.00265 .","code":""},{"path":[]},{"path":"/reference/label_switching.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Checking for Label Switching in the Mallows Mixture Model — label_switching","text":"","code":"if (FALSE) {   # This example shows how to assess if label switching happens in BayesMallows    library(BayesMallows)   # We start by creating a directory in which csv files with individual   # cluster probabilities should be saved in each step of the MCMC algorithm   dir.create(\"./test_label_switch\")   # Next, we go into this directory   setwd(\"./test_label_switch/\")   # For comparison, we run compute_mallows with and without saving the cluster   # probabilities The purpose of this is to assess the time it takes to save   # the cluster probabilites   system.time(m <- compute_mallows(rankings = sushi_rankings,                                    n_clusters = 6, nmc = 2000,                                    save_ind_clus = FALSE, verbose = TRUE))   # With this options, compute_mallows will save cluster_probs2.csv,   # cluster_probs3.csv, ..., cluster_probs[nmc].csv.   system.time(m <- compute_mallows(rankings = sushi_rankings, n_clusters = 6,                                    nmc = 2000,                                    save_ind_clus = TRUE, verbose = TRUE))    # Next, we check convergence of alpha   assess_convergence(m)    # We set the burnin to 1000   burnin <- 1000    # Find all files that were saved. Note that the first file saved is cluster_probs2.csv   cluster_files <- list.files(pattern = \"cluster\\\\_probs[[:digit:]]+\\\\.csv\")    # Check the size of the files that were saved.   paste(sum(do.call(file.size, list(cluster_files))) * 1e-6, \"MB\")    # Find the iteration each file corresponds to, by extracting its number   iteration_number <- as.integer(     gsub(\"(^[a-zA-Z\\\\_\\\\.]*)([0-9]+)([a-zA-Z\\\\_\\\\.]+$)\", \"\\\\2\",          cluster_files, perl = TRUE))   # Remove all files before burnin   file.remove(cluster_files[iteration_number <= burnin])   # Update the vector of files, after the deletion   cluster_files <- list.files(pattern = \"cluster\\\\_probs[[:digit:]]+\\\\.csv\")   # Create 3d array, with dimensions (iterations, assessors, clusters)   prob_array <- array(dim = c(length(cluster_files), m$n_assessors, m$n_clusters))   # Read each file, adding to the right element of the array   for(i in seq_along(cluster_files)){     prob_array[i, , ] <- as.matrix(       read.csv(cluster_files[[i]], header = FALSE))   }     # Create an integer array of latent allocations, as this is required by label.switching   z <- subset(m$cluster_assignment, iteration > burnin)   z$value <- as.integer(gsub(\"Cluster \", \"\", z$value))   z <- stats::reshape(z, direction = \"wide\", idvar = \"iteration\", timevar = \"assessor\")   z$iteration <- NULL   z <- as.matrix(z)    # Now apply Stephen's algorithm   library(label.switching)   ls <- label.switching(\"STEPHENS\", z = z, K = m$n_clusters, p = prob_array)    # Check the proportion of cluster assignments that were switched   mean(apply(ls$permutations$STEPHENS, 1, function(x) !all.equal(x, seq(1, m$n_clusters))))    # Remove the rest of the csv files   file.remove(cluster_files)   # Move up one directory   setwd(\"..\")   # Remove the directory in which the csv files were saved   file.remove(\"./test_label_switch/\") }"},{"path":"/reference/obs_freq.html","id":null,"dir":"Reference","previous_headings":"","what":"Observation frequencies in the Bayesian Mallows model — obs_freq","title":"Observation frequencies in the Bayesian Mallows model — obs_freq","text":"one assessor given exact rankings preferences, considerable speed-can obtained providing unique set rankings/preferences compute_mallows, instead providing number assessors obs_freq argument. topic illustrated . See also function rank_freq_distr easily compute observation frequencies.","code":""},{"path":[]},{"path":"/reference/obs_freq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Observation frequencies in the Bayesian Mallows model — obs_freq","text":"","code":"# The first example uses full rankings in the potato_visual dataset, but we assume # that each row in the data corresponds to between 100 and 500 assessors. set.seed(1234) # We start by generating random observation frequencies obs_freq <- sample(x = seq(from = 100L, to = 500L, by = 1L),                   size = nrow(potato_visual), replace = TRUE) # We also create a set of repeated indices, used to extend the matrix rows repeated_indices <- unlist(Map(function(x, y) rep(x, each = y),                                seq_len(nrow(potato_visual)), obs_freq)) # The potato_repeated matrix consists of all rows repeated corresponding to # the number of assessors in the obs_freq vector. This is how a large dataset # would look like without using the obs_freq argument potato_repeated <- potato_visual[repeated_indices, ]  # We now first compute the Mallows model using obs_freq # This takes about 0.2 seconds system.time({   m_obs_freq <- compute_mallows(rankings = potato_visual, obs_freq = obs_freq, nmc = 10000) }) #>    user  system elapsed  #>   0.285   0.024   0.309  # Next we use the full ranking matrix # This takes about 11.3 seconds, about 50 times longer! if (FALSE) { system.time({   m_rep <- compute_mallows(rankings = potato_repeated, nmc = 10000) })    # We set the burnin to 2000 for both   m_obs_freq$burnin <- 2000   m_rep$burnin <- 2000    # Note that the MCMC algorithms did not run with the same   # random number seeds in these two experiments, but still   # the posterior distributions look similar   plot(m_obs_freq, burnin = 2000, \"alpha\")   plot(m_rep, burnin = 2000, \"alpha\")    plot(m_obs_freq, burnin = 2000, \"rho\", items = 1:4)   plot(m_rep, burnin = 2000, \"rho\", items = 1:4) }   # Next we repeated the exercise with the pairwise preference data # in the beach dataset. Note that we first must compute the # transitive closure for each participant. If two participants # have provided different preferences with identical transitive closure, # then we can treat them as identical beach_tc <- generate_transitive_closure(beach_preferences) # Next, we confirm that each participant has a unique transitive closure # We do this by sorting first by top_item and then by bottom_item, # and then concatenating, whereupon we check how many participants there # are for each unique concatenation # This returns zero rows, so there are no participants with the same transitive closure beach_tc <- beach_tc[order(beach_tc$assessor, beach_tc$top_item,                            beach_tc$bottom_item), ] aggr_df <- do.call(rbind, lapply(split(beach_tc, f = beach_tc$assessor), function(x){   x$concat_ranks <- paste(c(x$bottom_item, x$top_item), collapse = \",\")   x }))  aggr_df <- aggregate(list(num_assessors = aggr_df$assessor),                      aggr_df[, \"concat_ranks\", drop = FALSE],                      FUN = function(x) length(unique(x)))  nrow(aggr_df[aggr_df$num_assessors > 1, , drop = FALSE]) #> [1] 0  # We now illustrate the weighting procedure by assuming that there are # more than one assessor per unique transitive closure. We generate an # obs_freq vector such that each unique transitive closure is repeated 1-4 times. set.seed(9988) obs_freq <- sample(x = 1:4, size = length(unique(beach_preferences$assessor)), replace = TRUE)  # Next, we create a new hypthetical beach_preferences dataframe where each # assessor is replicated 1-4 times beach_pref_rep <- do.call(   rbind,   lapply(split(beach_preferences, f = beach_preferences$assessor),          function(dd){            ret <- merge(              dd,              data.frame(                new_assessor = seq_len(obs_freq[unique(dd$assessor)])                ), all = TRUE)            ret$assessor <- paste(ret$assessor, ret$new_assessor, sep = \",\")            ret$new_assessor <- NULL            ret            }))  # We generate transitive closure for these preferences beach_tc_rep <- generate_transitive_closure(beach_pref_rep) # We can check that the number of unique assessors is now larger, # and equal to the sum of obs_freq sum(obs_freq) #> [1] 151 length(unique(beach_tc_rep$assessor)) #> [1] 151  # We generate the initial rankings for the repeated and the \"unrepeated\" # data beach_rankings <- generate_initial_ranking(beach_tc, n_items = 15) beach_rankings_rep <- generate_initial_ranking(beach_tc_rep, n_items = 15)  if (FALSE) { # We then run the Bayesian Mallows rank model, first for the # unrepeated data with a obs_freq argument. This takes about 1.9 seconds system.time({   model_fit_obs_freq <- compute_mallows(rankings = beach_rankings,                                        preferences = beach_tc,                                        obs_freq = obs_freq,                                        save_aug = TRUE,                                        nmc = 10000)  })  # Next for the repeated data. This takes about 4.8 seconds. system.time({   model_fit_rep <- compute_mallows(rankings = beach_rankings_rep,                                    preferences = beach_tc_rep,                                    save_aug = TRUE,                                    nmc = 10000)  })  # As demonstrated here, using a obs_freq argument to exploit patterns in data # where multiple assessors have given identical rankings or preferences, may # lead to considerable speedup.  }"},{"path":"/reference/plot.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Posterior Distributions — plot.BayesMallows","title":"Plot Posterior Distributions — plot.BayesMallows","text":"Plot posterior distributions parameters Mallows Rank model.","code":""},{"path":"/reference/plot.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Posterior Distributions — plot.BayesMallows","text":"","code":"# S3 method for BayesMallows plot(x, burnin = x$burnin, parameter = \"alpha\", items = NULL, ...)"},{"path":"/reference/plot.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Posterior Distributions — plot.BayesMallows","text":"x object type BayesMallows, returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults x$burnin, must provided x$burnin exist. See assess_convergence. parameter Character string defining parameter plot. Available options \"alpha\", \"rho\", \"cluster_probs\", \"cluster_assignment\", \"theta\". items items study diagnostic plot rho. Either vector item names, corresponding x$items vector indices. NULL, five items selected randomly. used parameter = \"rho\". ... arguments passed plot (used).","code":""},{"path":[]},{"path":"/reference/plot.BayesMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Posterior Distributions — plot.BayesMallows","text":"","code":"# The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the Mallows # model: model_fit <- compute_mallows(potato_visual)  # Se the documentation to compute_mallows for how to assess the convergence # of the algorithm # We set the burnin = 1000 model_fit$burnin <- 1000 # By default, the scale parameter \"alpha\" is plotted plot(model_fit)  if (FALSE) {   # We can also plot the latent rankings \"rho\"   plot(model_fit, parameter = \"rho\")   # By default, a random subset of 5 items are plotted   # Specify which items to plot in the items argument.   plot(model_fit, parameter = \"rho\",        items = c(2, 4, 6, 9, 10, 20))   # When the ranking matrix has column names, we can also   # specify these in the items argument.   # In this case, we have the following names:   colnames(potato_visual)   # We can therefore get the same plot with the following call:   plot(model_fit, parameter = \"rho\",        items = c(\"P2\", \"P4\", \"P6\", \"P9\", \"P10\", \"P20\"))   }  if (FALSE) {   # Plots of mixture parameters:   # We can run a mixture of Mallows models, using the n_clusters argument   # We use the sushi example data. See the documentation of compute_mallows for a more elaborate   # example   model_fit <- compute_mallows(sushi_rankings, n_clusters = 5)   model_fit$burnin <- 1000   # We can then plot the posterior distributions of the cluster probabilities   plot(model_fit, parameter = \"cluster_probs\")   # We can also get a cluster assignment plot, showing the assessors along the horizontal   # axis and the clusters along the vertical axis. The color show the probability   # of belonging to each clusters. The assessors are sorted along the horizontal   # axis according to their maximum a posterior cluster assignment. This plot   # illustrates the posterior uncertainty in cluster assignments.   plot(model_fit, parameter = \"cluster_assignment\")   # See also ?assign_cluster for a function which returns the cluster assignment   # back in a dataframe. }"},{"path":"/reference/plot.SMCMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot SMC Posterior Distributions — plot.SMCMallows","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"Plot posterior distributions SMC-Mallow parameters.","code":""},{"path":"/reference/plot.SMCMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"","code":"# S3 method for SMCMallows plot(   x,   nmc = nrow(x$rho_samples[, 1, ]),   burnin = 0,   parameter = \"alpha\",   time = ncol(x$rho_samples[, 1, ]),   C = 1,   colnames = NULL,   items = NULL,   ... )"},{"path":"/reference/plot.SMCMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"x object type SMC-Mallows, returned example smc_mallows_new_users. nmc Number Monte Carlo samples burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. parameter Character string defining parameter plot. Available options \"alpha\" \"rho\". time Integer determining update slice plot C Number cluster colnames vector item names. NULL, generic names generated items ranking. items Either vector item names, vector indices. NULL, five items selected randomly. ... arguments passed plot (used).","code":""},{"path":"/reference/plot.SMCMallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"plot posterior distributions","code":""},{"path":[]},{"path":"/reference/plot.SMCMallows.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"Waldir Leoncio","code":""},{"path":"/reference/plot.SMCMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"","code":"set.seed(994)  n_items <- dim(sushi_rankings)[2] metric <- \"footrule\"  # Estimate the logarithm of the partition function of the Mallows rank model logz_estimate <- estimate_partition_function(   method = \"importance_sampling\",   alpha_vector = seq(from = 0, to = 15, by = 0.5), n_items = n_items,   metric = metric, nmc = 1e2, degree = 10 )  # Perform the resample-move SMC algorithm smc_test <- smc_mallows_new_users(   R_obs = sushi_rankings[1:100, ], type = \"complete\", n_items = n_items,   metric = metric, leap_size = floor(n_items / 5), N = 100, Time = 10,   logz_estimate = logz_estimate, mcmc_kernel_app = 5, num_new_obs = 5,   alpha_prop_sd = 0.5, lambda = 0.15, alpha_max = 1e3 )  # Plot rho plot(smc_test, colnames = colnames(sushi_rankings), parameter = \"rho\") #> Items not provided by user or more than 5 items in a ranking. Picking 5 at random.   # Plot alpha plot(smc_test, parameter = \"alpha\")"},{"path":"/reference/plot_elbow.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Within-Cluster Sum of Distances — plot_elbow","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"Plot within-cluster sum distances corresponding cluster consensus different number clusters. function useful selecting number mixture.","code":""},{"path":"/reference/plot_elbow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"","code":"plot_elbow(..., burnin = NULL)"},{"path":"/reference/plot_elbow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"... One objects returned compute_mallows, separated comma, list objects. Typically, object run different number mixtures, specified n_clusters argument compute_mallows. burnin number iterations discard burnin. Either vector numbers, one model, single number taken burnin models. model provided burnin element, taken default.","code":""},{"path":"/reference/plot_elbow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"boxplot number clusters horizontal axis -cluster sum distances vertical axis.","code":""},{"path":[]},{"path":"/reference/plot_elbow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"","code":"# DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA if (FALSE) {   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(n_clusters = n_clusters,                                      rankings = sushi_rankings,                                      include_wcd = TRUE)   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   plot_elbow(models, burnin = 1000)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., n_clusters = 5.    # Having chosen the number of clusters, we can now study the final model   # Rerun with 5 clusters   mixture_model <- compute_mallows(rankings = sushi_rankings, n_clusters = 5,                                    include_wcd = TRUE)   # Delete the models object to free some memory   rm(models)   # Set the burnin   mixture_model$burnin <- 1000   # Plot the posterior distributions of alpha per cluster   plot(mixture_model)   # Compute the posterior interval of alpha per cluster   compute_posterior_intervals(mixture_model, parameter = \"alpha\")   # Plot the posterior distributions of cluster probabilities   plot(mixture_model, parameter = \"cluster_probs\")   # Plot the posterior probability of cluster assignment   plot(mixture_model, parameter = \"cluster_assignment\")   # Plot the posterior distribution of \"tuna roll\" in each cluster   plot(mixture_model, parameter = \"rho\", items = \"tuna roll\")   # Compute the cluster-wise CP consensus, and show one column per cluster   cp <- compute_consensus(mixture_model, type = \"CP\")   cp$cumprob <- NULL   stats::reshape(cp, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(cp$cluster))))    # Compute the MAP consensus, and show one column per cluster   map <- compute_consensus(mixture_model, type = \"MAP\")   map$probability <- NULL   stats::reshape(map, direction = \"wide\", idvar = \"map_ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(map$cluster))))    # RUNNING IN PARALLEL   # Computing Mallows models with different number of mixtures in parallel leads to   # considerably speedup   library(parallel)   cl <- makeCluster(detectCores() - 1)   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(n_clusters = n_clusters,                                      rankings = sushi_rankings,                                      include_wcd = TRUE, cl = cl)   stopCluster(cl) }"},{"path":"/reference/plot_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"Plot posterior probability, per item, ranked among top-\\(k\\) assessor. plot useful data take form pairwise preferences.","code":""},{"path":"/reference/plot_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"","code":"plot_top_k(   model_fit,   burnin = model_fit$burnin,   k = 3,   rel_widths = c(model_fit$n_clusters, 10) )"},{"path":"/reference/plot_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"model_fit object type BayesMallows, returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. k Integer specifying k top-\\(k\\). rel_widths relative widths plots rho per cluster plot assessors, respectively. argument passed plot_grid.","code":""},{"path":[]},{"path":"/reference/plot_top_k.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"","code":"if (FALSE) {   # We use the example dataset with beach preferences. Se the documentation to   # compute_mallows for how to assess the convergence of the algorithm   # We need to save the augmented data, so setting this option to TRUE   model_fit <- compute_mallows(preferences = beach_preferences,                                save_aug = TRUE)   # We set burnin = 1000   model_fit$burnin <- 1000   # By default, the probability of being top-3 is plotted   plot_top_k(model_fit)   # We can also plot the probability of being top-5, for each item   plot_top_k(model_fit, k = 5)   # We get the underlying numbers with predict_top_k   probs <- predict_top_k(model_fit)   # To find all items ranked top-3 by assessors 1-3 with probability more than 80 %,   # we do   subset(probs, assessor %in% 1:3 & prob > 0.8)  }"},{"path":"/reference/potato_true_ranking.html","id":null,"dir":"Reference","previous_headings":"","what":"True ranking of the weights of 20 potatoes. — potato_true_ranking","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"True ranking weights 20 potatoes.","code":""},{"path":"/reference/potato_true_ranking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"","code":"potato_true_ranking"},{"path":"/reference/potato_true_ranking.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"object class numeric length 20.","code":""},{"path":"/reference/potato_true_ranking.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/potato_visual.html","id":null,"dir":"Reference","previous_headings":"","what":"Result of ranking potatoes by weight, where the assessors were only allowed\nto inspected the potatoes visually. 12 assessors ranked 20 potatoes. — potato_visual","title":"Result of ranking potatoes by weight, where the assessors were only allowed\nto inspected the potatoes visually. 12 assessors ranked 20 potatoes. — potato_visual","text":"Result ranking potatoes weight, assessors allowed inspected potatoes visually. 12 assessors ranked 20 potatoes.","code":""},{"path":"/reference/potato_visual.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Result of ranking potatoes by weight, where the assessors were only allowed\nto inspected the potatoes visually. 12 assessors ranked 20 potatoes. — potato_visual","text":"","code":"potato_visual"},{"path":"/reference/potato_visual.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Result of ranking potatoes by weight, where the assessors were only allowed\nto inspected the potatoes visually. 12 assessors ranked 20 potatoes. — potato_visual","text":"object class matrix (inherits array) 12 rows 20 columns.","code":""},{"path":"/reference/potato_visual.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Result of ranking potatoes by weight, where the assessors were only allowed\nto inspected the potatoes visually. 12 assessors ranked 20 potatoes. — potato_visual","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/potato_weighing.html","id":null,"dir":"Reference","previous_headings":"","what":"Result of ranking potatoes by weight, where the assessors were\nallowed to lift the potatoes. 12 assessors ranked 20 potatoes. — potato_weighing","title":"Result of ranking potatoes by weight, where the assessors were\nallowed to lift the potatoes. 12 assessors ranked 20 potatoes. — potato_weighing","text":"Result ranking potatoes weight, assessors allowed lift potatoes. 12 assessors ranked 20 potatoes.","code":""},{"path":"/reference/potato_weighing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Result of ranking potatoes by weight, where the assessors were\nallowed to lift the potatoes. 12 assessors ranked 20 potatoes. — potato_weighing","text":"","code":"potato_weighing"},{"path":"/reference/potato_weighing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Result of ranking potatoes by weight, where the assessors were\nallowed to lift the potatoes. 12 assessors ranked 20 potatoes. — potato_weighing","text":"object class matrix (inherits array) 12 rows 20 columns.","code":""},{"path":"/reference/potato_weighing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Result of ranking potatoes by weight, where the assessors were\nallowed to lift the potatoes. 12 assessors ranked 20 potatoes. — potato_weighing","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/predict_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"Predict posterior probability, per item, ranked among top-\\(k\\) assessor. useful data take form pairwise preferences.","code":""},{"path":"/reference/predict_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"","code":"predict_top_k(model_fit, burnin = model_fit$burnin, k = 3)"},{"path":"/reference/predict_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"model_fit object type BayesMallows, returned compute_mallows. burnin numeric value specifying number iterations discard burn-. Defaults model_fit$burnin, must provided model_fit$burnin exist. See assess_convergence. k Integer specifying k top-\\(k\\).","code":""},{"path":"/reference/predict_top_k.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"dataframe columns assessor, item,  prob, row states probability given assessor   rates given item among top-\\(k\\).","code":""},{"path":[]},{"path":"/reference/predict_top_k.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"","code":"if (FALSE) {   # We use the example dataset with beach preferences. Se the documentation to   # compute_mallows for how to assess the convergence of the algorithm   # We need to save the augmented data, so setting this option to TRUE   model_fit <- compute_mallows(preferences = beach_preferences,                                save_aug = TRUE)   # We set burnin = 1000   model_fit$burnin <- 1000   # By default, the probability of being top-3 is plotted   plot_top_k(model_fit)   # We can also plot the probability of being top-5, for each item   plot_top_k(model_fit, k = 5)   # We get the underlying numbers with predict_top_k   probs <- predict_top_k(model_fit)   # To find all items ranked top-3 by assessors 1-3 with probability more than 80 %,   # we do   subset(probs, assessor %in% 1:3 & prob > 0.8)  }"},{"path":"/reference/prepare_partition_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare partition functions — prepare_partition_function","title":"Prepare partition functions — prepare_partition_function","text":"Utility function estimating partition function Mallows model.","code":""},{"path":"/reference/prepare_partition_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare partition functions — prepare_partition_function","text":"","code":"prepare_partition_function(logz_estimate = NULL, metric, n_items)"},{"path":"/reference/prepare_partition_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare partition functions — prepare_partition_function","text":"logz_estimate Optional argument containing result calling estimate_partition_function. metric Metric used. n_items Number items.","code":""},{"path":"/reference/prepare_partition_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare partition functions — prepare_partition_function","text":"List two elements, cardinalities logz_estimate, one NULL contains partition function estimates.","code":""},{"path":[]},{"path":"/reference/print.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for BayesMallows Objects — print.BayesMallows","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"default print method BayesMallows object.","code":""},{"path":"/reference/print.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"","code":"# S3 method for BayesMallows print(x, ...)"},{"path":"/reference/print.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"x object type BayesMallows, returned compute_mallows. ... arguments passed print (used).","code":""},{"path":[]},{"path":"/reference/print.BayesMallowsMixtures.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for BayesMallowsMixtures Objects — print.BayesMallowsMixtures","title":"Print Method for BayesMallowsMixtures Objects — print.BayesMallowsMixtures","text":"default print method BayesMallowsMixtures object.","code":""},{"path":"/reference/print.BayesMallowsMixtures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for BayesMallowsMixtures Objects — print.BayesMallowsMixtures","text":"","code":"# S3 method for BayesMallowsMixtures print(x, ...)"},{"path":"/reference/print.BayesMallowsMixtures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for BayesMallowsMixtures Objects — print.BayesMallowsMixtures","text":"x object type BayesMallowsMixtures, returned compute_mallows_mixtures. ... arguments passed print (used).","code":""},{"path":[]},{"path":"/reference/rank_conversion.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert between ranking and ordering. — rank_conversion","title":"Convert between ranking and ordering. — rank_conversion","text":"create_ranking takes vector matrix ordered items orderings returns corresponding vector matrix ranked items. create_ordering takes vector matrix rankings rankings returns corresponding vector matrix ordered items.","code":""},{"path":"/reference/rank_conversion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert between ranking and ordering. — rank_conversion","text":"","code":"create_ranking(orderings)  create_ordering(rankings)"},{"path":"/reference/rank_conversion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert between ranking and ordering. — rank_conversion","text":"orderings vector matrix ordered items. matrix, size N times n, N number samples n number items. rankings vector matrix ranked items. matrix, N times n, N number samples n number items.","code":""},{"path":"/reference/rank_conversion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert between ranking and ordering. — rank_conversion","text":"vector matrix rankings. Missing orderings coded NA propagated corresponding missing ranks vice versa.","code":""},{"path":"/reference/rank_conversion.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Convert between ranking and ordering. — rank_conversion","text":"create_ranking(): Convert ordering ranking. create_ordering(): Convert ranking ordering.","code":""},{"path":[]},{"path":"/reference/rank_conversion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert between ranking and ordering. — rank_conversion","text":"","code":"# A vector of ordered items. orderings <- c(5, 1, 2, 4, 3) # Get ranks rankings <- create_ranking(orderings) # rankings is c(2, 3, 5, 4, 1) # Finally we convert it backed to an ordering. orderings_2 <- create_ordering(rankings) # Confirm that we get back what we had all.equal(orderings, orderings_2) #> [1] TRUE  # Next, we have a matrix with N = 19 samples # and n = 4 items set.seed(21) N <- 10 n <- 4 orderings <- t(replicate(N, sample.int(n))) # Convert the ordering to ranking rankings <- create_ranking(orderings) # Now we try to convert it back to an ordering. orderings_2 <- create_ordering(rankings) # Confirm that we get back what we had all.equal(orderings, orderings_2) #> [1] TRUE"},{"path":"/reference/rank_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Distance between a set of rankings and a given rank sequence — rank_distance","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"Compute distance matrix rankings rank   sequence.","code":""},{"path":"/reference/rank_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"","code":"rank_distance(rankings, rho, metric, obs_freq = 1)"},{"path":"/reference/rank_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"rankings matrix size \\(N \\)\\(\\times\\)\\( n_items\\) rankings row. Alternatively, \\(N\\) equals 1, rankings can vector. rho ranking sequence. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\", \"footrule\" \"spearman\". obs_freq Vector observation frequencies length \\(N\\), length 1, means ranks given weight. Defaults 1.","code":""},{"path":"/reference/rank_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"vector distances according given metric.","code":""},{"path":"/reference/rank_distance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"implementation Cayley distance based C++ translation Rankcluster::distCayley (Grimonprez Jacques 2016) .","code":""},{"path":"/reference/rank_distance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"Grimonprez Q, Jacques J (2016). Rankcluster: Model-Based Clustering Multivariate Partial Ranking Data. R package version 0.94, https://CRAN.R-project.org/package=Rankcluster.","code":""},{"path":[]},{"path":"/reference/rank_distance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Distance between a set of rankings and a given rank sequence — rank_distance","text":"","code":"# Distance between two vectors of rankings: rank_distance(1:5, 5:1, metric = \"kendall\") #> [1] 10 rank_distance(c(2, 4, 3, 6, 1, 7, 5), c(3, 5, 4, 7, 6, 2, 1), metric = \"cayley\") #> [1] 6 rank_distance(c(4, 2, 3, 1), c(3, 4, 1, 2), metric = \"hamming\") #> [1] 4 rank_distance(c(1, 3, 5, 7, 9, 8, 6, 4, 2), c(1, 2, 3, 4, 9, 8, 7, 6, 5), \"ulam\") #> [1] 4 rank_distance(c(8, 7, 1, 2, 6, 5, 3, 4), c(1, 2, 8, 7, 3, 4, 6, 5), \"footrule\") #> [1] 32 rank_distance(c(1, 6, 2, 5, 3, 4), c(4, 3, 5, 2, 6, 1), \"spearman\") #> [1] 54  # Difference between a metric and a vector # We set the burn-in and thinning too low for the example to run fast data0 <- sample_mallows(rho0 = 1:10, alpha = 20, n_samples = 1000,                         burnin = 10, thinning = 1)  rank_distance(rankings = data0, rho = 1:10, metric = \"kendall\") #>    [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>   [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>   [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 #>  [186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 #>  [223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 #>  [371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 #>  [408] 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 #>  [445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 #>  [482] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [519] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [667] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 #>  [704] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 #>  [741] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [778] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 #>  [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #> [1000] 0"},{"path":"/reference/rank_freq_distr.html","id":null,"dir":"Reference","previous_headings":"","what":"Frequency distribution of the ranking sequences — rank_freq_distr","title":"Frequency distribution of the ranking sequences — rank_freq_distr","text":"Construct frequency distribution distinct ranking   sequences dataset individual rankings. can   interest , also used speed computation providing   obs_freq argument compute_mallows.","code":""},{"path":"/reference/rank_freq_distr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Frequency distribution of the ranking sequences — rank_freq_distr","text":"","code":"rank_freq_distr(rankings)"},{"path":"/reference/rank_freq_distr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Frequency distribution of the ranking sequences — rank_freq_distr","text":"rankings matrix individual rankings row.","code":""},{"path":"/reference/rank_freq_distr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Frequency distribution of the ranking sequences — rank_freq_distr","text":"Numeric matrix distinct rankings row   corresponding frequencies indicated last (n_items+1)-th   column.","code":""},{"path":[]},{"path":"/reference/rank_freq_distr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Frequency distribution of the ranking sequences — rank_freq_distr","text":"","code":"# Create example data. We set the burn-in and thinning very low # for the sampling to go fast data0 <- sample_mallows(rho0 = 1:5, alpha = 10, n_samples = 1000,                         burnin = 10, thinning = 1) # Find the frequency distribution rank_freq_distr(rankings = data0) #>                freq #> [1,] 1 2 3 4 5  927 #> [2,] 1 2 3 5 4   24 #> [3,] 1 2 4 3 5   15 #> [4,] 1 3 2 4 5    9 #> [5,] 2 1 3 4 5   25"},{"path":"/reference/rmallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Mallows distribution. — rmallows","title":"Sample from the Mallows distribution. — rmallows","text":"Sample Mallows distribution arbitrary distance metric using Metropolis-Hastings algorithm.","code":""},{"path":"/reference/rmallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Mallows distribution. — rmallows","text":"","code":"rmallows(   rho0,   alpha0,   n_samples,   burnin,   thinning,   leap_size = 1L,   metric = \"footrule\" )"},{"path":"/reference/rmallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Mallows distribution. — rmallows","text":"rho0 Vector specifying latent consensus ranking. alpha0 Scalar specifying scale parameter. n_samples Integer specifying number random samples generate. burnin Integer specifying number iterations discard burn-. thinning Integer specifying number MCMC iterations perform time random rank vector sampled. leap_size Integer specifying step size leap--shift proposal distribution. metric Character string specifying distance measure use. Available options \"footrule\" (default), \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\".","code":""},{"path":"/reference/rmallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample from the Mallows distribution. — rmallows","text":"references Rd macro \\insertAllCites help page.","code":""},{"path":"/reference/run_mcmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Worker function for computing the posterior distribution. — run_mcmc","title":"Worker function for computing the posterior distribution. — run_mcmc","text":"Worker function computing posterior distribution.","code":""},{"path":"/reference/run_mcmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Worker function for computing the posterior distribution. — run_mcmc","text":"","code":"run_mcmc(   rankings,   obs_freq,   nmc,   constraints,   cardinalities,   logz_estimate,   rho_init,   metric = \"footrule\",   error_model = \"none\",   Lswap = 1L,   n_clusters = 1L,   include_wcd = FALSE,   leap_size = 1L,   alpha_prop_sd = 0.5,   alpha_init = 5,   alpha_jump = 1L,   lambda = 0.1,   alpha_max = 1e+06,   psi = 10L,   rho_thinning = 1L,   aug_thinning = 1L,   clus_thin = 1L,   save_aug = FALSE,   verbose = FALSE,   kappa_1 = 1,   kappa_2 = 1,   save_ind_clus = FALSE )"},{"path":"/reference/run_mcmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Worker function for computing the posterior distribution. — run_mcmc","text":"rankings set complete rankings, one sample per column. n_assessors samples n_items items, rankings n_items x n_assessors. obs_freq vector observation frequencies (weights) apply rankings. nmc Number Monte Carlo samples. constraints List lists lists, returned `generate_constraints`. cardinalities Used metric equals \"footrule\" \"spearman\" computing partition function. Defaults R_NilValue. logz_estimate Estimate log partition function. metric distance metric use. One \"spearman\", \"footrule\", \"kendall\", \"cayley\", \"hamming\". error_model Error model use. Lswap Swap parameter used Swap proposal proposing rank augmentations case non-transitive pairwise comparisons. n_clusters Number clusters. Defaults 1. include_wcd Boolean defining whether store within-cluster distance. leap_size Leap--shift step size. alpha_prop_sd Standard deviation proposal distribution alpha. alpha_init Initial value alpha. alpha_jump many times sample rho time sample alpha. Setting alpha_jump high number can significantly speed computation time, since expensive computation partition function. lambda Parameter prior distribution. alpha_max Maximum value alpha, used truncating exponential prior distribution. psi Hyperparameter Dirichlet prior distribution used clustering. rho_thinning Thinning parameter. Keep every rho_thinning rank sample posterior distribution. aug_thinning Integer specifying thinning data augmentation. clus_thin Integer specifying thinning saving cluster assignments. save_aug Whether save augmented data every aug_thinningth iteration. verbose Logical specifying whether print progress Metropolis-Hastings algorithm. TRUE, notification printed every 1000th iteration. kappa_1 Hyperparameter \\(theta\\) Bernoulli error model. Defaults 1.0. kappa_2 Hyperparameter \\(theta\\) Bernoulli error model. Defaults 1.0. save_ind_clus Whether save individual cluster probabilities step, thinned specified argument clus_thin. results csv files cluster_probs1.csv, cluster_probs2.csv, ..., saved calling directory. option may slow code considerably, necessary detecting label switching using Stephen's algorithm.","code":""},{"path":"/reference/sample_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"A synthetic 3D matrix generated using the sample_mallows function — sample_dataset","title":"A synthetic 3D matrix generated using the sample_mallows function — sample_dataset","text":"synthetic 3D matrix (n_users, n_items, Time) generated using sample_mallows function. test datasets used run SMC-Mallows framework cases know users system original ranking information partial rankings. However point time, observe extra information existing user form rank item previously known (NA). datasets contrived first time step (sample_dataset[, , 1]) observed top m / 2 items user, m number items ranking. , increase time, observe next top ranked item one user time, next top ranked item, complete dataset sample_dataset[, , Time].","code":""},{"path":"/reference/sample_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A synthetic 3D matrix generated using the sample_mallows function — sample_dataset","text":"","code":"sample_dataset"},{"path":"/reference/sample_dataset.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A synthetic 3D matrix generated using the sample_mallows function — sample_dataset","text":"object class array dimension 10 x 6 x 31.","code":""},{"path":"/reference/sample_dataset.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A synthetic 3D matrix generated using the sample_mallows function — sample_dataset","text":"https://github.com/anjastein/SMC-Mallows/tree/main/data","code":""},{"path":[]},{"path":"/reference/sample_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Samples from the Mallows Rank Model — sample_mallows","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"Generate random samples Mallows Rank Model (Mallows 1957)  consensus ranking \\(\\rho\\) scale parameter \\(\\alpha\\). samples obtained running Metropolis-Hastings algorithm described Appendix C Vitelli et al. (2018) .","code":""},{"path":"/reference/sample_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"","code":"sample_mallows(   rho0,   alpha0,   n_samples,   leap_size = max(1L, floor(n_items/5)),   metric = \"footrule\",   diagnostic = FALSE,   burnin = ifelse(diagnostic, 0, 1000),   thinning = ifelse(diagnostic, 1, 1000),   items_to_plot = NULL,   max_lag = 1000L )"},{"path":"/reference/sample_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"rho0 Vector specifying latent consensus ranking Mallows rank model. alpha0 Scalar specifying scale parameter Mallows rank model. n_samples Integer specifying number random samples generate. diagnostic = TRUE, number must larger 1. leap_size Integer specifying step size leap--shift proposal distribution. metric Character string specifying distance measure use. Available options \"footrule\" (default), \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". See also rmm function PerMallows package (Irurozki et al. 2016)  sampling Mallows model Cayley, Hamming, Kendall, Ulam distances. diagnostic Logical specifying whether output convergence diagnostics. TRUE, diagnostic plot printed, together returned samples. burnin Integer specifying number iterations discard burn-. Defaults 1000 diagnostic = FALSE, else 0. thinning Integer specifying number MCMC iterations perform time random rank vector sampled. Defaults 1000 diagnostic = FALSE, else 1. items_to_plot Integer vector used diagnostic = TRUE, order specify items plot diagnostic output. provided, 5 items picked random. max_lag Integer specifying maximum lag use computation autocorrelation. Defaults 1000L. argument passed stats::acf. used diagnostic = TRUE.","code":""},{"path":"/reference/sample_mallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"Irurozki E, Calvo B, Lozano JA (2016). “PerMallows: R Package Mallows Generalized Mallows Models.” Journal Statistical Software, 71(12), 1--30. doi:10.18637/jss.v071.i12 . Mallows CL (1957). “Non-Null Ranking Models. .” Biometrika, 44(1/2), 114--130. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/sample_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"","code":"# Sample 100 random rankings from a Mallows distribution with footrule distance set.seed(1) # Number of items n_items <- 15 # Set the consensus ranking rho0 <- seq(from = 1, to = n_items, by = 1) # Set the scale alpha0 <- 10 # Number of samples n_samples <- 100 # We first do a diagnostic run, to find the thinning and burnin to use # We set n_samples to 1000, in order to run 1000 diagnostic iterations. test <- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,                        n_samples = 1000, burnin = 1, thinning = 1) #> Items not provided by user. Picking 5 at random.  # When items_to_plot is not set, 5 items are picked at random. We can change this. # We can also reduce the number of lags computed in the autocorrelation plots test <- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,                        n_samples = 1000, burnin = 1, thinning = 1,                        items_to_plot = c(1:3, 10, 15), max_lag = 500)  # From the autocorrelation plot, it looks like we should use # a thinning of at least 200. We set thinning = 1000 to be safe, # since the algorithm in any case is fast. The Markov Chain # seems to mix quickly, but we set the burnin to 1000 to be safe. # We now run sample_mallows again, to get the 100 samples we want: samples <- sample_mallows(rho0 = rho0, alpha0 = alpha0, n_samples = 100,                           burnin = 1000, thinning = 1000) # The samples matrix now contains 100 rows with rankings of 15 items. # A good diagnostic, in order to confirm that burnin and thinning are set high # enough, is to run compute_mallows on the samples model_fit <- compute_mallows(samples, nmc = 10000) # The highest posterior density interval covers alpha0 = 10. compute_posterior_intervals(model_fit, burnin = 2000, parameter = \"alpha\") #>   parameter mean median conf_level           hpdi central_interval #> 1     alpha 9.84   9.83       95 % [9.338,10.311]   [9.372,10.353]"},{"path":"/reference/smc_mallows_new_item_rank.html","id":null,"dir":"Reference","previous_headings":"","what":"SMC-Mallows new item rank — smc_mallows_new_item_rank","title":"SMC-Mallows new item rank — smc_mallows_new_item_rank","text":"Function perform resample-move SMC algorithm receive new item ranks existing user time step. correction augmentation done filling missing item ranks using pseudolikelihood augmentation.","code":""},{"path":"/reference/smc_mallows_new_item_rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SMC-Mallows new item rank — smc_mallows_new_item_rank","text":"","code":"smc_mallows_new_item_rank(   n_items,   R_obs,   N,   Time,   logz_estimate,   cardinalities,   mcmc_kernel_app,   aug_rankings_init = NULL,   rho_samples_init = NULL,   alpha_samples_init = 0L,   alpha = 0,   alpha_prop_sd = 0.5,   lambda = 0.1,   alpha_max = 1e+06,   aug_method = \"random\",   verbose = FALSE,   alpha_fixed = FALSE,   metric = \"footrule\",   leap_size = 1L )"},{"path":"/reference/smc_mallows_new_item_rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SMC-Mallows new item rank — smc_mallows_new_item_rank","text":"n_items Integer number items ranking R_obs 3D matrix size n_assessors n_items Time containing set observed rankings Time time steps N Integer specifying number particles Time Integer specifying number time steps SMC algorithm logz_estimate Estimate partition function, computed estimate_partition_function. cardinalities Cardinalities exact computation partition function, returned prepare_partition_function. mcmc_kernel_app Integer value number applications apply MCMC move kernel aug_rankings_init Initial values augmented rankings. rho_samples_init Initial values rho samples. alpha_samples_init Initial values alpha samples. alpha numeric value scale parameter. alpha_prop_sd Numeric value standard deviation prior distribution alpha lambda Strictly positive numeric value specifying rate parameter truncated exponential prior distribution alpha. alpha_max Maximum value alpha truncated exponential prior distribution. aug_method character string specifying approach filling missing data, options \"pseudolikelihood\" \"random\" verbose Logical specifying whether print progress SMC-Mallows algorithm. Defaults FALSE. alpha_fixed Logical indicating whether sample alpha . metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". leap_size leap_size Integer specifying step size leap--shift proposal distribution","code":""},{"path":"/reference/smc_mallows_new_item_rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SMC-Mallows new item rank — smc_mallows_new_item_rank","text":"3d matrix containing: samples : rho, alpha augmented rankings, effective sample size iteration SMC algorithm.","code":""},{"path":[]},{"path":"/reference/smc_mallows_new_users.html","id":null,"dir":"Reference","previous_headings":"","what":"SMC-Mallows New Users — smc_mallows_new_users","title":"SMC-Mallows New Users — smc_mallows_new_users","text":"Function perform resample-move SMC algorithm receive new users complete rankings time step. See Chapter 4 (Stein 2023)","code":""},{"path":"/reference/smc_mallows_new_users.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SMC-Mallows New Users — smc_mallows_new_users","text":"","code":"smc_mallows_new_users(   R_obs,   type,   n_items,   N,   Time,   mcmc_kernel_app,   num_new_obs,   alpha_prop_sd = 0.5,   lambda = 0.1,   alpha_max = 1e+06,   alpha = 0,   aug_method = \"random\",   logz_estimate = NULL,   cardinalities = NULL,   verbose = FALSE,   metric = \"footnote\",   leap_size = 1L )"},{"path":"/reference/smc_mallows_new_users.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SMC-Mallows New Users — smc_mallows_new_users","text":"R_obs Matrix containing full set observed rankings size n_assessors n_items type One \"complete\", \"partial\", \"partial_alpha_fixed\". n_items Integer number items ranking N Integer specifying number particles Time Integer specifying number time steps SMC algorithm mcmc_kernel_app Integer value number applications apply MCMC move kernel num_new_obs Integer value number new observations (complete rankings) time step alpha_prop_sd Numeric value specifying standard deviation lognormal proposal distribution used \\(\\alpha\\) Metropolis-Hastings algorithm. Defaults 0.1. lambda Strictly positive numeric value specifying rate parameter truncated exponential prior distribution \\(\\alpha\\). Defaults 0.1. n_cluster > 1, mixture component \\(\\alpha_{c}\\) prior distribution. alpha_max Maximum value alpha truncated exponential prior distribution. alpha numeric value scale parameter known fixed. aug_method character string specifying approach filling missing data, options \"pseudolikelihood\" \"random\". logz_estimate Estimate partition function, computed estimate_partition_function. cardinalities Cardinalities exact evaluation partition function, returned prepare_partition_function. verbose Logical specifying whether print progress SMC-Mallows algorithm. Defaults FALSE. metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". leap_size leap_size Integer specifying step size leap--shift proposal distribution","code":""},{"path":"/reference/smc_mallows_new_users.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SMC-Mallows New Users — smc_mallows_new_users","text":"set particles containing value rho alpha","code":""},{"path":[]},{"path":"/reference/smc_mallows_new_users.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SMC-Mallows New Users — smc_mallows_new_users","text":"","code":"# Generate basic elements data <- sushi_rankings[1:100, ] n_items <- ncol(sushi_rankings) metric <- \"footrule\" num_new_obs <- 10  # Prepare exact partition function cardinalities <- prepare_partition_function(metric = metric,                                             n_items = n_items)$cardinalities  # Calculating rho and alpha samples samples <- smc_mallows_new_users(   R_obs = data, type = \"complete\", n_items = n_items, metric = metric,   leap_size = floor(n_items / 5), N = 100, Time = nrow(data) / num_new_obs,   mcmc_kernel_app = 5, cardinalities = cardinalities,   alpha_prop_sd = 0.1, lambda = 0.001, alpha_max = 1e6,   num_new_obs = num_new_obs, verbose = TRUE )  # Studying the structure of the output str(samples) #> List of 4 #>  $ rho_samples       : num [1:100, 1:10, 1:11] 3 10 3 10 10 10 2 10 5 10 ... #>  $ alpha_samples     : num [1:100, 1:11] 0.205 1.969 2.554 0.311 1.462 ... #>  $ augmented_rankings: num[0 , 0 , 0 ]  #>  $ ESS               : num [1, 1:10] 10.74 3.13 14.56 9.47 28.28 ... #>  - attr(*, \"class\")= chr \"SMCMallows\""},{"path":"/reference/sushi_rankings.html","id":null,"dir":"Reference","previous_headings":"","what":"Sushi Rankings — sushi_rankings","title":"Sushi Rankings — sushi_rankings","text":"Complete rankings 10 types sushi 5000 assessors (Kamishima 2003) .","code":""},{"path":"/reference/sushi_rankings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sushi Rankings — sushi_rankings","text":"","code":"sushi_rankings"},{"path":"/reference/sushi_rankings.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sushi Rankings — sushi_rankings","text":"object class matrix (inherits array) 5000 rows 10 columns.","code":""},{"path":"/reference/sushi_rankings.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sushi Rankings — sushi_rankings","text":"Kamishima T (2003). “Nantonac Collaborative Filtering: Recommendation Based Order Responses.” Proceedings Ninth ACM SIGKDD International Conference Knowledge Discovery Data Mining, 583--588.","code":""},{"path":[]},{"path":"/news/index.html","id":"bayesmallows-150","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.5.0","title":"BayesMallows 1.5.0","text":"Bug augmentation intransitive pairwise preferences fixed. (https://github.com/ocbe-uio/BayesMallows/issues/345) Bug plot.BayesMallows posterior distribution ‘parameter = “rho”’ fixed. Thanks Lorenzo Zuccato points issue. (https://github.com/ocbe-uio/BayesMallows/issues/342) Argument obs_freq internal function rmallows() removed, used. Thanks Lorenzo Zuccato pointing (https://github.com/ocbe-uio/BayesMallows/issues/337). Argument save_clus compute_mallows() removed, used. compute_mallows() now supports parallel chains, providing ‘cl’ argument. See vignette “MCMC Parallel Chains” tutorial. compute_rho_consensus() SMC Mallows deprecated favor compute_consensus(). compute_posterior_intervals_rho() compute_posterior_intervals_alpha() SMC Mallows deprecated factor compute_posterior_intervals() argument parameter = “rho” parameter = “alpha”. Documentation functions now grouped families. lik_db_mix() now deprecated favor get_mallows_loglik() Unusued argument removed internal function augment_pairwise(). Thanks Lorenzo Zuccato making us aware (https://github.com/ocbe-uio/BayesMallows/issues/313).","code":""},{"path":"/news/index.html","id":"bayesmallows-140","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.4.0","title":"BayesMallows 1.4.0","text":"CRAN release: 2023-10-04 Bug fix: psi argument compute_mallows() compute_mallows_mixtures(), specifying concentration parameter Dirichlet prior, now forwarded underlying run_mcmc() function. Previously, argument effect, default psi=10 used regardless input. Thanks Lorenzo Zuccato discovering bug. SMC functions now accept exact partition functions available. Removed SMC functions deprecated version 1.2.0 (#301) Website deployed https://ocbe-uio.github.io/BayesMallows. Reordering authors, Waldir Leoncio appears second list.","code":""},{"path":"/news/index.html","id":"bayesmallows-132","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.2","title":"BayesMallows 1.3.2","text":"CRAN release: 2023-08-24 Fixed LTO compilation notes CRAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-131","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.1","title":"BayesMallows 1.3.1","text":"CRAN release: 2023-08-21 Fixed package documentation issue CRAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-130","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.0","title":"BayesMallows 1.3.0","text":"CRAN release: 2023-03-10 Added heat_plot() function (#255) Replaced deprecated ggplot2::aes_ function ggplot2::aes. Refactoring SMC functions (#257) Improved validation documentation SMC post-processing functions (#262)","code":""},{"path":"/news/index.html","id":"bayesmallows-122","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.2","title":"BayesMallows 1.2.2","text":"CRAN release: 2023-02-03 Added plot.SMCMallows() method Changed default values argument order several SMC functions (see PR #269) Modifications internal C++ code avoid CRAN NOTEs.","code":""},{"path":"/news/index.html","id":"bayesmallows-121","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.1","title":"BayesMallows 1.2.1","text":"CRAN release: 2022-11-04 PerMallows package removed Imports risk removed CRAN. means Ulam distance 95 items, user compute importance sampling estimate. Refactoring data augmentation function SMC Mallows. Improved documentation sample_dataset","code":""},{"path":"/news/index.html","id":"bayesmallows-120","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.0","title":"BayesMallows 1.2.0","text":"CRAN release: 2022-05-24 Fixed bug caused assess_convergence() fail ‘parameter = “cluster_probs”’. Fixed bug smc_mallows_new_users_partial() smc_mallows_new_users_partial_alpha_fixed(). metropolis_hastings_aug_ranking_pseudo() deprecated. Please use metropolis_hastings_aug_ranking() instead, pseudo=TRUE. smc_mallows_new_users_partial_alpha_fixed(), smc_mallows_new_users_complete(), smc_mallows_new_users_partial() deprecated. Please use smc_mallows_new_users() instead, set type= argument “complete”, “partial”, “partial_alpha_fixed”. smc_mallows_new_item_rank_alpha_fixed() deprecated. Please use smc_mallows_new_item_rank() instead, argument alpha_fixed=TRUE. Fixed unexpected behavior leap--shift proposal distribution SMC Mallows, causing function propose current rank vector nonzero probability. BayesMallows longer depends ‘dplyr’. Quite extensive internal refactoring C++ code. Function lik_db_mix renamed get_mallows_loglik. lik_db_mix still exists deprecated. initial rankings provided, compute_mallows() compute_mallows_mixtures() use independent initial rho cluster. Previously single initial rho used cluster. potentially improve convergence, lead different results n_clusters>=2 given random number seed.","code":""},{"path":"/news/index.html","id":"bayesmallows-112","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.2","title":"BayesMallows 1.1.2","text":"CRAN release: 2022-04-11 Fixed issue stats::reshape causing error R-oldrel. Fixed issue checking class objects, now consistently use inherits(). Internal C++ fixes comply CRAN checks.","code":""},{"path":"/news/index.html","id":"bayesmallows-111","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.1","title":"BayesMallows 1.1.1","text":"CRAN release: 2022-04-01 Fixed C++ errors leading CRAN issues.","code":""},{"path":"/news/index.html","id":"bayesmallows-110","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.0","title":"BayesMallows 1.1.0","text":"CRAN release: 2021-12-03 Major update, introducing whole new class methods using sequential Monte Carlo. Also reducing number dependencies.","code":""},{"path":"/news/index.html","id":"bayesmallows-1049001","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4.9001","title":"BayesMallows 1.0.4.9001","text":"major update, new functions estimating Bayesian Mallows model using sequential Monte Carlo. methods described vignette titled “SMC-Mallows Tutorial”.","code":""},{"path":"/news/index.html","id":"bayesmallows-1049000","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4.9000","title":"BayesMallows 1.0.4.9000","text":"Removed large number dependencies converting base R code. make package easier install across range systems, less vulnerable changes packages.","code":""},{"path":"/news/index.html","id":"bayesmallows-104","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4","title":"BayesMallows 1.0.4","text":"CRAN release: 2021-11-17 Incorporates changes since 1.0.3, also remove PLMIX Imports.","code":""},{"path":"/news/index.html","id":"bayesmallows-1039001","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3.9001","title":"BayesMallows 1.0.3.9001","text":"Fixed bug caused plot_top_k fail plotting clusters. Improved default value rel_widths argument plot_top_k. Wrote unit tests check bugs don’t appear .","code":""},{"path":"/news/index.html","id":"bayesmallows-1039000","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3.9000","title":"BayesMallows 1.0.3.9000","text":"Fixed bug caused importance sampling fail running parallel. Fixed issue error message trying plot error probability compute_mallows set compute error probability. Increased number unit tests.","code":""},{"path":"/news/index.html","id":"bayesmallows-103","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3","title":"BayesMallows 1.0.3","text":"CRAN release: 2021-10-14 Fixed critical bug caused results wrong one mixture component compute_mallows() compute_mallows_mixtures(). Thanks Anja Stein discovering bug.","code":""},{"path":"/news/index.html","id":"bayesmallows-102","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.2","title":"BayesMallows 1.0.2","text":"CRAN release: 2021-06-04 Function generate_initial_ranking() now two additional options generating random initial rankings. can help convergence problems, allowing MCMC algorithm run range different starting points.","code":""},{"path":"/news/index.html","id":"bayesmallows-101","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.1","title":"BayesMallows 1.0.1","text":"CRAN release: 2021-02-23 Fixes bug lik_db_mix expected_dist, scaling parameter used different parametrization rest package. functions package now use consistent parametrization Mallows model, stated vignette.","code":""},{"path":"/news/index.html","id":"bayesmallows-050","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.5.0","title":"BayesMallows 0.5.0","text":"CRAN release: 2020-08-28 Function compute_consensus now includes option computing consensus augmented ranks.","code":""},{"path":"/news/index.html","id":"bayesmallows-044","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.4","title":"BayesMallows 0.4.4","text":"CRAN release: 2020-08-07 Fixed bug predict_top_k plot_top_k using aug_thinning > 1.","code":""},{"path":"/news/index.html","id":"bayesmallows-043","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.3","title":"BayesMallows 0.4.3","text":"CRAN release: 2020-06-20 Updated README vignette.","code":""},{"path":"/news/index.html","id":"bayesmallows-042","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.2","title":"BayesMallows 0.4.2","text":"CRAN release: 2020-03-23 Updating unit test make sure BayesMallows compatible dplyr version 1.0.0.","code":""},{"path":"/news/index.html","id":"bayesmallows-041","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.1","title":"BayesMallows 0.4.1","text":"CRAN release: 2019-09-05 Improvement plotting functions, noted .","code":""},{"path":"/news/index.html","id":"bayesmallows-0409002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9002","title":"BayesMallows 0.4.0.9002","text":"plot.BayesMallows plot_elbow longer print titles automatically.","code":""},{"path":"/news/index.html","id":"bayesmallows-0409001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9001","title":"BayesMallows 0.4.0.9001","text":"assess_convergence longer prints legends clusters, cluster number essentially arbitrary.","code":""},{"path":"/news/index.html","id":"bayesmallows-0409000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9000","title":"BayesMallows 0.4.0.9000","text":"Added CITATION. Updated test random number seed.","code":""},{"path":"/news/index.html","id":"bayesmallows-040","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0","title":"BayesMallows 0.4.0","text":"CRAN release: 2019-02-22 Implements fixes since version 0.3.1 . Fixed typo y-axis label elbow plot. Fixed issue caused cluster probabilities differ across platforms, despite using seed. https://stackoverflow.com/questions/54822702","code":""},{"path":"/news/index.html","id":"bayesmallows-0319005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9005","title":"BayesMallows 0.3.1.9005","text":"Fixed bug caused compute_mallows work (without giving errors) rankings contained missing values. Fixed bug caused compute_mallows fail preferences integer columns.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9004","title":"BayesMallows 0.3.1.9004","text":"Changed name save_individual_cluster_probs save_ind_clus, save typing.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9003","title":"BayesMallows 0.3.1.9003","text":"Added user prompt asking user really wants save csv files, save_individual_cluster_probs = TRUE compute_mallows. Added alpha_max, truncation exponential prior alpha, user option compute_mallows.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9002","title":"BayesMallows 0.3.1.9002","text":"Added functionality checking label switching. See ?label_switching info.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9001","title":"BayesMallows 0.3.1.9001","text":"internal function compute_importance_sampling_estimate updated avoid numerical overflow. Previously, importance sampling failed 200 items. Now works way 10,000 items.","code":""},{"path":"/news/index.html","id":"bayesmallows-031","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1","title":"BayesMallows 0.3.1","text":"CRAN release: 2019-02-01 update parts C++ code, avoid failing sanitizer checks clang-UBSAN gcc-UBSAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-030","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.0","title":"BayesMallows 0.3.0","text":"CRAN release: 2019-01-30 See bullet points , since 0.2.0.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209006","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9006","title":"BayesMallows 0.2.0.9006","text":"generate_transitive_closure, generate_initial_ranking, generate_constraints now able run parallel. Large changes underlying code base make maintainable affect user.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9005","title":"BayesMallows 0.2.0.9005","text":"estimate_partition_function now option run parallel, leading significant speed-.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9004","title":"BayesMallows 0.2.0.9004","text":"Implemented Bernoulli error model. Set error_model = \"bernoulli\" compute_mallows order use . Examples come later.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9003","title":"BayesMallows 0.2.0.9003","text":"Added parallelization option compute_mallows_mixtures added parallel Suggests field.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9002","title":"BayesMallows 0.2.0.9002","text":"Deprecated functions compute_cp_consensus compute_map_consensus removed. Use compute_consensus instead.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9001","title":"BayesMallows 0.2.0.9001","text":"Clusters now factor variables sorted according cluster number. Hence, plot legends, “Cluster 10” comes “Cluster 9”, rather “Cluster 1” used now, character. plot.BayesMallows longer contains print statements forces display plots. Instead plots returned function. Using p <- plot(fit) hence longer display plot, whereas using plot(fit) without assigning object, displays plot. now plot always shown rho alpha.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9000","title":"BayesMallows 0.2.0.9000","text":"compute_mallows sample_mallows now support Ulam distance, argument metric = \"ulam\". Slimmed vignette significantly, order avoid clang-UBSAN error caused running vignette (caused Rcpp, cf. issue). long vignette longer needed case, since functions well documented executable examples.","code":""},{"path":"/news/index.html","id":"bayesmallows-020","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0","title":"BayesMallows 0.2.0","text":"CRAN release: 2018-11-30 New release CRAN, contains updates 0.1.1, described .","code":""},{"path":"/news/index.html","id":"bayesmallows-0119009","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9009","title":"BayesMallows 0.1.1.9009","text":"Rankcluster package removed dependencies.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119008","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9008","title":"BayesMallows 0.1.1.9008","text":"Fixed bug Cayley distance. distance, computational shortcut p. 8 Vitelli et al. (2018), JMLR, work. However, still used. Now, Cayley distance always computed complete rank vectors. Fixed bug default argument leap_size compute_mallows. used floor(n_items / 5), evaluates zero n_items <= 4. Updated max(1L, floor(n_items / 5)). Added Hamming distance (metric = \"hamming\") option compute_mallows sample_mallows.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119007","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9007","title":"BayesMallows 0.1.1.9007","text":"Updated generate_initial_ranking, generate_transitive_closure, sample_mallows avoid errors package tibble version 2.0.0 released. update purely internal.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119006","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9006","title":"BayesMallows 0.1.1.9006","text":"Objects class BayesMallows BayesMallowsMixtures now default print functions, hence avoiding excessive amounts informations printed console user happens write name object press Return. compute_mallows_mixtures longer sets include_wcd = TRUE default. user can choose argument. compute_mallows new argument save_clus, can set FALSE saving cluster assignments.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9005","title":"BayesMallows 0.1.1.9005","text":"assess_convergence now automatically plots mixtures. compute_mallows_mixtures now returns object class BayesMallowsMixtures.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9004","title":"BayesMallows 0.1.1.9004","text":"assess_convergence now adds prefix Assessor plots parameter = \"Rtilde\". predict_top_k now exported function. Previously internal.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9003","title":"BayesMallows 0.1.1.9003","text":"compute_posterior_intervals now default parameter = \"alpha\". now, argument default. Argument type plot.BayesMallows assess_convergence renamed parameter, consistent.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9002","title":"BayesMallows 0.1.1.9002","text":"Argument save_augment_data compute_mallows renamed save_aug. compute_mallows fills implied ranks assessor one missing rank. avoids unnecessary augmentation MCMC. generate_ranking generate_ordering now work missing ranks.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9001","title":"BayesMallows 0.1.1.9001","text":"Argument cluster_assignment_thinning compute_mallows renamed clus_thin.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9000","title":"BayesMallows 0.1.1.9000","text":"Change interface computing consensus ranking. Now, CP MAP consensus computed compute_consensus function, argument type equal either \"CP\" \"MAP\".","code":""}]
