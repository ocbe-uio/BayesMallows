[{"path":"/articles/BayesMallows.html","id":"analysis-of-complete-rankings","dir":"Articles","previous_headings":"","what":"Analysis of complete rankings","title":"Introduction","text":"illustrate case complete rankings potato datasets described Section 4 (Liu et al. 2019). short, bag 20 potatoes bought, 12 assessors asked rank potatoes weight, first visual inspection, next holding potatoes hand. datasets available BayesMallows matrices names potato_weighing potato_visual, respectively. true ranking potatoes’ weights available vector potato_true_ranking. general, compute_mallows expects ranking datasets one row assessor one column item. row proper permutation, possibly missing values. interested posterior distribution level agreement assessors, described \\(\\alpha\\), latent ranking potatoes, described \\(\\boldsymbol{\\rho}\\). refer attached replication script random number seeds exact reproducibility. start defining data object, case consists complete rankings. First, test run check convergence MCMC algorithm, get trace plots assess_convergence. default, assess_convergence returns trace plot \\(\\alpha\\), shown figure . algorithm seems mixing well around 500 iterations. Next, study convergence \\(\\mathbf{\\rho}\\). avoid overly complex plots, pick potatoes \\(1-5\\) specifying items argument. plot shows MCMC algorithm seems converged around 1,000 iterations. trace plots, decide discard first 1,000 MCMC samples burn-. rerun algorithm get 20,000 samples burn-. object bmm_visual S3 class BayesMallows, plot posterior distribution \\(\\alpha\\) plot.BayesMallows. can also get posterior credible intervals \\(\\alpha\\) using compute_posterior_intervals, returns highest posterior density intervals (HPDI) central intervals data.frame. Next, can go study posterior distribution \\(\\boldsymbol{\\rho}\\). argument provided, number items exceeds five, five items picked random plotting. show potatoes, explicitly set .","code":"complete_data <- setup_rank_data(rankings = potato_visual) bmm_test <- compute_mallows(data = complete_data) assess_convergence(bmm_test) assess_convergence(bmm_test, parameter = \"rho\", items = 1:5) bmm_visual <- compute_mallows(   data = complete_data,    compute_options = set_compute_options(nmc = 21000, burnin = 1000)   ) plot(bmm_visual) compute_posterior_intervals(bmm_visual, decimals = 1L) #>   parameter mean median       hpdi central_interval #> 1     alpha 10.9   10.9 [9.5,12.3]       [9.5,12.3] plot(bmm_visual, parameter = \"rho\", items = 1:20)"},{"path":"/articles/BayesMallows.html","id":"jumping-over-the-scale-parameter","dir":"Articles","previous_headings":"Analysis of complete rankings","what":"Jumping over the scale parameter","title":"Introduction","text":"Updating \\(\\alpha\\) every step MCMC algorithm may necessary, number posterior samples typically large enough obtain good estimates posterior distribution. alpha_jump argument, can tell MCMC algorithm update \\(\\alpha\\) every alpha_jump-th iteration. update \\(\\alpha\\) every 10th time \\(\\boldsymbol{\\rho}\\) updated, ","code":"bmm_visual <- compute_mallows(   data = complete_data,    compute_options =      set_compute_options(nmc = 21000, burnin = 1000, alpha_jump = 10)   )"},{"path":"/articles/BayesMallows.html","id":"other-distance-metric","dir":"Articles","previous_headings":"Analysis of complete rankings","what":"Other distance metric","title":"Introduction","text":"default, compute_mallows uses footrule distance, user can also choose use Cayley, Kendall, Hamming, Spearman, Ulam distance. Running analysis potato data Spearman distance done command particular case Spearman distance, BayesMallows integer sequences computing exact partition function 14 fewer items. case precomputed importance sampling estimate part package, used instead.","code":"bmm <- compute_mallows(   data = complete_data,    model_options = set_model_options(metric = \"spearman\"),   compute_options = set_compute_options(nmc = 21000, burnin = 1000) )"},{"path":"/articles/BayesMallows.html","id":"analysis-of-preference-data","dir":"Articles","previous_headings":"","what":"Analysis of preference data","title":"Introduction","text":"Unless argument error_model set_model_options set, pairwise preference data assumed consistent within assessor. data provided dataframe following three columns, one row per pairwise comparison: assessor identifier assessor; either numeric vector containing assessor index, character vector containing unique name assessor. bottom_item numeric vector containing index item disfavored pairwise comparison. top_item numeric vector containing index item preferred pairwise comparison. dataframe structure can given preferences argument setup_rank_data, generate full set implied rankings assessor well initial ranking matrix consistent pairwise preferences. illustrate beach preference data containing stated pairwise preferences random subsets 15 images beaches, 60 assessors (Vitelli et al. 2018). dataset provided dataframe beach_preferences, whose first six rows shown : can define rank data object based preferences. instructive compare computed transitive closure stated preferences. Let’s preferences stated assessor 1 involving beach 2. first look raw preferences. use function get_transitive_closure obtain transitive closure, focus subset: Assessor 1 performed one direct comparison involving beach 2, assessor stated beach 15 preferred beach 2. implied orderings, hand, contain two preferences involving beach 2. addition statement beach 15 preferred beach 2, orderings stated assessor 1 imply assessor prefers beach 6 beach 2.","code":"head(beach_preferences) #>   assessor bottom_item top_item #> 1        1           2       15 #> 2        1           5        3 #> 3        1          13        3 #> 4        1           4        7 #> 5        1           5       15 #> 6        1          12        6 beach_data <- setup_rank_data(preferences = beach_preferences) subset(beach_preferences, assessor == 1 & (bottom_item == 2 | top_item == 2)) #>   assessor bottom_item top_item #> 1        1           2       15 tc <- get_transitive_closure(beach_data) subset(tc, assessor == 1 & (bottom_item == 2 | top_item == 2)) #>    assessor bottom_item top_item #> 11        1           2        6 #> 44        1           2       15"},{"path":"/articles/BayesMallows.html","id":"convergence-diagnostics","dir":"Articles","previous_headings":"Analysis of preference data","what":"Convergence diagnostics","title":"Introduction","text":"potato data, can test run assess convergence MCMC algorithm. time use beach_data object generated , based stated preferences. also set save_aug = TRUE save augmented rankings MCMC step, hence letting us assess convergence augmented rankings. Running assess_convergence \\(\\alpha\\) \\(\\boldsymbol{\\rho}\\) shows good convergence 1000 iterations. check convergence data augmentation scheme, need set parameter = \"Rtilde\", also specify items assessors plot. Let us start considering items 2, 6, 15 assessor 1, studied . convergence plot illustrates augmented rankings vary, also obeying implied ordering. investigation transitive closure, find orderings implied beach 1 beach 15 assessor 2. , following statement returns zero rows. following command, create trace plots confirm : expected, traces augmented rankings beach 1 15 assessor 2 cross , since ordering implied . Ideally, look trace plots augmented ranks assessors sure algorithm close convergence. can plot assessors 1-8 setting assessors = 1:8. also quite arbitrarily pick items 13-15, procedure can repeated items. plot indicates good mixing.","code":"bmm_test <- compute_mallows(   data = beach_data,   compute_options = set_compute_options(save_aug = TRUE)) assess_convergence(bmm_test) assess_convergence(bmm_test, parameter = \"rho\", items = 1:6) assess_convergence(   bmm_test, parameter = \"Rtilde\", items = c(2, 6, 15), assessors = 1) subset(tc, assessor == 2 & bottom_item %in% c(1, 15) & top_item %in% c(1, 15)) #> [1] assessor    bottom_item top_item    #> <0 rows> (or 0-length row.names) assess_convergence(   bmm_test, parameter = \"Rtilde\", items = c(1, 15), assessors = 2) assess_convergence(   bmm_test, parameter = \"Rtilde\", items = 13:15, assessors = 1:8)"},{"path":"/articles/BayesMallows.html","id":"posterior-distributions","dir":"Articles","previous_headings":"Analysis of preference data","what":"Posterior distributions","title":"Introduction","text":"Based convergence diagnostics, fairly conservative, discard first 2,000 MCMC iterations burn-, take 20,000 additional samples. posterior distributions \\(\\alpha\\) \\(\\boldsymbol{\\rho}\\) can studied shown previous sections. Posterior intervals latent rankings beach obtained compute_posterior_intervals: can also rank beaches according cumulative probability (CP) consensus (Vitelli et al. 2018) maximum posterior (MAP) rankings. done function compute_consensus, following call returns CP consensus: column cumprob shows probability given rank lower. Looking second row, example, means beach 6 probability 1 latent rank \\(\\rho_{6} \\leq 2\\). Next, beach 3 probability 0.738 latent rank \\(\\rho_{3}\\leq 3\\). example Bayesian framework can used rank items, also give posterior assessments uncertainty rankings. MAP consensus obtained similarly, setting type = \"MAP\". Keeping mind ranking beaches based sparse pairwise preferences, can also ask: beach \\(\\), probability ranked top-\\(k\\) assessor \\(j\\), probability latent rank among top-\\(k\\). function plot_top_k plots probabilities. default, sets k = 3, heatplot probability ranked top-3 obtained call: plot shows, beach indicated left axis, probability assessor \\(j\\) ranks beach among top-3. example, see assessor 1 low probability ranking beach 9 among top-3, assessor 3 high probability . function predict_top_k returns dataframe underlying probabilities. example, order find beaches among top-3 assessors 1-5 90 % probability, : Note assessor 2 appear table, .e., beaches least 90 % certain beach among assessor 2’s top-3.","code":"bmm_beaches <- compute_mallows(   data = beach_data,   compute_options =      set_compute_options(nmc = 22000, burnin = 2000, save_aug = TRUE) ) compute_posterior_intervals(bmm_beaches, parameter = \"rho\") #>    parameter    item mean median    hpdi central_interval #> 1        rho  Item 1    7      7     [7]              [7] #> 2        rho  Item 2   15     15    [15]             [15] #> 3        rho  Item 3    3      3   [3,4]            [3,4] #> 4        rho  Item 4   12     12 [11,13]          [11,14] #> 5        rho  Item 5    9      9  [8,10]           [8,10] #> 6        rho  Item 6    2      2   [1,2]            [1,2] #> 7        rho  Item 7    8      8   [8,9]           [8,10] #> 8        rho  Item 8   12     12 [11,13]          [11,14] #> 9        rho  Item 9    1      1   [1,2]            [1,2] #> 10       rho Item 10    6      6   [5,6]            [5,6] #> 11       rho Item 11    4      4   [3,4]            [3,5] #> 12       rho Item 12   13     13 [12,14]          [12,14] #> 13       rho Item 13   10     10  [9,10]           [9,10] #> 14       rho Item 14   13     14 [11,14]          [11,14] #> 15       rho Item 15    5      5   [4,5]            [4,6] compute_consensus(bmm_beaches, type = \"CP\") #>      cluster ranking    item cumprob #> 1  Cluster 1       1  Item 9 0.89815 #> 2  Cluster 1       2  Item 6 1.00000 #> 3  Cluster 1       3  Item 3 0.72665 #> 4  Cluster 1       4 Item 11 0.95160 #> 5  Cluster 1       5 Item 15 0.95400 #> 6  Cluster 1       6 Item 10 0.97645 #> 7  Cluster 1       7  Item 1 1.00000 #> 8  Cluster 1       8  Item 7 0.62585 #> 9  Cluster 1       9  Item 5 0.85950 #> 10 Cluster 1      10 Item 13 1.00000 #> 11 Cluster 1      11  Item 4 0.46870 #> 12 Cluster 1      12  Item 8 0.84435 #> 13 Cluster 1      13 Item 12 0.61905 #> 14 Cluster 1      14 Item 14 0.99665 #> 15 Cluster 1      15  Item 2 1.00000 compute_consensus(bmm_beaches, type = \"MAP\") #>      cluster map_ranking    item probability #> 1  Cluster 1           1  Item 9     0.04955 #> 2  Cluster 1           2  Item 6     0.04955 #> 3  Cluster 1           3  Item 3     0.04955 #> 4  Cluster 1           4 Item 11     0.04955 #> 5  Cluster 1           5 Item 15     0.04955 #> 6  Cluster 1           6 Item 10     0.04955 #> 7  Cluster 1           7  Item 1     0.04955 #> 8  Cluster 1           8  Item 7     0.04955 #> 9  Cluster 1           9  Item 5     0.04955 #> 10 Cluster 1          10 Item 13     0.04955 #> 11 Cluster 1          11  Item 4     0.04955 #> 12 Cluster 1          12  Item 8     0.04955 #> 13 Cluster 1          13 Item 14     0.04955 #> 14 Cluster 1          14 Item 12     0.04955 #> 15 Cluster 1          15  Item 2     0.04955 plot_top_k(bmm_beaches) subset(predict_top_k(bmm_beaches), prob > .9 & assessor %in% 1:5) #>     assessor    item    prob #> 301        1  Item 6 0.99435 #> 303        3  Item 6 0.99600 #> 305        5  Item 6 0.97605 #> 483        3  Item 9 1.00000 #> 484        4  Item 9 0.99975 #> 601        1 Item 11 0.95030"},{"path":"/articles/BayesMallows.html","id":"clustering","dir":"Articles","previous_headings":"","what":"Clustering","title":"Introduction","text":"BayesMallows comes set sushi preference data, 5,000 assessors ranked set 10 types sushi (Kamishima 2003). interesting see can find subsets assessors similar preferences. sushi dataset analyzed BMM Vitelli et al. (2018), results paper differ somewhat obtained , due bug function used sample cluster probabilities Dirichlet distribution. start defining data object.","code":"sushi_data <- setup_rank_data(sushi_rankings)"},{"path":"/articles/BayesMallows.html","id":"convergence-diagnostics-1","dir":"Articles","previous_headings":"Clustering","what":"Convergence diagnostics","title":"Introduction","text":"function compute_mallows_mixtures computes multiple Mallows models different numbers mixture components. returns list models class BayesMallowsMixtures, list element contains model given number mixture components. arguments n_clusters, specifies number mixture components compute, optional parameter cl can set return value makeCluster function parallel package, ellipsis (...) passing arguments compute_mallows. Hypothesizing may need 10 clusters find useful partitioning assessors, start test runs 1, 4, 7, 10 mixture components order assess convergence. set number Monte Carlo samples 5,000, since test run, save within-cluster distances MCMC iteration hence set include_wcd = FALSE. function assess_convergence automatically creates grid plot given object class BayesMallowsMixtures, can check convergence \\(\\alpha\\) command resulting plot shows chains seem close convergence quite quickly. can also make sure posterior distributions cluster probabilities \\(\\tau_{c}\\), \\((c = 1, \\dots, C)\\) converged properly, setting parameter = \"cluster_probs\". Note one cluster, cluster probability fixed value 1, number mixture components, chains seem mixing well.","code":"library(\"parallel\") cl <- makeCluster(detectCores()) bmm <- compute_mallows_mixtures(   n_clusters = c(1, 4, 7, 10),   data = sushi_data,   compute_options = set_compute_options(nmc = 5000, include_wcd = FALSE),   cl = cl) stopCluster(cl) assess_convergence(bmm) assess_convergence(bmm, parameter = \"cluster_probs\")"},{"path":"/articles/BayesMallows.html","id":"deciding-on-the-number-of-mixture-components","dir":"Articles","previous_headings":"Clustering","what":"Deciding on the number of mixture components","title":"Introduction","text":"Given convergence assessment previous section, fairly confident burn-1,000 sufficient. run 40,000 additional iterations, try 1 10 mixture components. goal now determine number mixture components use, order create elbow plot, set include_wcd = TRUE compute within-cluster distances step MCMC algorithm. Since posterior distributions \\(\\rho_{c}\\) (\\(c = 1,\\dots,C\\)) highly peaked, save memory saving every 10th value \\(\\boldsymbol{\\rho}\\) setting rho_thinning = 10. create elbow plot: Although clear-cut, see within-cluster sum distances levels around 5 clusters, hence choose use 5 clusters model.","code":"cl <- makeCluster(detectCores()) bmm <- compute_mallows_mixtures(   n_clusters = 1:10,    data = sushi_data,   compute_options =      set_compute_options(nmc = 11000, burnin = 1000,                         rho_thinning = 10, include_wcd = TRUE),   cl = cl) stopCluster(cl) plot_elbow(bmm)"},{"path":"/articles/BayesMallows.html","id":"posterior-distributions-1","dir":"Articles","previous_headings":"Clustering","what":"Posterior distributions","title":"Introduction","text":"chosen 5 mixture components, go fit final model, still running 10,000 iterations burnin. time call compute_mallows set n_clusters = 5. also set clus_thinning = 10 save cluster assignments assessor every 10th iteration, rho_thinning = 10 save estimated latent rank every 10th iteration. Note thinning done saving values every iteration result large objects stored memory, thus slowing computation. statistical efficiency, best avoid thinning. can plot posterior distributions \\(\\alpha\\) \\(\\boldsymbol{\\rho}\\) cluster using plot.BayesMallows shown previously potato data. Since five clusters, easiest way visualizing posterior rankings choosing single item. can also show posterior distributions cluster probabilities. Using argument parameter = \"cluster_assignment\", can visualize posterior probability assessor belonging cluster: number underlying plot can found using assign_cluster. can find clusterwise consensus rankings using compute_consensus. Note estimating cluster specific parameters, label switching potential problem needs handled. BayesMallows ignores label switching issues inside MCMC, shown approach better ensuring full convergence chain (Jasra, Holmes, Stephens 2005; Celeux, Hurn, Robert 2000). MCMC iterations can re-ordered convergence achieved, example using implementation Stephens’ algorithm (Stephens 2000) provided R package label.switching (Papastamoulis 2016). full example assess label switching provided examples compute_mallows function.","code":"bmm <- compute_mallows(   data = sushi_data,   model_options = set_model_options(n_cluster = 5),   compute_options = set_compute_options(     nmc = 11000, burnin = 1000, clus_thinning = 10, rho_thinning = 10) ) plot(bmm) plot(bmm, parameter = \"rho\", items = 1) plot(bmm, parameter = \"cluster_probs\") plot(bmm, parameter = \"cluster_assignment\") cp_consensus <- compute_consensus(bmm) reshape(   cp_consensus,    direction = \"wide\",    idvar = \"ranking\",   timevar = \"cluster\",   varying = list(unique(cp_consensus$cluster)),   drop = \"cumprob\"   ) #>    ranking     Cluster 1     Cluster 2     Cluster 3     Cluster 4     Cluster 5 #> 1        1        shrimp    fatty tuna    fatty tuna    sea urchin    fatty tuna #> 2        2       sea eel          tuna    salmon roe    fatty tuna    sea urchin #> 3        3         squid       sea eel    sea urchin    salmon roe          tuna #> 4        4           egg        shrimp          tuna       sea eel    salmon roe #> 5        5    fatty tuna     tuna roll        shrimp        shrimp       sea eel #> 6        6          tuna         squid     tuna roll          tuna     tuna roll #> 7        7     tuna roll           egg         squid         squid        shrimp #> 8        8 cucumber roll cucumber roll       sea eel     tuna roll         squid #> 9        9    salmon roe    salmon roe           egg           egg           egg #> 10      10    sea urchin    sea urchin cucumber roll cucumber roll cucumber roll"},{"path":[]},{"path":"/articles/SMC-Mallows.html","id":"new-users-with-complete-rankings","dir":"Articles","previous_headings":"","what":"New users with complete rankings","title":"Sequential Monte Carlo for the Bayesian Mallows model","text":"use sushi_rankings dataset illustrate methodology (Kamishima 2003). dataset contains 5000 complete rankings 10 sushi dishes. SMC methodology designed case date arrive batches. Assume initially 300 observed rankings, data_batch1: estimate model data using compute_mallows(), runs full Metropolis-Hastings algorithm. assess convergence, find 300 appropriate burnin value. Trace plot SMC model. saved model, assume receive another batch preferences later timepoint, additional 300 rankings. can now update initial model, without rerunning full Metropolis-Hastings algorithm, calling update_mallows(). function uses sequential Monte Carlo algorithm Stein (2023), extracts thinned sample size n_particles model1 initial values. posterior summary methods can used model2. example, can plot posterior \\(\\alpha\\). Posterior distribution scale parameter model 2. can plot posterior latent ranks selected items: Posterior distribution selected latent rankings model 2. Next, assume get yet another set rankings later, now size 1000. can re-update model. can plot posterior quantities, plots reveal expected, posterior uncertainty rankings decreased added data. Posterior distribution selected latent rankings model 3. Finally, add batch last data re-update model. posterior uncertainty now small: Posterior distribution selected latent rankings model 4. comparison posterior intervals dispersion parameter model. Note intervals get increasingly narrower data added. assurance implementation correct, can compare final model get running compute_mallows complete dataset: can compare posteriors \\(\\alpha\\) two models. Note although rather wiggly, agree well location scale. Posterior distribution scale parameter Metropolis-Hastings run complete data. Posterior distribution scale parameter model 4. posterior intervals also good agreement. cumulative probability consensus also good agrement:","code":"head(sushi_rankings) #>      shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll cucumber roll #> [1,]      2       8   10     3          4          1   5          9         7             6 #> [2,]      1       8    6     4         10          9   3          5         7             2 #> [3,]      2       8    3     4          6          7  10          1         5             9 #> [4,]      4       7    5     6          1          2   8          3         9            10 #> [5,]      4      10    7     5          9          3   2          8         1             6 #> [6,]      4       6    2    10          7          5   1          9         8             3 data_batch1 <- sushi_rankings[1:300, ] model1 <- compute_mallows(data = setup_rank_data(data_batch1)) assess_convergence(model1) burnin(model1) <- 300 data_batch2 <- sushi_rankings[301:600, ] model2 <- update_mallows(   model = model1,    new_data = setup_rank_data(data_batch2),    smc_options = set_smc_options(n_particles = 1000)) plot(model2) plot(model2, parameter = \"rho\", items = c(\"shrimp\", \"sea eel\", \"tuna\")) data_batch3 <- sushi_rankings[601:1600, ] model3 <- update_mallows(model2, new_data = setup_rank_data(data_batch3)) plot(model3, parameter = \"rho\", items = c(\"shrimp\", \"sea eel\", \"tuna\")) data_batch4 <- sushi_rankings[1601:5000, ] model4 <- update_mallows(   model3,    new_data = setup_rank_data(rankings = data_batch4)) plot(model4, parameter = \"rho\", items = c(\"shrimp\", \"sea eel\", \"tuna\")) rbind(   compute_posterior_intervals(model1),   compute_posterior_intervals(model2),    compute_posterior_intervals(model3),   compute_posterior_intervals(model4) ) #>   parameter  mean median          hpdi central_interval #> 1     alpha 1.768  1.766 [1.603,1.917]    [1.604,1.922] #> 2     alpha 1.777  1.773 [1.630,1.935]    [1.620,1.931] #> 3     alpha 1.753  1.756 [1.676,1.827]    [1.677,1.827] #> 4     alpha 1.712  1.714 [1.667,1.748]    [1.669,1.752] mod_bmm <- compute_mallows(   data = setup_rank_data(rankings = sushi_rankings),   compute_options = set_compute_options(nmc = 5000, burnin = 1000)   ) plot(mod_bmm) plot(model4) rbind(   compute_posterior_intervals(mod_bmm),   compute_posterior_intervals(model4) ) #>   parameter  mean median          hpdi central_interval #> 1     alpha 1.691  1.690 [1.648,1.734]    [1.643,1.732] #> 2     alpha 1.712  1.714 [1.667,1.748]    [1.669,1.752] compute_consensus(model4) #>      cluster ranking          item cumprob #> 1  Cluster 1       1    fatty tuna   1.000 #> 2  Cluster 1       2    salmon roe   1.000 #> 3  Cluster 1       3          tuna   1.000 #> 4  Cluster 1       4        shrimp   1.000 #> 5  Cluster 1       5       sea eel   1.000 #> 6  Cluster 1       6     tuna roll   0.835 #> 7  Cluster 1       7         squid   1.000 #> 8  Cluster 1       8    sea urchin   1.000 #> 9  Cluster 1       9           egg   1.000 #> 10 Cluster 1      10 cucumber roll   1.000 compute_consensus(mod_bmm) #>      cluster ranking          item cumprob #> 1  Cluster 1       1    fatty tuna       1 #> 2  Cluster 1       2    sea urchin       1 #> 3  Cluster 1       3          tuna       1 #> 4  Cluster 1       4    salmon roe       1 #> 5  Cluster 1       5        shrimp       1 #> 6  Cluster 1       6       sea eel       1 #> 7  Cluster 1       7     tuna roll       1 #> 8  Cluster 1       8         squid       1 #> 9  Cluster 1       9           egg       1 #> 10 Cluster 1      10 cucumber roll       1"},{"path":"/articles/SMC-Mallows.html","id":"new-users-with-partial-or-complete-rankings","dir":"Articles","previous_headings":"","what":"New users with partial or complete rankings","title":"Sequential Monte Carlo for the Bayesian Mallows model","text":"functionality extends directly partial ranks, including top-\\(k\\) rankings rankings missing random. Pairwise preferences also supported, although demonstrated . demonstration shall assume can observe top-5 ranked items user sushi_rankings dataset. , assume start batch data, time 100 rankings: estimate model using compute_mallows(). Since NAs data, compute_mallows() run imputation missing ranks. trace plot shows convergence reached quickly. Trace plot SMC model. set burnin 300. posterior \\(\\alpha\\) initial run: Posterior distribution scale parameter initial run. Next, assume receive 100 top-5 rankings: now update initial model, using SMC. default, uniform distribution used propose new values augmented ranks. pseudo-likelihood proposal developed Stein (2023) can used instead, setting aug_method = \"pseudo\" call set_compute_options(), . posterior \\(\\alpha\\): Posterior distribution scale parameter updating model based new rankings. even data arrives, can update model . example, assume now get set complete rankings, missingness: update model just : Posterior distribution scale parameter updating model based new rankings.","code":"data_partial <- sushi_rankings data_partial[data_partial > 5] <- NA head(data_partial) #>      shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll cucumber roll #> [1,]      2      NA   NA     3          4          1   5         NA        NA            NA #> [2,]      1      NA   NA     4         NA         NA   3          5        NA             2 #> [3,]      2      NA    3     4         NA         NA  NA          1         5            NA #> [4,]      4      NA    5    NA          1          2  NA          3        NA            NA #> [5,]      4      NA   NA     5         NA          3   2         NA         1            NA #> [6,]      4      NA    2    NA         NA          5   1         NA        NA             3 data_batch1 <- data_partial[1:100, ] model1 <- compute_mallows(   data = setup_rank_data(data_batch1),   compute_options = set_compute_options(nmc = 10000)   ) assess_convergence(model1) burnin(model1) <- 300 plot(model1) data_batch2 <- data_partial[101:200, ] model2 <- update_mallows(   model = model1,    new_data = setup_rank_data(data_batch2),    smc_options = set_smc_options(n_particles = 1000),   compute_options = set_compute_options(     aug_method = \"pseudo\", pseudo_aug_metric = \"footrule\")   ) plot(model2) data_batch3 <- sushi_rankings[201:300, ] model3 <- update_mallows(model2, new_data = setup_rank_data(data_batch3)) plot(model3)"},{"path":"/articles/SMC-Mallows.html","id":"users-updating-their-rankings","dir":"Articles","previous_headings":"","what":"Users updating their rankings","title":"Sequential Monte Carlo for the Bayesian Mallows model","text":"Another setting supported existing users update partial rankings. example, users can initially give top-5 rankings, subsequently update top-10 rankings, top-20 rankings, etc. Another setting ranks missing random, users subsequently provide rankings. main methodological issue case, augmented rankings previous SMC timepoint may conflict new rankings. case, augmented rankings must corrected, described Chapter 6 Stein (2023). provide example sushi data. assume initial batch data contains top-3 rankings provided first 100 users. keep track existing users updating preferences, also need user ID case, required number vector. fit standard Metropolis-Hastings algorithm data, yielding starting point. Convergence seems quick, set burnin 300. Trace plot initial run sushi batch 1. Next, assume receive top-5 rankings users. now update model using SMC. can plot posterior distributions \\(\\alpha\\) . Posterior sushi batch 1. Posterior sushi batch 2. Next, assume receive top-8 rankings users. proceeding, instructive study situation needs special care. augmented rankings user 1 particle 1: Next, show data provided user 1 data_batch3: comparing non-missing ranks, can check consistent : provided data consistent augmented rankings case. means augmented rankings user 1 particle 1 need corrected algorithm. Luckily, happens automatically implementation, can update model . Next plot posterior: Posterior sushi batch 3. Now assume get batch new users, without missing ranks. can treated just ones, need new user IDs. posterior model. Posterior sushi batch 4. can confirm implementation sensible giving complete data compute_mallows: trace plot indicates good convergence, set burnin 300. Trace plot MCMC run sushi data. see posterior close one model4: Posterior MCMC sushi data.","code":"set.seed(123) sushi_reduced <- sushi_rankings[1:100, ] data_batch1 <- ifelse(sushi_reduced > 3, NA_real_, sushi_reduced) rownames(data_batch1) <- seq_len(nrow(data_batch1)) head(data_batch1) #>   shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll cucumber roll #> 1      2      NA   NA     3         NA          1  NA         NA        NA            NA #> 2      1      NA   NA    NA         NA         NA   3         NA        NA             2 #> 3      2      NA    3    NA         NA         NA  NA          1        NA            NA #> 4     NA      NA   NA    NA          1          2  NA          3        NA            NA #> 5     NA      NA   NA    NA         NA          3   2         NA         1            NA #> 6     NA      NA    2    NA         NA         NA   1         NA        NA             3 mod_init <- compute_mallows(   data = setup_rank_data(     rankings = data_batch1,     user_ids = as.numeric(rownames(data_batch1))) ) assess_convergence(mod_init) burnin(mod_init) <- 300 data_batch2 <- ifelse(sushi_reduced > 5, NA_real_, sushi_reduced) rownames(data_batch2) <- seq_len(nrow(data_batch2)) model2 <- update_mallows(   model = mod_init,    new_data = setup_rank_data(     rankings = data_batch2, user_ids = as.numeric(rownames(data_batch2))),   compute_options = set_compute_options(     aug_method = \"pseudo\", pseudo_aug_metric = \"footrule\")   ) plot(mod_init) + ggtitle(\"Posterior of dispersion parameter after data batch 1\") plot(model2) + ggtitle(\"Posterior of dispersion parameter after data batch 2\") data_batch3 <- ifelse(sushi_reduced > 8, NA_real_, sushi_reduced) rownames(data_batch3) <- seq_len(nrow(data_batch3)) (v1 <- model2$augmented_rankings[, 1, 1]) #>  [1]  2  7 10  3  4  1  5  6  8  9 (v2a <- unname(data_batch3[1, ])) #>  [1]  2  8 NA  3  4  1  5 NA  7  6 (v2b <- v2a[!is.na(v2a)]) #> [1] 2 8 3 4 1 5 7 6 v1[v1 %in% v2b] #> [1] 2 7 3 4 1 5 6 8 all(v1[v1 %in% v2b] == v2b) #> [1] FALSE model3 <- update_mallows(   model = mod_init,    new_data = setup_rank_data(     rankings = data_batch3, user_ids = as.numeric(rownames(data_batch3)))) plot(model3) + ggtitle(\"Posterior of dispersion parameter after data batch 3\") data_batch4 <- sushi_rankings[500:600, ] rownames(data_batch4) <- 500:600 head(data_batch4) #>     shrimp sea eel tuna squid sea urchin salmon roe egg fatty tuna tuna roll cucumber roll #> 500      6       5    4     8          2          3   7          1         9            10 #> 501      3       9    5     8          4          2   6          1         7            10 #> 502      3       1    8     5          4          7   9          2         6            10 #> 503      8       6    3     1          4          5   9          7         2            10 #> 504      4       7    1     2          9         10   3          8         5             6 #> 505      1       5    6     8          3          4   9          2         7            10 model4 <- update_mallows(   model = model3,    new_data = setup_rank_data(     rankings = data_batch4, user_ids = as.numeric(rownames(data_batch4)))) plot(model4) + ggtitle(\"Posterior of dispersion parameter after data batch 4\") full_data <- rbind(data_batch3, data_batch4) mod_bmm <- compute_mallows(data = setup_rank_data(rankings = full_data)) assess_convergence(mod_bmm) burnin(mod_bmm) <- 300 plot(mod_bmm)"},{"path":[]},{"path":"/articles/parallel_chains.html","id":"why-parallel-chains","dir":"Articles","previous_headings":"","what":"Why Parallel Chains?","title":"MCMC with Parallel Chains","text":"Modern computers multiple cores, computing clusters one can get access hundreds cores easily. running Markov Chains parallel \\(K\\) cores, ideally different starting points, achieve least following: time wait get required number post-burnin samples scales like \\(1/K\\). can check convergence comparing chains.","code":""},{"path":"/articles/parallel_chains.html","id":"parallel-chains-with-complete-rankings","dir":"Articles","previous_headings":"","what":"Parallel Chains with Complete Rankings","title":"MCMC with Parallel Chains","text":"“BayesMallows” use “parallel” package parallel computation. Parallelization obtained starting cluster providing argument. Note also give one initial value dispersion parameter \\(\\alpha\\) chain. can assess convergence usual way: can also assess convergence latent ranks \\(\\boldsymbol{\\rho}\\). Since initial value \\(\\boldsymbol{\\rho}\\) sampled uniformly, two chains automatically get different initial values. Based convergence plots, set burnin 3000. can now use tools assessing posterior distributions usual. post-burnin samples parallel chains simply combined, . plot posterior distribution \\(\\alpha\\). Next plot posterior distribution \\(\\boldsymbol{\\rho}\\).","code":"library(parallel) cl <- makeCluster(4) fit <- compute_mallows(   data = setup_rank_data(rankings = potato_visual),    compute_options = set_compute_options(nmc = 5000),   cl = cl ) stopCluster(cl) assess_convergence(fit) assess_convergence(fit, parameter = \"rho\", items = 1:3) burnin(fit) <- 3000 plot(fit) plot(fit, parameter = \"rho\", items = 4:7)"},{"path":"/articles/parallel_chains.html","id":"parallel-chains-with-pairwise-preferences","dir":"Articles","previous_headings":"","what":"Parallel Chains with Pairwise Preferences","title":"MCMC with Parallel Chains","text":"case parallel chains might strongly needed incomplete data, e.g., arising pairwise preferences. case MCMC algorithm needs perform data augmentation, tends slow sticky. illustrate beach preference data, referring Sørensen et al. (2020) thorough introduction aspects directly related parallelism. run four parallel chains, letting package generate random initial rankings, providing vector initial values \\(\\alpha\\).","code":"beach_data <- setup_rank_data(preferences = beach_preferences) cl <- makeCluster(4) fit <- compute_mallows(   data = beach_data,   compute_options = set_compute_options(nmc = 4000, save_aug = TRUE),   initial_values = set_initial_values(alpha_init = runif(4, 1, 4)),   cl = cl ) stopCluster(cl)"},{"path":"/articles/parallel_chains.html","id":"trace-plots","dir":"Articles","previous_headings":"Parallel Chains with Pairwise Preferences","what":"Trace Plots","title":"MCMC with Parallel Chains","text":"convergence plots shows long-range autocorrelation, otherwise seems mix relatively well. convergence plot \\(\\boldsymbol{\\rho}\\): avoid overplotting, ’s good idea pick low number assessors chains. look items 1-3 assessors 1 2.","code":"assess_convergence(fit) assess_convergence(fit, parameter = \"rho\", items = 4:6) assess_convergence(fit,   parameter = \"Rtilde\",   items = 1:3, assessors = 1:2 )"},{"path":"/articles/parallel_chains.html","id":"posterior-quantities","dir":"Articles","previous_headings":"Parallel Chains with Pairwise Preferences","what":"Posterior Quantities","title":"MCMC with Parallel Chains","text":"Based trace plots, chains seem mixing well. set burnin 1000. can now study posterior distributions. posterior \\(\\alpha\\). Note increasing nmc argument compute_mallows , density appear smoother. vignette kept low reduce run time. can also look posterior \\(\\boldsymbol{\\rho}\\). can also compute posterior intervals usual way: can compute consensus ranking: can compute probability top-\\(k\\), \\(k=4\\):","code":"burnin(fit) <- 1000 plot(fit) plot(fit, parameter = \"rho\", items = 6:9) compute_posterior_intervals(fit, parameter = \"alpha\") #>   parameter  mean median          hpdi central_interval #> 1     alpha 4.798  4.793 [4.242,5.373]    [4.235,5.371] compute_posterior_intervals(fit, parameter = \"rho\") #>    parameter    item mean median    hpdi central_interval #> 1        rho  Item 1    7      7     [7]            [6,7] #> 2        rho  Item 2   15     15    [15]          [14,15] #> 3        rho  Item 3    3      3   [3,4]            [3,4] #> 4        rho  Item 4   11     11 [11,13]          [11,13] #> 5        rho  Item 5    9      9  [8,10]           [8,10] #> 6        rho  Item 6    2      2   [1,2]            [1,2] #> 7        rho  Item 7    9      8  [8,10]           [8,10] #> 8        rho  Item 8   12     12 [11,13]          [11,14] #> 9        rho  Item 9    1      1   [1,2]            [1,2] #> 10       rho Item 10    6      6   [5,6]            [5,7] #> 11       rho Item 11    4      4   [3,5]            [3,5] #> 12       rho Item 12   13     13 [12,14]          [11,14] #> 13       rho Item 13   10     10  [8,10]           [8,10] #> 14       rho Item 14   13     14 [12,14]          [12,14] #> 15       rho Item 15    5      5   [4,5]            [4,6] compute_consensus(fit) #>      cluster ranking    item   cumprob #> 1  Cluster 1       1  Item 9 0.8691667 #> 2  Cluster 1       2  Item 6 1.0000000 #> 3  Cluster 1       3  Item 3 0.6391667 #> 4  Cluster 1       4 Item 11 0.9404167 #> 5  Cluster 1       5 Item 15 0.9559167 #> 6  Cluster 1       6 Item 10 0.9636667 #> 7  Cluster 1       7  Item 1 1.0000000 #> 8  Cluster 1       8  Item 7 0.5473333 #> 9  Cluster 1       9  Item 5 0.9255833 #> 10 Cluster 1      10 Item 13 1.0000000 #> 11 Cluster 1      11  Item 4 0.6924167 #> 12 Cluster 1      12  Item 8 0.7833333 #> 13 Cluster 1      13 Item 12 0.6158333 #> 14 Cluster 1      14 Item 14 0.9958333 #> 15 Cluster 1      15  Item 2 1.0000000 compute_consensus(fit, type = \"MAP\") #>      cluster map_ranking    item probability #> 1  Cluster 1           1  Item 9   0.2683333 #> 2  Cluster 1           2  Item 6   0.2683333 #> 3  Cluster 1           3  Item 3   0.2683333 #> 4  Cluster 1           4 Item 11   0.2683333 #> 5  Cluster 1           5 Item 15   0.2683333 #> 6  Cluster 1           6 Item 10   0.2683333 #> 7  Cluster 1           7  Item 1   0.2683333 #> 8  Cluster 1           8  Item 7   0.2683333 #> 9  Cluster 1           9  Item 5   0.2683333 #> 10 Cluster 1          10 Item 13   0.2683333 #> 11 Cluster 1          11  Item 4   0.2683333 #> 12 Cluster 1          12  Item 8   0.2683333 #> 13 Cluster 1          13 Item 12   0.2683333 #> 14 Cluster 1          14 Item 14   0.2683333 #> 15 Cluster 1          15  Item 2   0.2683333 plot_top_k(fit, k = 4)"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Oystein Sorensen. Author, maintainer. Waldir Leoncio. Author. Valeria Vitelli. Author. Marta Crispino. Author. Qinghua Liu. Author. Cristina Mollica. Author. Luca Tardella. Author. Anja Stein. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). “BayesMallows: R Package Bayesian Mallows Model.” R Journal, 12(1), 324–342. doi:10.32614/RJ-2020-026.","code":"@Article{,   author = {{\\O}ystein S{\\o}rensen and Marta Crispino and Qinghua Liu and Valeria Vitelli},   doi = {10.32614/RJ-2020-026},   title = {BayesMallows: An R Package for the Bayesian Mallows Model},   journal = {The R Journal},   number = {1},   pages = {324--342},   volume = {12},   year = {2020}, }"},{"path":"/index.html","id":"bayesmallows","dir":"","previous_headings":"","what":"Bayesian Preference Learning with the Mallows Rank Model","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"package provides general framework analyzing rank preference data based Bayesian Mallows model.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"install current release, use install current development version, use","code":"install.packages(\"BayesMallows\") # install.packages(\"remotes\") remotes::install_github(\"ocbe-uio/BayesMallows\")"},{"path":"/index.html","id":"basic-usage-example","dir":"","previous_headings":"","what":"Basic Usage Example","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"get started, load package package comes several example datasets. simplest one contains 12 persons’ assessments weights 20 potatoes, either visual inspection (potato_visual) lifting potatoes comparing relative weights hand (potato_weighing).","code":"library(BayesMallows) set.seed(123)"},{"path":"/index.html","id":"metropolis-hastings-algorithm","dir":"","previous_headings":"Basic Usage Example","what":"Metropolis-Hastings Algorithm","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"fit Bayesian Mallows model potato_visual dataset using Metropolis-Hastings algorithm first described Vitelli et al. (2018), Next, can see diagnostic plot Metropolis-Hastings algorithm assess_convergence(). plot scale parameter, measures variation individual rankings.  Setting burnin 500, obtain plot posterior distribution scale parameter :  examples, please see introductory vignette, function documentation. use parallel chains described vignette.","code":"potato_data <- setup_rank_data(potato_visual) fit <- compute_mallows(data = potato_data) assess_convergence(fit) burnin(fit) <- 500 plot(fit)"},{"path":"/index.html","id":"sequential-monte-carlo-algorithm","dir":"","previous_headings":"Basic Usage Example","what":"Sequential Monte Carlo Algorithm","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"package also supports updating Bayesian Mallows model using sequential Monte Carlo, algorithm described Stein (2023). example, order update model fitted potato ranks based comparing relative weights hand, can go plot posterior distribution scale parameter updated model.  Sequential Monte Carlo can typically useful new data arrives batches, require Metropolis-Hastings algorithm rerun. See vignette information.","code":"new_data <- setup_rank_data(rankings = potato_weighing) updated_fit <- update_mallows(model = fit, new_data = new_data) plot(updated_fit)"},{"path":[]},{"path":"/index.html","id":"methodology","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Methodology","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"BayesMallows package currently implements complete model described Vitelli et al. (2018), includes large number distance metrics, handling missing ranks pairwise comparisons, clustering users similar preferences. extension non-transitive pairwise comparisons Crispino et al. (2019) also implemented. addition, partition function Mallows model can estimated using importance sampling algorithm Vitelli et al. (2018) asymptotic approximation Mukherjee (2016). review ranking models general, see Liu, Crispino, et al. (2019). Crispino Antoniano-Villalobos (2022) outlines informative priors can used within model. Updating posterior distribution based new data, using sequential Monte Carlo methods, implemented described separate vignette. computational algorithms described detail Stein (2023).","code":""},{"path":"/index.html","id":"applications","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Applications","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"Among current applications, Liu, Reiner, et al. (2019) applied Bayesian Mallows model providing personalized recommendations based clicking data, Barrett Crispino (2018) used model Crispino et al. (2019) analyze listeners’ understanding music. Eliseussen, Fleischer, Vitelli (2022) presented extended model variable selection genome-wide transcriptomic analyses.","code":""},{"path":"/index.html","id":"future-extensions","dir":"","previous_headings":"The Bayesian Mallows Model","what":"Future Extensions","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"Plans future extensions package include implementation variational Bayes algorithm approximation posterior distribution. sequential Monte Carlo algorithms also extended cover larger part model framework, add options specifications prior distributions.","code":""},{"path":"/index.html","id":"compilation-with-thread-building-blocks-tbb","dir":"","previous_headings":"","what":"Compilation with Thread Building Blocks (TBB)","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"Parts underlying C++ code can easily parallelized. support oneAPI Threading Building Blocks. want make use , build package source -ltbb argument. M1 Mac can achieved first installing TBB brew install tbb adding following line ~/.R/Makevars: arguments platforms similar. TBB available, package fall back sequential processing.","code":"CXX17=g++-13 -I/opt/homebrew/include -L/opt/homebrew/lib -ltbb"},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"using BayesMallows package academic work, please cite Sørensen et al. (2020), addition relevant methodological papers.","code":"citation(\"BayesMallows\") #> To cite package 'BayesMallows' in publications use: #>  #>   Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). \"BayesMallows: An R #>   Package for the Bayesian Mallows Model.\" _The R Journal_, *12*(1), #>   324-342. doi:10.32614/RJ-2020-026 #>   <https://doi.org/10.32614/RJ-2020-026>. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Article{, #>     author = {{\\O}ystein S{\\o}rensen and Marta Crispino and Qinghua Liu and Valeria Vitelli}, #>     doi = {10.32614/RJ-2020-026}, #>     title = {BayesMallows: An R Package for the Bayesian Mallows Model}, #>     journal = {The R Journal}, #>     number = {1}, #>     pages = {324--342}, #>     volume = {12}, #>     year = {2020}, #>   }"},{"path":"/index.html","id":"contribution","dir":"","previous_headings":"","what":"Contribution","title":"Bayesian Preference Learning with the Mallows Rank Model","text":"open source project, contributions welcome. Feel free open Issue, Pull Request, e-mail us.","code":""},{"path":[]},{"path":"/reference/BayesMallows-package.html","id":null,"dir":"Reference","previous_headings":"","what":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model — BayesMallows-package","title":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model — BayesMallows-package","text":"implementation Bayesian version Mallows rank model (Vitelli et al., Journal Machine Learning Research, 2018 https://jmlr.org/papers/v18/15-481.html; Crispino et al., Annals Applied Statistics, 2019 doi:10.1214/18-AOAS1203 ; Sorensen et al., R Journal, 2020 doi:10.32614/RJ-2020-026 ; Stein, PhD Thesis, 2023 https://eprints.lancs.ac.uk/id/eprint/195759). Metropolis-Hastings sequential Monte Carlo algorithms estimating models available. Cayley, footrule, Hamming, Kendall, Spearman, Ulam distances supported models. rank data analyzed can form complete rankings, top-k rankings, partially missing rankings, well consistent inconsistent pairwise preferences. Several functions plotting studying posterior distributions parameters provided. package also provides functions estimating partition function (normalizing constant) Mallows rank model, importance sampling algorithm Vitelli et al. asymptotic approximation IPFP algorithm (Mukherjee, Annals Statistics, 2016 doi:10.1214/15-AOS1389 ).","code":""},{"path":"/reference/BayesMallows-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model — BayesMallows-package","text":"Sørensen Ø, Crispino M, Liu Q, Vitelli V (2020). “BayesMallows: R Package Bayesian Mallows Model.” R Journal, 12(1), 324--342. doi:10.32614/RJ-2020-026 .","code":""},{"path":[]},{"path":"/reference/BayesMallows-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"BayesMallows: Bayesian Preference Learning with the Mallows Rank Model — BayesMallows-package","text":"Maintainer: Oystein Sorensen oystein.sorensen.1985@gmail.com (ORCID) Authors: Waldir Leoncio w.l.netto@medisin.uio.Valeria Vitelli valeria.vitelli@medisin.uio.(ORCID) Marta Crispino crispino.marta8@gmail.com Qinghua Liu qinghual@math.uio.Cristina Mollica cristina.mollica@uniroma1.Luca Tardella Anja Stein","code":""},{"path":"/reference/assess_convergence.html","id":null,"dir":"Reference","previous_headings":"","what":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"assess_convergence provides trace plots parameters Mallows Rank model, order study convergence Metropolis-Hastings algorithm.","code":""},{"path":"/reference/assess_convergence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"","code":"assess_convergence(model_fit, ...)  # S3 method for BayesMallows assess_convergence(   model_fit,   parameter = c(\"alpha\", \"rho\", \"Rtilde\", \"cluster_probs\", \"theta\"),   items = NULL,   assessors = NULL,   ... )  # S3 method for BayesMallowsMixtures assess_convergence(   model_fit,   parameter = c(\"alpha\", \"cluster_probs\"),   items = NULL,   assessors = NULL,   ... )"},{"path":"/reference/assess_convergence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"model_fit fitted model object class BayesMallows returned compute_mallows() object class BayesMallowsMixtures returned compute_mallows_mixtures(). ... arguments passed methods. Currently used. parameter Character string specifying parameter plot. Available options \"alpha\", \"rho\", \"Rtilde\", \"cluster_probs\", \"theta\". items items study diagnostic plot rho. Either vector item names, corresponding model_fit$data$items vector indices. NULL, five items selected randomly. used parameter = \"rho\" parameter = \"Rtilde\". assessors Numeric vector specifying assessors study diagnostic plot \"Rtilde\".","code":""},{"path":"/reference/assess_convergence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trace Plots from Metropolis-Hastings Algorithm — assess_convergence","text":"","code":"set.seed(1) # Fit a model on the potato_visual data mod <- compute_mallows(setup_rank_data(potato_visual)) # Check for convergence assess_convergence(mod)  assess_convergence(mod, parameter = \"rho\", items = 1:20)"},{"path":"/reference/assign_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Assessors to Clusters — assign_cluster","title":"Assign Assessors to Clusters — assign_cluster","text":"Assign assessors clusters finding cluster highest posterior probability.","code":""},{"path":"/reference/assign_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Assessors to Clusters — assign_cluster","text":"","code":"assign_cluster(model_fit, soft = TRUE, expand = FALSE)"},{"path":"/reference/assign_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Assessors to Clusters — assign_cluster","text":"model_fit object type BayesMallows, returned compute_mallows(). soft logical specifying whether perform soft hard clustering. soft=TRUE, cluster probabilities returned, whereas soft=FALSE, maximum posterior (MAP) cluster probability returned, per assessor. case tie two cluster assignments, random cluster taken MAP estimate. expand logical specifying whether expand rowset assessor also include clusters assessor 0 posterior assignment probability. used soft = TRUE. Defaults FALSE.","code":""},{"path":"/reference/assign_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Assessors to Clusters — assign_cluster","text":"dataframe. soft = FALSE, one row per assessor, columns assessor, probability map_cluster. soft = TRUE, n_cluster rows per assessor, additional column cluster.","code":""},{"path":[]},{"path":"/reference/assign_cluster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Assessors to Clusters — assign_cluster","text":"","code":"# Fit a model with three clusters to the simulated example data set.seed(1) mixture_model <- compute_mallows(   data = setup_rank_data(cluster_data),   model_options = set_model_options(n_clusters = 3),   compute_options = set_compute_options(nmc = 5000, burnin = 1000) )  head(assign_cluster(mixture_model)) #>   assessor   cluster probability map_cluster #> 1        1 Cluster 1     0.20525   Cluster 2 #> 2        1 Cluster 2     0.77775   Cluster 2 #> 3        1 Cluster 3     0.01700   Cluster 2 #> 4        2 Cluster 1     0.10625   Cluster 2 #> 5        2 Cluster 2     0.87125   Cluster 2 #> 6        2 Cluster 3     0.02250   Cluster 2 head(assign_cluster(mixture_model, soft = FALSE)) #>    assessor probability map_cluster #> 2         1     0.77775   Cluster 2 #> 5         2     0.87125   Cluster 2 #> 8         3     0.91450   Cluster 2 #> 11        4     0.92000   Cluster 2 #> 14        5     0.77675   Cluster 2 #> 17        6     0.91525   Cluster 2"},{"path":"/reference/asymptotic_partition_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"Compute asymptotic approximation logarithm partition function, using iteration algorithm Mukherjee (2016) .","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"","code":"asymptotic_partition_function(   alpha_vector,   n_items,   metric,   K,   n_iterations = 1000L,   tol = 1e-09 )"},{"path":"/reference/asymptotic_partition_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"alpha_vector numeric vector alpha values. n_items Integer specifying number items. metric One \"footrule\" \"spearman\". K Integer. n_iterations Integer specifying number iterations. tol Stopping criterion algorithm. previous matrix subtracted updated, maximum absolute relative difference tol, iteration stops.","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"vector, containing partition function value alpha.","code":""},{"path":"/reference/asymptotic_partition_function.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Asymptotic Approximation of Partition Function — asymptotic_partition_function","text":"Mukherjee S (2016). “Estimation exponential families permutations.” Annals Statistics, 44(2), 853--875. doi:10.1214/15-aos1389 .","code":""},{"path":"/reference/beach_preferences.html","id":null,"dir":"Reference","previous_headings":"","what":"Beach preferences — beach_preferences","title":"Beach preferences — beach_preferences","text":"Example dataset (Vitelli et al. 2018) , Section 6.2.","code":""},{"path":"/reference/beach_preferences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Beach preferences — beach_preferences","text":"","code":"beach_preferences"},{"path":"/reference/beach_preferences.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Beach preferences — beach_preferences","text":"object class data.frame 1442 rows 3 columns.","code":""},{"path":"/reference/beach_preferences.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Beach preferences — beach_preferences","text":"Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/bernoulli_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated intransitive pairwise preferences — bernoulli_data","title":"Simulated intransitive pairwise preferences — bernoulli_data","text":"Simulated dataset based potato_visual data. Based rankings potato_visual, n-choose-2 = 190 pairs items sampled assessor. probability .9, pairwise preference agreement potato_visual, probability .1, disagreement. Hence, data generating mechanism Bernoulli error model (Crispino et al. 2019)  \\(\\theta=0.1\\).","code":""},{"path":"/reference/bernoulli_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated intransitive pairwise preferences — bernoulli_data","text":"","code":"bernoulli_data"},{"path":"/reference/bernoulli_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated intransitive pairwise preferences — bernoulli_data","text":"object class data.frame 2280 rows 3 columns.","code":""},{"path":[]},{"path":"/reference/burnin-set.html","id":null,"dir":"Reference","previous_headings":"","what":"Set the burnin — burnin<-","title":"Set the burnin — burnin<-","text":"Set update burnin model computed using Metropolis-Hastings.","code":""},{"path":"/reference/burnin-set.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set the burnin — burnin<-","text":"","code":"burnin(model, ...) <- value  # S3 method for BayesMallows burnin(model, ...) <- value  # S3 method for BayesMallowsMixtures burnin(model, ...) <- value"},{"path":"/reference/burnin-set.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set the burnin — burnin<-","text":"model object class BayesMallows returned compute_mallows() object class BayesMallowsMixtures returned compute_mallows_mixtures(). ... Optional arguments passed methods. Currently used. value integer specifying burnin. model class BayesMallowsMixtures, single value assumed burnin model element. Alternatively, value can specified integer vector length model, hence separate burnin can set number mixture components.","code":""},{"path":"/reference/burnin-set.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set the burnin — burnin<-","text":"object class BayesMallows burnin set.","code":""},{"path":[]},{"path":"/reference/burnin-set.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set the burnin — burnin<-","text":"","code":"set.seed(445) mod <- compute_mallows(setup_rank_data(potato_visual)) assess_convergence(mod)  burnin(mod) #> NULL burnin(mod) <- 1500 burnin(mod) #> [1] 1500 plot(mod)  #' models <- compute_mallows_mixtures(   data = setup_rank_data(cluster_data),   n_clusters = 1:3) burnin(models) #> [[1]] #> NULL #>  #> [[2]] #> NULL #>  #> [[3]] #> NULL #>  burnin(models) <- 100 burnin(models) #> [[1]] #> [1] 100 #>  #> [[2]] #> [1] 100 #>  #> [[3]] #> [1] 100 #>  burnin(models) <- c(100, 300, 200) burnin(models) #> [[1]] #> [1] 100 #>  #> [[2]] #> [1] 300 #>  #> [[3]] #> [1] 200 #>"},{"path":"/reference/burnin.html","id":null,"dir":"Reference","previous_headings":"","what":"See the burnin — burnin","title":"See the burnin — burnin","text":"See current burnin value model.","code":""},{"path":"/reference/burnin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"See the burnin — burnin","text":"","code":"burnin(model, ...)  # S3 method for BayesMallows burnin(model, ...)  # S3 method for BayesMallowsMixtures burnin(model, ...)  # S3 method for SMCMallows burnin(model, ...)"},{"path":"/reference/burnin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"See the burnin — burnin","text":"model model object. ... Optional arguments passed methods. Currently used.","code":""},{"path":"/reference/burnin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"See the burnin — burnin","text":"integer specifying burnin, exists. Otherwise NULL.","code":""},{"path":[]},{"path":"/reference/burnin.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"See the burnin — burnin","text":"","code":"set.seed(445) mod <- compute_mallows(setup_rank_data(potato_visual)) assess_convergence(mod)  burnin(mod) #> NULL burnin(mod) <- 1500 burnin(mod) #> [1] 1500 plot(mod)  #' models <- compute_mallows_mixtures(   data = setup_rank_data(cluster_data),   n_clusters = 1:3) burnin(models) #> [[1]] #> NULL #>  #> [[2]] #> NULL #>  #> [[3]] #> NULL #>  burnin(models) <- 100 burnin(models) #> [[1]] #> [1] 100 #>  #> [[2]] #> [1] 100 #>  #> [[3]] #> [1] 100 #>  burnin(models) <- c(100, 300, 200) burnin(models) #> [[1]] #> [1] 100 #>  #> [[2]] #> [1] 300 #>  #> [[3]] #> [1] 200 #>"},{"path":"/reference/cluster_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated clustering data — cluster_data","title":"Simulated clustering data — cluster_data","text":"Simulated dataset 60 complete rankings five items, three different clusters.","code":""},{"path":"/reference/cluster_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated clustering data — cluster_data","text":"","code":"cluster_data"},{"path":"/reference/cluster_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated clustering data — cluster_data","text":"object class matrix (inherits array) 60 rows 5 columns.","code":""},{"path":[]},{"path":"/reference/compute_consensus.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Consensus Ranking — compute_consensus","title":"Compute Consensus Ranking — compute_consensus","text":"Compute consensus ranking using either cumulative probability (CP) maximum posteriori (MAP) consensus (Vitelli et al. 2018) . mixture models, consensus given mixture. Consensus augmented ranks can also computed assessor, setting parameter = \"Rtilde\".","code":""},{"path":"/reference/compute_consensus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Consensus Ranking — compute_consensus","text":"","code":"compute_consensus(model_fit, ...)  # S3 method for BayesMallows compute_consensus(   model_fit,   type = c(\"CP\", \"MAP\"),   parameter = c(\"rho\", \"Rtilde\"),   assessors = 1L,   ... )  # S3 method for SMCMallows compute_consensus(model_fit, type = c(\"CP\", \"MAP\"), parameter = \"rho\", ...)"},{"path":"/reference/compute_consensus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Consensus Ranking — compute_consensus","text":"model_fit model fit. ... arguments passed methods. Currently used. type Character string specifying consensus compute. Either \"CP\" \"MAP\". Defaults \"CP\". parameter Character string defining parameter compute consensus. Defaults \"rho\". Available options \"rho\" \"Rtilde\", latter giving consensus rankings augmented ranks. assessors parameter = \"rho\", integer vector used define assessors compute augmented ranking. Defaults 1L, yields augmented rankings assessor 1.","code":""},{"path":"/reference/compute_consensus.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Consensus Ranking — compute_consensus","text":"Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/compute_consensus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Consensus Ranking — compute_consensus","text":"","code":"# The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the # Mallows model: model_fit <- compute_mallows(setup_rank_data(potato_visual))  # Se the documentation to compute_mallows for how to assess the convergence of # the algorithm. Having chosen burin = 1000, we compute posterior intervals burnin(model_fit) <- 1000 # We then compute the CP consensus. compute_consensus(model_fit, type = \"CP\") #>      cluster ranking item cumprob #> 1  Cluster 1       1  P12   1.000 #> 2  Cluster 1       2  P13   1.000 #> 3  Cluster 1       3   P9   0.978 #> 4  Cluster 1       4  P10   0.973 #> 5  Cluster 1       5  P17   0.879 #> 6  Cluster 1       6   P7   0.917 #> 7  Cluster 1       7  P14   1.000 #> 8  Cluster 1       8  P16   1.000 #> 9  Cluster 1       9   P1   0.540 #> 10 Cluster 1      10   P5   0.657 #> 11 Cluster 1      11  P11   1.000 #> 12 Cluster 1      12  P19   1.000 #> 13 Cluster 1      13  P20   0.568 #> 14 Cluster 1      14  P18   1.000 #> 15 Cluster 1      15   P6   0.977 #> 16 Cluster 1      16   P4   0.621 #> 17 Cluster 1      17   P2   0.908 #> 18 Cluster 1      18  P15   1.000 #> 19 Cluster 1      19   P3   1.000 #> 20 Cluster 1      20   P8   1.000 # And we compute the MAP consensus compute_consensus(model_fit, type = \"MAP\") #>      cluster map_ranking item probability #> 1  Cluster 1           1  P12       0.154 #> 2  Cluster 1           2  P13       0.154 #> 3  Cluster 1           3   P9       0.154 #> 4  Cluster 1           4  P10       0.154 #> 5  Cluster 1           5  P17       0.154 #> 6  Cluster 1           6   P7       0.154 #> 7  Cluster 1           7  P14       0.154 #> 8  Cluster 1           8  P16       0.154 #> 9  Cluster 1           9   P1       0.154 #> 10 Cluster 1          10   P5       0.154 #> 11 Cluster 1          11  P11       0.154 #> 12 Cluster 1          12  P19       0.154 #> 13 Cluster 1          13  P20       0.154 #> 14 Cluster 1          14  P18       0.154 #> 15 Cluster 1          15   P6       0.154 #> 16 Cluster 1          16   P4       0.154 #> 17 Cluster 1          17   P2       0.154 #> 18 Cluster 1          18  P15       0.154 #> 19 Cluster 1          19   P3       0.154 #> 20 Cluster 1          20   P8       0.154  if (FALSE) {   # CLUSTERWISE CONSENSUS   # We can run a mixture of Mallows models, using the n_clusters argument   # We use the sushi example data. See the documentation of compute_mallows for   # a more elaborate example   model_fit <- compute_mallows(     setup_rank_data(sushi_rankings),     model_options = set_model_options(n_clusters = 5))   # Keeping the burnin at 1000, we can compute the consensus ranking per cluster   burnin(model_fit) <- 1000   cp_consensus_df <- compute_consensus(model_fit, type = \"CP\")   # We can now make a table which shows the ranking in each cluster:   cp_consensus_df$cumprob <- NULL   stats::reshape(cp_consensus_df, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\",                  varying = list(sort(unique(cp_consensus_df$cluster)))) }  if (FALSE) {   # MAP CONSENSUS FOR PAIRWISE PREFENCE DATA   # We use the example dataset with beach preferences.   model_fit <- compute_mallows(setup_rank_data(preferences = beach_preferences))   # We set burnin = 1000   burnin(model_fit) <- 1000   # We now compute the MAP consensus   map_consensus_df <- compute_consensus(model_fit, type = \"MAP\")    # CP CONSENSUS FOR AUGMENTED RANKINGS   # We use the example dataset with beach preferences.   model_fit <- compute_mallows(     setup_rank_data(preferences = beach_preferences),     compute_options = set_compute_options(save_aug = TRUE, aug_thinning = 2))   # We set burnin = 1000   burnin(model_fit) <- 1000   # We now compute the CP consensus of augmented ranks for assessors 1 and 3   cp_consensus_df <- compute_consensus(     model_fit, type = \"CP\", parameter = \"Rtilde\", assessors = c(1L, 3L))   # We can also compute the MAP consensus for assessor 2   map_consensus_df <- compute_consensus(     model_fit, type = \"MAP\", parameter = \"Rtilde\", assessors = 2L)    # Caution!   # With very sparse data or with too few iterations, there may be ties in the   # MAP consensus. This is illustrated below for the case of only 5 post-burnin   # iterations. Two MAP rankings are equally likely in this case (and for this   # seed).   model_fit <- compute_mallows(     setup_rank_data(preferences = beach_preferences),     compute_options = set_compute_options(       nmc = 1005, save_aug = TRUE, aug_thinning = 1))   burnin(model_fit) <- 1000   compute_consensus(model_fit, type = \"MAP\", parameter = \"Rtilde\",                     assessors = 2L) }"},{"path":"/reference/compute_expected_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Expected value of metrics under a Mallows rank model — compute_expected_distance","title":"Expected value of metrics under a Mallows rank model — compute_expected_distance","text":"Compute expectation several metrics Mallows rank model.","code":""},{"path":"/reference/compute_expected_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expected value of metrics under a Mallows rank model — compute_expected_distance","text":"","code":"compute_expected_distance(   alpha,   n_items,   metric = c(\"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\") )"},{"path":"/reference/compute_expected_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expected value of metrics under a Mallows rank model — compute_expected_distance","text":"alpha Non-negative scalar specifying scale (precision) parameter Mallows rank model. n_items Integer specifying number items. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\", \"footrule\", \"spearman\".","code":""},{"path":"/reference/compute_expected_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expected value of metrics under a Mallows rank model — compute_expected_distance","text":"scalar providing expected value metric Mallows rank model distance specified metric argument.","code":""},{"path":[]},{"path":"/reference/compute_expected_distance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expected value of metrics under a Mallows rank model — compute_expected_distance","text":"","code":"compute_expected_distance(1, 5, metric = \"kendall\") #> [1] 4.177277 compute_expected_distance(2, 6, metric = \"cayley\") #> [1] 3.212053 compute_expected_distance(1.5, 7, metric = \"hamming\") #> [1] 5.761023 compute_expected_distance(5, 30, \"ulam\") #> [1] 21.33016 compute_expected_distance(3.5, 45, \"footrule\") #> [1] 377.5987 compute_expected_distance(4, 10, \"spearman\") #> [1] 7.220669"},{"path":"/reference/compute_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Preference Learning with the Mallows Rank Model — compute_mallows","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"Compute posterior distributions parameters Bayesian Mallows Rank Model, given rankings preferences stated set assessors. BayesMallows package uses following parametrization Mallows rank model (Mallows 1957) : $$p(r|\\alpha,\\rho) = \\frac{1}{Z_{n}(\\alpha)} \\exp\\left\\{\\frac{-\\alpha}{n}   d(r,\\rho)\\right\\}$$ \\(r\\) ranking, \\(\\alpha\\) scale parameter, \\(\\rho\\) latent consensus ranking, \\(Z_{n}(\\alpha)\\) partition function (normalizing constant), \\(d(r,\\rho)\\) distance function measuring distance \\(r\\) \\(\\rho\\). refer Vitelli et al. (2018)  details Bayesian Mallows model. compute_mallows always returns posterior distributions latent consensus ranking \\(\\rho\\) scale parameter \\(\\alpha\\). Several distance measures supported, preferences can take form complete incomplete rankings, well pairwise preferences. compute_mallows can also compute mixtures Mallows models, clustering assessors similar preferences.","code":""},{"path":"/reference/compute_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"","code":"compute_mallows(   data,   model_options = set_model_options(),   compute_options = set_compute_options(),   priors = set_priors(),   initial_values = set_initial_values(),   pfun_estimate = NULL,   verbose = FALSE,   cl = NULL )"},{"path":"/reference/compute_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"data object class \"BayesMallowsData\" returned setup_rank_data(). model_options object class \"BayesMallowsModelOptions\" returned set_model_options(). compute_options object class \"BayesMallowsComputeOptions\" returned set_compute_options(). priors object class \"BayesMallowsPriors\" returned set_priors(). initial_values object class \"BayesMallowsInitialValues\" returned set_initial_values(). pfun_estimate Object returned estimate_partition_function(). Defaults NULL, used footrule, Spearman, Ulam distances cardinalities available, cf. get_cardinalities(). verbose Logical specifying whether print progress Metropolis-Hastings algorithm. TRUE, notification printed every 1000th iteration. Defaults FALSE. cl Optional cluster returned parallel::makeCluster(). provided, chains run parallel, one node cl.","code":""},{"path":"/reference/compute_mallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"object class BayesMallows.","code":""},{"path":"/reference/compute_mallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"Mallows CL (1957). “Non-Null Ranking Models. .” Biometrika, 44(1/2), 114--130. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/compute_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preference Learning with the Mallows Rank Model — compute_mallows","text":"","code":"# ANALYSIS OF COMPLETE RANKINGS # The example datasets potato_visual and potato_weighing contain complete # rankings of 20 items, by 12 assessors. We first analyse these using the Mallows # model: set.seed(1) model_fit <- compute_mallows(   data = setup_rank_data(rankings = potato_visual),   compute_options = set_compute_options(nmc = 2000)   )  # We study the trace plot of the parameters assess_convergence(model_fit, parameter = \"alpha\")  assess_convergence(model_fit, parameter = \"rho\", items = 1:4)   # Based on these plots, we set burnin = 1000. burnin(model_fit) <- 1000 # Next, we use the generic plot function to study the posterior distributions # of alpha and rho plot(model_fit, parameter = \"alpha\")  plot(model_fit, parameter = \"rho\", items = 10:15)   # We can also compute the CP consensus posterior ranking compute_consensus(model_fit, type = \"CP\") #>      cluster ranking item cumprob #> 1  Cluster 1       1  P12   1.000 #> 2  Cluster 1       2  P13   1.000 #> 3  Cluster 1       3   P9   0.876 #> 4  Cluster 1       4  P10   0.916 #> 5  Cluster 1       5  P17   0.748 #> 6  Cluster 1       6   P7   0.882 #> 7  Cluster 1       7  P14   0.977 #> 8  Cluster 1       8  P16   0.845 #> 9  Cluster 1       9   P5   0.605 #> 10 Cluster 1      10   P1   0.743 #> 11 Cluster 1      11  P11   0.965 #> 12 Cluster 1      12  P19   1.000 #> 13 Cluster 1      13  P20   0.582 #> 14 Cluster 1      14  P18   1.000 #> 15 Cluster 1      15   P6   0.916 #> 16 Cluster 1      16   P4   0.727 #> 17 Cluster 1      17   P2   0.780 #> 18 Cluster 1      18  P15   1.000 #> 19 Cluster 1      19   P3   1.000 #> 20 Cluster 1      20   P8   1.000  # And we can compute the posterior intervals: # First we compute the interval for alpha compute_posterior_intervals(model_fit, parameter = \"alpha\") #>   parameter   mean median           hpdi central_interval #> 1     alpha 10.632 10.704 [8.468,12.259]   [8.473,12.352] # Then we compute the interval for all the items compute_posterior_intervals(model_fit, parameter = \"rho\") #>    parameter item mean median      hpdi central_interval #> 1        rho   P1   10     10    [9,12]           [9,12] #> 2        rho   P2   17     17   [16,18]          [16,18] #> 3        rho   P3   19     19      [19]             [19] #> 4        rho   P4   16     16   [16,18]          [15,18] #> 5        rho   P5    9      9 [3][6,11]           [3,11] #> 6        rho   P6   15     15   [15,16]          [15,18] #> 7        rho   P7    6      6     [5,7]            [5,7] #> 8        rho   P8   20     20      [20]             [20] #> 9        rho   P9    3      3     [3,4]            [3,4] #> 10       rho  P10    4      4     [3,5]            [3,5] #> 11       rho  P11   11     11    [9,11]           [9,12] #> 12       rho  P12    1      1       [1]              [1] #> 13       rho  P13    2      2       [2]              [2] #> 14       rho  P14    7      7     [6,7]            [6,7] #> 15       rho  P15   18     18   [17,18]          [15,18] #> 16       rho  P16    8      8     [8,9]            [8,9] #> 17       rho  P17    5      5  [5,6][8]            [5,8] #> 18       rho  P18   14     14   [13,14]          [13,14] #> 19       rho  P19   12     12  [10][12]          [10,12] #> 20       rho  P20   13     13   [13,14]          [13,14]  # ANALYSIS OF PAIRWISE PREFERENCES # The example dataset beach_preferences contains pairwise # preferences between beaches stated by 60 assessors. There # is a total of 15 beaches in the dataset. beach_data <- setup_rank_data(   preferences = beach_preferences ) # We then run the Bayesian Mallows rank model # We save the augmented data for diagnostics purposes. model_fit <- compute_mallows(   data = beach_data,   compute_options = set_compute_options(save_aug = TRUE),   verbose = TRUE) #> First 1000 iterations of Metropolis-Hastings algorithm completed. # We can assess the convergence of the scale parameter assess_convergence(model_fit)  # We can assess the convergence of latent rankings. Here we # show beaches 1-5. assess_convergence(model_fit, parameter = \"rho\", items = 1:5)  # We can also look at the convergence of the augmented rankings for # each assessor. assess_convergence(model_fit, parameter = \"Rtilde\",                    items = c(2, 4), assessors = c(1, 2))  # Notice how, for assessor 1, the lines cross each other, while # beach 2 consistently has a higher rank value (lower preference) for # assessor 2. We can see why by looking at the implied orderings in # beach_tc subset(get_transitive_closure(beach_data), assessor %in% c(1, 2) &          bottom_item %in% c(2, 4) & top_item %in% c(2, 4)) #>    assessor bottom_item top_item #> 49        2           2        4 # Assessor 1 has no implied ordering between beach 2 and beach 4, # while assessor 2 has the implied ordering that beach 4 is preferred # to beach 2. This is reflected in the trace plots.   # CLUSTERING OF ASSESSORS WITH SIMILAR PREFERENCES if (FALSE) {   # The example dataset sushi_rankings contains 5000 complete   # rankings of 10 types of sushi   # We start with computing a 3-cluster solution   model_fit <- compute_mallows(     data = setup_rank_data(sushi_rankings),     model_options = set_model_options(n_clusters = 3),     compute_options = set_compute_options(nmc = 10000),     verbose = TRUE)   # We then assess convergence of the scale parameter alpha   assess_convergence(model_fit)   # Next, we assess convergence of the cluster probabilities   assess_convergence(model_fit, parameter = \"cluster_probs\")   # Based on this, we set burnin = 1000   # We now plot the posterior density of the scale parameters alpha in   # each mixture:   burnin(model_fit) <- 1000   plot(model_fit, parameter = \"alpha\")   # We can also compute the posterior density of the cluster probabilities   plot(model_fit, parameter = \"cluster_probs\")   # We can also plot the posterior cluster assignment. In this case,   # the assessors are sorted according to their maximum a posteriori cluster estimate.   plot(model_fit, parameter = \"cluster_assignment\")   # We can also assign each assessor to a cluster   cluster_assignments <- assign_cluster(model_fit, soft = FALSE)   }  # DETERMINING THE NUMBER OF CLUSTERS if (FALSE) {   # Continuing with the sushi data, we can determine the number of cluster   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(     n_clusters = n_clusters,     data = setup_rank_data(rankings = sushi_rankings),     compute_options = set_compute_options(       nmc = 6000, alpha_jump = 10, include_wcd = TRUE)     )   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   burnin(models) <- 1000   plot_elbow(models)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., at 6 clusters. }  # SPEEDING UP COMPUTION WITH OBSERVATION FREQUENCIES With a large number of # assessors taking on a relatively low number of unique rankings, the # observation_frequency argument allows providing a rankings matrix with the # unique set of rankings, and the observation_frequency vector giving the number # of assessors with each ranking. This is illustrated here for the potato_visual # dataset # # assume each row of potato_visual corresponds to between 1 and 5 assessors, as # given by the observation_frequency vector if (FALSE) {   set.seed(1234)   observation_frequency <- sample.int(n = 5, size = nrow(potato_visual), replace = TRUE)   m <- compute_mallows(     setup_rank_data(rankings = potato_visual, observation_frequency = observation_frequency))    # INTRANSITIVE PAIRWISE PREFERENCES   set.seed(1234)   mod <- compute_mallows(     setup_rank_data(preferences = bernoulli_data),     compute_options = set_compute_options(nmc = 5000),     priors = set_priors(kappa = c(1, 10)),     model_options = set_model_options(error_model = \"bernoulli\")   )    assess_convergence(mod)   assess_convergence(mod, parameter = \"theta\")   burnin(mod) <- 3000    plot(mod)   plot(mod, parameter = \"theta\") } # CHEKING FOR LABEL SWITCHING if (FALSE) {   # This example shows how to assess if label switching happens in BayesMallows   # We start by creating a directory in which csv files with individual   # cluster probabilities should be saved in each step of the MCMC algorithm   # NOTE: For computational efficiency, we use much fewer MCMC iterations than one   # would normally do.   dir.create(\"./test_label_switch\")   # Next, we go into this directory   setwd(\"./test_label_switch/\")   # For comparison, we run compute_mallows with and without saving the cluster   # probabilities The purpose of this is to assess the time it takes to save   # the cluster probabilites.   system.time(m <- compute_mallows(     setup_rank_data(rankings = sushi_rankings),     model_options = set_model_options(n_clusters = 3),     compute_options = set_compute_options(nmc = 500, save_ind_clus = FALSE),     verbose = TRUE))   # With this options, compute_mallows will save cluster_probs2.csv,   # cluster_probs3.csv, ..., cluster_probs[nmc].csv.   system.time(m <- compute_mallows(     setup_rank_data(rankings = sushi_rankings),     model_options = set_model_options(n_clusters = 3),     compute_options = set_compute_options(nmc = 500, save_ind_clus = TRUE),     verbose = TRUE))    # Next, we check convergence of alpha   assess_convergence(m)    # We set the burnin to 200   burnin <- 200    # Find all files that were saved. Note that the first file saved is   # cluster_probs2.csv   cluster_files <- list.files(pattern = \"cluster\\\\_probs[[:digit:]]+\\\\.csv\")    # Check the size of the files that were saved.   paste(sum(do.call(file.size, list(cluster_files))) * 1e-6, \"MB\")    # Find the iteration each file corresponds to, by extracting its number   iteration_number <- as.integer(     regmatches(x = cluster_files,m = regexpr(pattern = \"[0-9]+\", cluster_files)                ))   # Remove all files before burnin   file.remove(cluster_files[iteration_number <= burnin])   # Update the vector of files, after the deletion   cluster_files <- list.files(pattern = \"cluster\\\\_probs[[:digit:]]+\\\\.csv\")   # Create 3d array, with dimensions (iterations, assessors, clusters)   prob_array <- array(     dim = c(length(cluster_files), m$data$n_assessors, m$n_clusters))   # Read each file, adding to the right element of the array   for(i in seq_along(cluster_files)){     prob_array[i, , ] <- as.matrix(       read.csv(cluster_files[[i]], header = FALSE))   }    # Create an integer array of latent allocations, as this is required by   # label.switching   z <- subset(m$cluster_assignment, iteration > burnin)   z$value <- as.integer(gsub(\"Cluster \", \"\", z$value))   z$chain <- NULL   z <- reshape(z, direction = \"wide\", idvar = \"iteration\", timevar = \"assessor\")   z$iteration <- NULL   z <- as.matrix(z)    # Now apply Stephen's algorithm   library(label.switching)   switch_check <- label.switching(\"STEPHENS\", z = z,                                   K = m$n_clusters, p = prob_array)    # Check the proportion of cluster assignments that were switched   mean(apply(switch_check$permutations$STEPHENS, 1, function(x) {     !all(x == seq(1, m$n_clusters, by = 1))   }))    # Remove the rest of the csv files   file.remove(cluster_files)   # Move up one directory   setwd(\"..\")   # Remove the directory in which the csv files were saved   file.remove(\"./test_label_switch/\") }"},{"path":"/reference/compute_mallows_mixtures.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"Convenience function computing Mallows models varying numbers mixtures. useful deciding number mixtures use final model.","code":""},{"path":"/reference/compute_mallows_mixtures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"","code":"compute_mallows_mixtures(   n_clusters,   data,   model_options = set_model_options(),   compute_options = set_compute_options(),   priors = set_priors(),   initial_values = set_initial_values(),   pfun_estimate = NULL,   verbose = FALSE,   cl = NULL )"},{"path":"/reference/compute_mallows_mixtures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"n_clusters Integer vector specifying number clusters use. data object class \"BayesMallowsData\" returned setup_rank_data(). model_options object class \"BayesMallowsModelOptions\" returned set_model_options(). compute_options object class \"BayesMallowsComputeOptions\" returned set_compute_options(). priors object class \"BayesMallowsPriors\" returned set_priors(). initial_values object class \"BayesMallowsInitialValues\" returned set_initial_values(). pfun_estimate Object returned estimate_partition_function(). Defaults NULL, used footrule, Spearman, Ulam distances cardinalities available, cf. get_cardinalities(). verbose Logical specifying whether print progress Metropolis-Hastings algorithm. TRUE, notification printed every 1000th iteration. Defaults FALSE. cl Optional cluster returned parallel::makeCluster(). provided, chains run parallel, one node cl.","code":""},{"path":"/reference/compute_mallows_mixtures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"list Mallows models class BayesMallowsMixtures, one element number mixtures computed. object can studied plot_elbow().","code":""},{"path":"/reference/compute_mallows_mixtures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"n_clusters argument set_model_options() ignored calling compute_mallows_mixtures.","code":""},{"path":[]},{"path":"/reference/compute_mallows_mixtures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Mixtures of Mallows Models — compute_mallows_mixtures","text":"","code":"# SIMULATED CLUSTER DATA set.seed(1) n_clusters <- seq(from = 1, to = 5) models <- compute_mallows_mixtures(   n_clusters = n_clusters, data = setup_rank_data(cluster_data),   compute_options = set_compute_options(nmc = 2000, include_wcd = TRUE))  # There is good convergence for 1, 2, and 3 cluster, but not for 5. # Also note that there seems to be label switching around the 7000th iteration # for the 2-cluster solution. assess_convergence(models)  # We can create an elbow plot, suggesting that there are three clusters, exactly # as simulated. burnin(models) <- 1000 plot_elbow(models)   # We now fit a model with three clusters mixture_model <- compute_mallows(   data = setup_rank_data(cluster_data),   model_options = set_model_options(n_clusters = 3),   compute_options = set_compute_options(nmc = 2000))  # The trace plot for this model looks good. It seems to converge quickly. assess_convergence(mixture_model)  # We set the burnin to 500 burnin(mixture_model) <- 500  # We can now look at posterior quantities # Posterior of scale parameter alpha plot(mixture_model)  plot(mixture_model, parameter = \"rho\", items = 4:5)  # There is around 33 % probability of being in each cluster, in agreemeent # with the data simulating mechanism plot(mixture_model, parameter = \"cluster_probs\")  # We can also look at a cluster assignment plot plot(mixture_model, parameter = \"cluster_assignment\")   # DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA if (FALSE) {   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(     n_clusters = n_clusters, data = setup_rank_data(sushi_rankings),     compute_options = set_compute_options(include_wcd = TRUE))   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   burnin(models) <- 1000   plot_elbow(models)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., n_clusters = 5.    # Having chosen the number of clusters, we can now study the final model   # Rerun with 5 clusters   mixture_model <- compute_mallows(     rankings = sushi_rankings,     model_options = set_model_options(n_clusters = 5),     compute_options = set_compute_options(include_wcd = TRUE))   # Delete the models object to free some memory   rm(models)   # Set the burnin   burnin(mixture_model) <- 1000   # Plot the posterior distributions of alpha per cluster   plot(mixture_model)   # Compute the posterior interval of alpha per cluster   compute_posterior_intervals(mixture_model, parameter = \"alpha\")   # Plot the posterior distributions of cluster probabilities   plot(mixture_model, parameter = \"cluster_probs\")   # Plot the posterior probability of cluster assignment   plot(mixture_model, parameter = \"cluster_assignment\")   # Plot the posterior distribution of \"tuna roll\" in each cluster   plot(mixture_model, parameter = \"rho\", items = \"tuna roll\")   # Compute the cluster-wise CP consensus, and show one column per cluster   cp <- compute_consensus(mixture_model, type = \"CP\")   cp$cumprob <- NULL   stats::reshape(cp, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(cp$cluster))))    # Compute the MAP consensus, and show one column per cluster   map <- compute_consensus(mixture_model, type = \"MAP\")   map$probability <- NULL   stats::reshape(map, direction = \"wide\", idvar = \"map_ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(map$cluster))))    # RUNNING IN PARALLEL   # Computing Mallows models with different number of mixtures in parallel leads to   # considerably speedup   library(parallel)   cl <- makeCluster(detectCores() - 1)   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(     n_clusters = n_clusters,     rankings = sushi_rankings,     compute_options = set_compute_options(include_wcd = TRUE),     cl = cl)   stopCluster(cl) }"},{"path":"/reference/compute_mallows_sequentially.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"Compute posterior distributions parameters Bayesian Mallows model using sequential Monte Carlo. based algorithms developed Stein (2023) . function differs update_mallows() takes data , uses SMC fit model step--step. Used way, SMC alternative Metropolis-Hastings, may work better settings. addition, allows visualization learning process.","code":""},{"path":"/reference/compute_mallows_sequentially.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"","code":"compute_mallows_sequentially(   data,   initial_values,   model_options = set_model_options(),   smc_options = set_smc_options(),   compute_options = set_compute_options(),   priors = set_priors(),   pfun_estimate = NULL )"},{"path":"/reference/compute_mallows_sequentially.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"data list objects class \"BayesMallowsData\" returned setup_rank_data(). list element interpreted data belonging given timepoint. initial_values object class \"BayesMallowsPriorSamples\" returned sample_prior(). model_options object class \"BayesMallowsModelOptions\" returned set_model_options(). smc_options object class \"SMCOptions\" returned set_smc_options(). compute_options object class \"BayesMallowsComputeOptions\" returned set_compute_options(). priors object class \"BayesMallowsPriors\" returned set_priors(). pfun_estimate Object returned estimate_partition_function(). Defaults NULL, used footrule, Spearman, Ulam distances cardinalities available, cf. get_cardinalities().","code":""},{"path":"/reference/compute_mallows_sequentially.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"object class BayesMallowsSequential.","code":""},{"path":"/reference/compute_mallows_sequentially.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"function new, plotting functions tools visualizing posterior distribution yet work. See examples workarounds.","code":""},{"path":"/reference/compute_mallows_sequentially.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"Stein (2023). Sequential Inference Mallows Model. Ph.D. thesis, Lancaster University.","code":""},{"path":[]},{"path":"/reference/compute_mallows_sequentially.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate the Bayesian Mallows Model Sequentially — compute_mallows_sequentially","text":"","code":"# Observe one ranking at each of 12 timepoints library(ggplot2) data <- lapply(seq_len(nrow(potato_visual)), function(i) {   setup_rank_data(potato_visual[i, ]) })  initial_values <- sample_prior(   n = 200, n_items = 20,   priors = set_priors(gamma = 3, lambda = .1))  mod <- compute_mallows_sequentially(   data = data,   initial_values = initial_values,   smc_options = set_smc_options(n_particles = 500, mcmc_steps = 20))  # We can see the acceptance ratio of the move step for each timepoint: get_acceptance_ratios(mod) #> $alpha_acceptance #>         [,1] #>  [1,] 0.9656 #>  [2,] 0.9655 #>  [3,] 0.9652 #>  [4,] 0.9664 #>  [5,] 0.9668 #>  [6,] 0.9661 #>  [7,] 0.9652 #>  [8,] 0.9658 #>  [9,] 0.9665 #> [10,] 0.9683 #> [11,] 0.9660 #> [12,] 0.9671 #>  #> $rho_acceptance #>       [,1] #>  [1,]    1 #>  [2,]    1 #>  [3,]    1 #>  [4,]    1 #>  [5,]    1 #>  [6,]    1 #>  [7,]    1 #>  [8,]    1 #>  [9,]    1 #> [10,]    1 #> [11,]    1 #> [12,]    1 #>  #> $aug_acceptance #>       [,1] #>  [1,]  NaN #>  [2,]  NaN #>  [3,]  NaN #>  [4,]  NaN #>  [5,]  NaN #>  [6,]  NaN #>  [7,]  NaN #>  [8,]  NaN #>  [9,]  NaN #> [10,]  NaN #> [11,]  NaN #> [12,]  NaN #>   plot_dat <- data.frame(   n_obs = seq_along(data),   alpha_mean = apply(mod$alpha_samples, 2, mean),   alpha_sd = apply(mod$alpha_samples, 2, sd) )  # Visualize how the dispersion parameter is being learned as more data arrive ggplot(plot_dat, aes(x = n_obs, y = alpha_mean, ymin = alpha_mean - alpha_sd,                      ymax = alpha_mean + alpha_sd)) +   geom_line() +   geom_ribbon(alpha = .1) +   ylab(expression(alpha)) +   xlab(\"Observations\") +   theme_classic() +   scale_x_continuous(     breaks = seq(min(plot_dat$n_obs), max(plot_dat$n_obs), by = 1))   # Visualize the learning of the rank for a given item (item 1 in this example) plot_dat <- data.frame(   n_obs = seq_along(data),   rank_mean = apply(mod$rho_samples[1, , ], 2, mean),   rank_sd = apply(mod$rho_samples[1, , ], 2, sd) )  ggplot(plot_dat, aes(x = n_obs, y = rank_mean, ymin = rank_mean - rank_sd,                      ymax = rank_mean + rank_sd)) +   geom_line() +   geom_ribbon(alpha = .1) +   xlab(\"Observations\") +   ylab(expression(rho[1])) +   theme_classic() +   scale_x_continuous(     breaks = seq(min(plot_dat$n_obs), max(plot_dat$n_obs), by = 1))"},{"path":"/reference/compute_observation_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Frequency distribution of the ranking sequences — compute_observation_frequency","title":"Frequency distribution of the ranking sequences — compute_observation_frequency","text":"Construct frequency distribution distinct ranking sequences dataset individual rankings. can interest , also used speed computation providing observation_frequency argument compute_mallows().","code":""},{"path":"/reference/compute_observation_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Frequency distribution of the ranking sequences — compute_observation_frequency","text":"","code":"compute_observation_frequency(rankings)"},{"path":"/reference/compute_observation_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Frequency distribution of the ranking sequences — compute_observation_frequency","text":"rankings matrix individual rankings row.","code":""},{"path":"/reference/compute_observation_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Frequency distribution of the ranking sequences — compute_observation_frequency","text":"Numeric matrix distinct rankings row corresponding frequencies indicated last (n_items+1)-th column.","code":""},{"path":[]},{"path":"/reference/compute_observation_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Frequency distribution of the ranking sequences — compute_observation_frequency","text":"","code":"# Create example data. We set the burn-in and thinning very low # for the sampling to go fast data0 <- sample_mallows(rho0 = 1:5, alpha = 10, n_samples = 1000,                         burnin = 10, thinning = 1) # Find the frequency distribution compute_observation_frequency(rankings = data0) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    1    2    3    4    5  955 #> [2,]    1    2    3    5    4   19 #> [3,]    1    2    4    3    5   19 #> [4,]    1    3    2    4    5    1 #> [5,]    1    3    4    2    5    2 #> [6,]    2    1    3    4    5    4  # The function also works when the data have missing values rankings <- matrix(c(1, 2, 3, 4,                      1, 2, 4, NA,                      1, 2, 4, NA,                      3, 2, 1, 4,                      NA, NA, 2, 1,                      NA, NA, 2, 1,                      NA, NA, 2, 1,                      2, NA, 1, NA), ncol = 4, byrow = TRUE)  compute_observation_frequency(rankings) #>      [,1] [,2] [,3] [,4] [,5] #> [1,]   NA   NA    2    1    3 #> [2,]    1    2    3    4    1 #> [3,]    1    2    4   NA    2 #> [4,]    2   NA    1   NA    1 #> [5,]    3    2    1    4    1"},{"path":"/reference/compute_posterior_intervals.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Posterior Intervals — compute_posterior_intervals","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"Compute posterior intervals parameters interest.","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"","code":"compute_posterior_intervals(model_fit, ...)  # S3 method for BayesMallows compute_posterior_intervals(   model_fit,   parameter = c(\"alpha\", \"rho\", \"cluster_probs\"),   level = 0.95,   decimals = 3L,   ... )  # S3 method for SMCMallows compute_posterior_intervals(   model_fit,   parameter = c(\"alpha\", \"rho\"),   level = 0.95,   decimals = 3L,   ... )"},{"path":"/reference/compute_posterior_intervals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"model_fit model object. ... arguments. Currently used. parameter Character string defining parameter compute posterior intervals . One \"alpha\", \"rho\", \"cluster_probs\". Default \"alpha\". level Decimal number \\([0,1]\\) specifying confidence level. Defaults 0.95. decimals Integer specifying number decimals include posterior intervals mean median. Defaults 3.","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"function computes Highest Posterior Density Interval (HPDI), may discontinuous bimodal distributions, central posterior interval, simply defined quantiles posterior distribution.","code":""},{"path":"/reference/compute_posterior_intervals.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"references Rd macro \\insertAllCites help page.","code":""},{"path":[]},{"path":"/reference/compute_posterior_intervals.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Posterior Intervals — compute_posterior_intervals","text":"","code":"set.seed(1) model_fit <- compute_mallows(   setup_rank_data(potato_visual),   compute_options = set_compute_options(nmc = 3000, burnin = 1000))  # First we compute the interval for alpha compute_posterior_intervals(model_fit, parameter = \"alpha\") #>   parameter   mean median           hpdi central_interval #> 1     alpha 10.778 10.818 [9.165,12.566]   [8.929,12.351] # We can reduce the number decimals compute_posterior_intervals(model_fit, parameter = \"alpha\", decimals = 2) #>   parameter  mean median         hpdi central_interval #> 1     alpha 10.78  10.82 [9.16,12.57]     [8.93,12.35] # By default, we get a 95 % interval. We can change that to 99 %. compute_posterior_intervals(model_fit, parameter = \"alpha\", level = 0.99) #>   parameter   mean median           hpdi central_interval #> 1     alpha 10.778 10.818 [8.276,12.748]   [8.276,12.748] # We can also compute the posterior interval for the latent ranks rho compute_posterior_intervals(model_fit, parameter = \"rho\") #>    parameter item mean median      hpdi central_interval #> 1        rho   P1   10     10    [9,12]           [9,12] #> 2        rho   P2   17     17   [16,18]          [16,18] #> 3        rho   P3   19     19      [19]             [19] #> 4        rho   P4   16     16   [16,18]          [16,18] #> 5        rho   P5    9      9 [3][9,11]           [3,11] #> 6        rho   P6   15     15      [15]          [15,16] #> 7        rho   P7    6      6     [5,7]            [5,7] #> 8        rho   P8   20     20      [20]             [20] #> 9        rho   P9    3      3     [3,4]            [3,4] #> 10       rho  P10    4      4  [4,5][7]            [3,7] #> 11       rho  P11   11     11    [9,12]           [9,12] #> 12       rho  P12    1      1       [1]              [1] #> 13       rho  P13    2      2       [2]              [2] #> 14       rho  P14    7      7     [6,7]            [6,7] #> 15       rho  P15   18     18   [17,18]          [17,18] #> 16       rho  P16    8      8     [8,9]            [8,9] #> 17       rho  P17    5      5  [4,6][8]            [4,8] #> 18       rho  P18   14     14   [13,14]          [13,14] #> 19       rho  P19   12     12   [10,12]          [10,12] #> 20       rho  P20   13     13   [13,14]          [13,14]  if (FALSE) {   # Posterior intervals of cluster probabilities   model_fit <- compute_mallows(     setup_rank_data(sushi_rankings),     model_options = set_model_options(n_clusters = 5))   burnin(model_fit) <- 1000    compute_posterior_intervals(model_fit, parameter = \"alpha\")    compute_posterior_intervals(model_fit, parameter = \"cluster_probs\") }"},{"path":"/reference/compute_rank_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"Compute distance matrix rankings rank sequence.","code":""},{"path":"/reference/compute_rank_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"","code":"compute_rank_distance(   rankings,   rho,   metric = c(\"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\"),   observation_frequency = 1 )"},{"path":"/reference/compute_rank_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"rankings matrix size \\(N \\times n_{items}\\) rankings row. Alternatively, \\(N\\) equals 1, rankings can vector. rho ranking sequence. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\", \"footrule\" \"spearman\". observation_frequency Vector observation frequencies length \\(N\\), length 1, means ranks given weight. Defaults 1.","code":""},{"path":"/reference/compute_rank_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"vector distances according given metric.","code":""},{"path":"/reference/compute_rank_distance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"implementation Cayley distance based C++ translation Rankcluster::distCayley() (Grimonprez Jacques 2016) .","code":""},{"path":"/reference/compute_rank_distance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"Grimonprez Q, Jacques J (2016). Rankcluster: Model-Based Clustering Multivariate Partial Ranking Data. R package version 0.94, https://CRAN.R-project.org/package=Rankcluster.","code":""},{"path":[]},{"path":"/reference/compute_rank_distance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Distance between a set of rankings and a given rank sequence — compute_rank_distance","text":"","code":"# Distance between two vectors of rankings: compute_rank_distance(1:5, 5:1, metric = \"kendall\") #> [1] 10 compute_rank_distance(c(2, 4, 3, 6, 1, 7, 5), c(3, 5, 4, 7, 6, 2, 1), metric = \"cayley\") #> [1] 6 compute_rank_distance(c(4, 2, 3, 1), c(3, 4, 1, 2), metric = \"hamming\") #> [1] 4 compute_rank_distance(c(1, 3, 5, 7, 9, 8, 6, 4, 2), c(1, 2, 3, 4, 9, 8, 7, 6, 5), \"ulam\") #> [1] 4 compute_rank_distance(c(8, 7, 1, 2, 6, 5, 3, 4), c(1, 2, 8, 7, 3, 4, 6, 5), \"footrule\") #> [1] 32 compute_rank_distance(c(1, 6, 2, 5, 3, 4), c(4, 3, 5, 2, 6, 1), \"spearman\") #> [1] 54  # Difference between a metric and a vector # We set the burn-in and thinning too low for the example to run fast data0 <- sample_mallows(rho0 = 1:10, alpha = 20, n_samples = 1000,                         burnin = 10, thinning = 1)  compute_rank_distance(rankings = data0, rho = 1:10, metric = \"kendall\") #>    [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>   [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>   [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [297] 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [482] 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [519] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [593] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [667] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 #>  [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #>  [741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #>  [778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 #>  [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #>  [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #> [1000] 0"},{"path":"/reference/create_ranking.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert between ranking and ordering. — create_ranking","title":"Convert between ranking and ordering. — create_ranking","text":"create_ranking takes vector matrix ordered items orderings returns corresponding vector matrix ranked items. create_ordering takes vector matrix rankings rankings returns corresponding vector matrix ordered items.","code":""},{"path":"/reference/create_ranking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert between ranking and ordering. — create_ranking","text":"","code":"create_ranking(orderings)  create_ordering(rankings)"},{"path":"/reference/create_ranking.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert between ranking and ordering. — create_ranking","text":"orderings vector matrix ordered items. matrix, size N times n, N number samples n number items. rankings vector matrix ranked items. matrix, N times n, N number samples n number items.","code":""},{"path":"/reference/create_ranking.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert between ranking and ordering. — create_ranking","text":"vector matrix rankings. Missing orderings coded NA propagated corresponding missing ranks vice versa.","code":""},{"path":[]},{"path":"/reference/create_ranking.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert between ranking and ordering. — create_ranking","text":"","code":"# A vector of ordered items. orderings <- c(5, 1, 2, 4, 3) # Get ranks rankings <- create_ranking(orderings) # rankings is c(2, 3, 5, 4, 1) # Finally we convert it backed to an ordering. orderings_2 <- create_ordering(rankings) # Confirm that we get back what we had all.equal(orderings, orderings_2) #> [1] TRUE  # Next, we have a matrix with N = 19 samples # and n = 4 items set.seed(21) N <- 10 n <- 4 orderings <- t(replicate(N, sample.int(n))) # Convert the ordering to ranking rankings <- create_ranking(orderings) # Now we try to convert it back to an ordering. orderings_2 <- create_ordering(rankings) # Confirm that we get back what we had all.equal(orderings, orderings_2) #> [1] TRUE"},{"path":"/reference/estimate_partition_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Partition Function — estimate_partition_function","title":"Estimate Partition Function — estimate_partition_function","text":"Estimate logarithm partition function Mallows rank model. Choose importance sampling algorithm described (Vitelli et al. 2018)  IPFP algorithm computing asymptotic approximation described (Mukherjee 2016) . Note exact partition functions can computed efficiently Cayley, Hamming Kendall distances number items, footrule distances 50 items, Spearman distance 20 items, Ulam distance 60 items. function thus intended complement cases. See get_cardinalities() details.","code":""},{"path":"/reference/estimate_partition_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Partition Function — estimate_partition_function","text":"","code":"estimate_partition_function(   method = c(\"importance_sampling\", \"asymptotic\"),   alpha_vector,   n_items,   metric,   n_iterations,   K = 20,   cl = NULL )"},{"path":"/reference/estimate_partition_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Partition Function — estimate_partition_function","text":"method Character string specifying method use order estimate logarithm partition function. Available options \"importance_sampling\" \"asymptotic\". alpha_vector Numeric vector \\(\\alpha\\) values compute importance sampling estimate. n_items Integer specifying number items. metric Character string specifying distance measure use. Available options \"footrule\" \"spearman\" method = \"asymptotic\" addition \"cayley\", \"hamming\", \"kendall\", \"ulam\" method = \"importance_sampling\". n_iterations Integer specifying number iterations use. method = \"importance_sampling\", number Monte Carlo samples generate. method = \"asymptotic\", hand, represents number iterations IPFP algorithm. K Integer specifying parameter \\(K\\) asymptotic approximation partition function. used method = \"asymptotic\". Defaults 20. cl Optional computing cluster used parallelization, returned parallel::makeCluster(). Defaults NULL. used method = \"importance_sampling\".","code":""},{"path":"/reference/estimate_partition_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Partition Function — estimate_partition_function","text":"matrix two column number rows equal degree fitted polynomial approximating partition function. matrix can supplied pfun_estimate argument compute_mallows().","code":""},{"path":"/reference/estimate_partition_function.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimate Partition Function — estimate_partition_function","text":"Mukherjee S (2016). “Estimation exponential families permutations.” Annals Statistics, 44(2), 853--875. doi:10.1214/15-aos1389 . Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/estimate_partition_function.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate Partition Function — estimate_partition_function","text":"","code":"# IMPORTANCE SAMPLING # Let us estimate logZ(alpha) for 20 items with Spearman distance # We create a grid of alpha values from 0 to 10 alpha_vector <- seq(from = 0, to = 10, by = 0.5) n_items <- 20 metric <- \"spearman\"  # We start with 1e3 Monte Carlo samples fit1 <- estimate_partition_function(   method = \"importance_sampling\", alpha_vector = alpha_vector,   n_items = n_items, metric = metric, n_iterations = 1e3) # A matrix containing powers of alpha and regression coefficients is returned fit1 #>       [,1]          [,2] #>  [1,]    0  4.232411e+01 #>  [2,]    1 -5.106832e+01 #>  [3,]    2  5.711640e+01 #>  [4,]    3 -4.016860e+01 #>  [5,]    4  1.762612e+01 #>  [6,]    5 -4.974544e+00 #>  [7,]    6  9.175853e-01 #>  [8,]    7 -1.099142e-01 #>  [9,]    8  8.232800e-03 #> [10,]    9 -3.500635e-04 #> [11,]   10  6.446402e-06 # The approximated partition function can hence be obtained: estimate1 <-   vapply(alpha_vector, function(a) sum(a^fit1[, 1] * fit1[, 2]), numeric(1))  # Now let us recompute with 2e3 Monte Carlo samples fit2 <- estimate_partition_function(   method = \"importance_sampling\", alpha_vector = alpha_vector,   n_items = n_items, metric = metric, n_iterations = 2e3) estimate2 <-   vapply(alpha_vector, function(a) sum(a^fit2[, 1] * fit2[, 2]), numeric(1))  # ASYMPTOTIC APPROXIMATION # We can also compute an estimate using the asymptotic approximation fit3 <- estimate_partition_function(   method = \"asymptotic\", alpha_vector = alpha_vector,   n_items = n_items, metric = metric, n_iterations = 50) estimate3 <-   vapply(alpha_vector, function(a) sum(a^fit3[, 1] * fit3[, 2]), numeric(1))  # We can now plot the estimates side-by-side plot(alpha_vector, estimate1, type = \"l\", xlab = expression(alpha),      ylab = expression(log(Z(alpha)))) lines(alpha_vector, estimate2, col = 2) lines(alpha_vector, estimate3, col = 3) legend(x = 7, y = 40, legend = c(\"IS,1e3\", \"IS,2e3\", \"IPFP\"),        col = 1:3, lty = 1)   # We see that the two importance sampling estimates, which are unbiased, # overlap. The asymptotic approximation seems a bit off. It can be worthwhile # to try different values of n_iterations and K.  # When we are happy, we can provide the coefficient vector in the # pfun_estimate argument to compute_mallows # Say we choose to use the importance sampling estimate with 1e4 Monte Carlo samples: model_fit <- compute_mallows(   setup_rank_data(potato_visual),   model_options = set_model_options(metric = \"spearman\"),   compute_options = set_compute_options(nmc = 200),   pfun_estimate = fit2)"},{"path":"/reference/get_acceptance_ratios.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Acceptance Ratios — get_acceptance_ratios","title":"Get Acceptance Ratios — get_acceptance_ratios","text":"Extract acceptance ratio Metropolis-Hastings algorithm used compute_mallows() move step update_mallows() compute_mallows_sequentially(). Currently function returns values, refined future. burnin set call compute_mallows(), acceptance ratio iterations reported. Otherwise post burnin acceptance ratio reported. SMC method acceptance ratios apply iterations, since burnin needed .","code":""},{"path":"/reference/get_acceptance_ratios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Acceptance Ratios — get_acceptance_ratios","text":"","code":"get_acceptance_ratios(model_fit, ...)  # S3 method for BayesMallows get_acceptance_ratios(model_fit, ...)  # S3 method for SMCMallows get_acceptance_ratios(model_fit, ...)"},{"path":"/reference/get_acceptance_ratios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Acceptance Ratios — get_acceptance_ratios","text":"model_fit model fit. ... arguments passed methods. Currently used.","code":""},{"path":[]},{"path":"/reference/get_acceptance_ratios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Acceptance Ratios — get_acceptance_ratios","text":"","code":"set.seed(1) mod <- compute_mallows(   data = setup_rank_data(potato_visual),   compute_options = set_compute_options(burnin = 200) )  get_acceptance_ratios(mod) #> $alpha_acceptance #> $alpha_acceptance[[1]] #> [1] 0.7038889 #>  #>  #> $rho_acceptance #> $rho_acceptance[[1]] #> [1] 0.4716667 #>  #>  #> $aug_acceptance #> $aug_acceptance[[1]] #> [1] NaN #>  #>"},{"path":"/reference/get_cardinalities.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cardinalities for each distance — get_cardinalities","title":"Get cardinalities for each distance — get_cardinalities","text":"partition function Mallows model can defined computationally efficient manner $$Z_{n}(\\alpha) = \\sum_{d_{n} \\  \\mathcal{D}_{n}} N_{m,n} e^{-(\\alpha/n) d_{m}}$$. equation, \\(\\mathcal{D}_{n}\\) set containing possible distances given number items, \\(d_{m}\\) element set. Finally, \\(N_{m,n}\\) number possible configurations items give particular distance. See Irurozki et al. (2016) , Vitelli et al. (2018) , Crispino et al. (2023)  details. footrule distance, cardinalities come entry A062869 -Line Encyclopedia Integer Sequences (OEIS) (Sloane Inc. 2020) . Spearman distance, come entry A175929, Ulam distance entry A126065.","code":""},{"path":"/reference/get_cardinalities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cardinalities for each distance — get_cardinalities","text":"","code":"get_cardinalities(n_items, metric = c(\"footrule\", \"spearman\", \"ulam\"))"},{"path":"/reference/get_cardinalities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cardinalities for each distance — get_cardinalities","text":"n_items Number items. metric Distance function, one \"footrule\", \"spearman\", \"ulam\".","code":""},{"path":"/reference/get_cardinalities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get cardinalities for each distance — get_cardinalities","text":"dataframe two columns, distance contains distance support set current number items, .e., \\(d_{m}\\), value contains number values particular distances, .e., \\(N_{m,n}\\).","code":""},{"path":"/reference/get_cardinalities.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get cardinalities for each distance — get_cardinalities","text":"Crispino M, Mollica C, Astuti V, Tardella L (2023). “Efficient accurate inference mixtures Mallows models Spearman distance.” Statistics Computing, 33(5). ISSN 1573-1375, doi:10.1007/s11222-023-10266-8 , http://dx.doi.org/10.1007/s11222-023-10266-8. Irurozki E, Calvo B, Lozano JA (2016). “PerMallows: R Package Mallows Generalized Mallows Models.” Journal Statistical Software, 71(12), 1--30. doi:10.18637/jss.v071.i12 . Sloane NJA, Inc. TOF (2020). “-line encyclopedia integer sequences.” https://oeis.org/. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/get_cardinalities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get cardinalities for each distance — get_cardinalities","text":"","code":"# Extract the cardinalities for four items with footrule distance n_items <- 4 dat <- get_cardinalities(n_items) # Compute the partition function at alpha = 2 alpha <- 2 sum(dat$value * exp(-alpha / n_items * dat$distance)) #> [1] 3.572331 #' # We can confirm that it is correct by enumerating all possible combinations all <- expand.grid(1:4, 1:4, 1:4, 1:4) perms <- all[apply(all, 1, function(x) length(unique(x)) == 4), ] sum(apply(perms, 1, function(x) exp(-alpha / n_items * sum(abs(x - 1:4))))) #> [1] 3.572331  # We do the same for the Spearman distance dat <- get_cardinalities(n_items, metric = \"spearman\") sum(dat$value * exp(-alpha / n_items * dat$distance)) #> [1] 2.497585 #' # We can confirm that it is correct by enumerating all possible combinations sum(apply(perms, 1, function(x) exp(-alpha / n_items * sum((x - 1:4)^2)))) #> [1] 2.497585"},{"path":"/reference/get_mallows_loglik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"Compute either likelihood log-likelihood value Mallows mixture model parameters dataset complete rankings.","code":""},{"path":"/reference/get_mallows_loglik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"","code":"get_mallows_loglik(   rho,   alpha,   weights,   metric = c(\"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\"),   rankings,   observation_frequency = NULL,   log = TRUE )"},{"path":"/reference/get_mallows_loglik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"rho matrix size n_clusters x n_items whose rows permutations first n_items integers corresponding modal rankings Mallows mixture components. alpha vector n_clusters non-negative scalar specifying scale (precision) parameters Mallows mixture components. weights vector n_clusters non-negative scalars specifying mixture weights. metric Character string specifying distance measure use. Available options \"kendall\", \"cayley\", \"hamming\", \"ulam\", \"footrule\", \"spearman\". rankings matrix observed rankings row. observation_frequency vector observation frequencies (weights) apply row rankings. can speed computation large number assessors share rank pattern. Defaults NULL, means row rankings multiplied 1. provided, observation_frequency must number elements rows rankings, rankings NULL. log logical; TRUE, log-likelihood value returned, otherwise exponential. Default TRUE.","code":""},{"path":"/reference/get_mallows_loglik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"likelihood log-likelihood value corresponding one observed complete rankings Mallows mixture rank model distance specified metric argument.","code":""},{"path":[]},{"path":"/reference/get_mallows_loglik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood and log-likelihood evaluation for a Mallows mixture model — get_mallows_loglik","text":"","code":"# Simulate a sample from a Mallows model with the Kendall distance  n_items <- 5 mydata <- sample_mallows(   n_samples = 100,   rho0 = 1:n_items,   alpha0 = 10,   metric = \"kendall\")  # Compute the likelihood and log-likelihood values under the true model... get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = mydata,   log = FALSE   ) #> [1] 5.786706e-72  get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = mydata,   log = TRUE   ) #> [1] -164.0306  # or equivalently, by using the frequency distribution freq_distr <- compute_observation_frequency(mydata) get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = freq_distr[, 1:n_items],   observation_frequency = freq_distr[, n_items + 1],   log = FALSE   ) #> [1] 5.786706e-72  get_mallows_loglik(   rho = rbind(1:n_items, 1:n_items),   alpha = c(10, 10),   weights = c(0.5, 0.5),   metric = \"kendall\",   rankings = freq_distr[, 1:n_items],   observation_frequency = freq_distr[, n_items + 1],   log = TRUE   ) #> [1] -164.0306"},{"path":"/reference/get_transitive_closure.html","id":null,"dir":"Reference","previous_headings":"","what":"Get transitive closure — get_transitive_closure","title":"Get transitive closure — get_transitive_closure","text":"simple method showing transitive closure computed setup_rank_data().","code":""},{"path":"/reference/get_transitive_closure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get transitive closure — get_transitive_closure","text":"","code":"get_transitive_closure(rank_data)"},{"path":"/reference/get_transitive_closure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get transitive closure — get_transitive_closure","text":"rank_data object class \"BayesMallowsData\" returned setup_rank_data.","code":""},{"path":"/reference/get_transitive_closure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get transitive closure — get_transitive_closure","text":"dataframe transitive closure, .","code":""},{"path":[]},{"path":"/reference/get_transitive_closure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get transitive closure — get_transitive_closure","text":"","code":"# Original beach preferences head(beach_preferences) #>   assessor bottom_item top_item #> 1        1           2       15 #> 2        1           5        3 #> 3        1          13        3 #> 4        1           4        7 #> 5        1           5       15 #> 6        1          12        6 dim(beach_preferences) #> [1] 1442    3 # We then create a rank data object dat <- setup_rank_data(preferences = beach_preferences) # The transitive closure contains additional filled-in preferences implied # by the stated preferences. head(get_transitive_closure(dat)) #>   assessor bottom_item top_item #> 1        1           4        1 #> 2        1           5        1 #> 3        1           7        1 #> 4        1           4        3 #> 5        1           5        3 #> 6        1           7        3 dim(get_transitive_closure(dat)) #> [1] 2921    3"},{"path":"/reference/heat_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Heat plot of posterior probabilities — heat_plot","title":"Heat plot of posterior probabilities — heat_plot","text":"Generates heat plot items consensus ordering along horizontal axis ranking along vertical axis. color denotes posterior probability.","code":""},{"path":"/reference/heat_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heat plot of posterior probabilities — heat_plot","text":"","code":"heat_plot(model_fit, ...)"},{"path":"/reference/heat_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heat plot of posterior probabilities — heat_plot","text":"model_fit object type BayesMallows, returned compute_mallows(). ... Additional arguments passed methods. particular, type = \"CP\" type = \"MAP\" can passed compute_consensus() determine order items along horizontal axis.","code":""},{"path":"/reference/heat_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heat plot of posterior probabilities — heat_plot","text":"ggplot object.","code":""},{"path":[]},{"path":"/reference/heat_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Heat plot of posterior probabilities — heat_plot","text":"","code":"set.seed(1) model_fit <- compute_mallows(   setup_rank_data(potato_visual),   compute_options = set_compute_options(nmc = 2000, burnin = 500))  heat_plot(model_fit)  heat_plot(model_fit, type = \"MAP\")"},{"path":"/reference/plot.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Posterior Distributions — plot.BayesMallows","title":"Plot Posterior Distributions — plot.BayesMallows","text":"Plot posterior distributions parameters Mallows Rank model.","code":""},{"path":"/reference/plot.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Posterior Distributions — plot.BayesMallows","text":"","code":"# S3 method for BayesMallows plot(x, parameter = \"alpha\", items = NULL, ...)"},{"path":"/reference/plot.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Posterior Distributions — plot.BayesMallows","text":"x object type BayesMallows, returned compute_mallows(). parameter Character string defining parameter plot. Available options \"alpha\", \"rho\", \"cluster_probs\", \"cluster_assignment\", \"theta\". items items study diagnostic plot rho. Either vector item names, corresponding x$data$items vector indices. NULL, five items selected randomly. used parameter = \"rho\". ... arguments passed plot (used).","code":""},{"path":[]},{"path":"/reference/plot.BayesMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Posterior Distributions — plot.BayesMallows","text":"","code":"model_fit <- compute_mallows(setup_rank_data(potato_visual)) burnin(model_fit) <- 1000  # By default, the scale parameter \"alpha\" is plotted plot(model_fit)  # We can also plot the latent rankings \"rho\" plot(model_fit, parameter = \"rho\") #> Items not provided by user. Picking 5 at random.  # By default, a random subset of 5 items are plotted # Specify which items to plot in the items argument. plot(model_fit, parameter = \"rho\",      items = c(2, 4, 6, 9, 10, 20))  # When the ranking matrix has column names, we can also # specify these in the items argument. # In this case, we have the following names: colnames(potato_visual) #>  [1] \"P1\"  \"P2\"  \"P3\"  \"P4\"  \"P5\"  \"P6\"  \"P7\"  \"P8\"  \"P9\"  \"P10\" \"P11\" \"P12\" #> [13] \"P13\" \"P14\" \"P15\" \"P16\" \"P17\" \"P18\" \"P19\" \"P20\" # We can therefore get the same plot with the following call: plot(model_fit, parameter = \"rho\",      items = c(\"P2\", \"P4\", \"P6\", \"P9\", \"P10\", \"P20\"))   if (FALSE) {   # Plots of mixture parameters:   model_fit <- compute_mallows(     setup_rank_data(sushi_rankings),     model_options = set_model_options(n_clusters = 5))   burnin(model_fit) <- 1000   # Posterior distributions of the cluster probabilities   plot(model_fit, parameter = \"cluster_probs\")   # Cluster assignment plot. Color shows the probability of belonging to each   # cluster.   plot(model_fit, parameter = \"cluster_assignment\") }"},{"path":"/reference/plot.SMCMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot SMC Posterior Distributions — plot.SMCMallows","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"Plot posterior distributions SMC-Mallow parameters.","code":""},{"path":"/reference/plot.SMCMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"","code":"# S3 method for SMCMallows plot(x, parameter = \"alpha\", items = NULL, ...)"},{"path":"/reference/plot.SMCMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"x object type SMC-Mallows. parameter Character string defining parameter plot. Available options \"alpha\" \"rho\". items Either vector item names, vector indices. NULL, five items selected randomly. ... arguments passed plot (used).","code":""},{"path":"/reference/plot.SMCMallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"plot posterior distributions","code":""},{"path":[]},{"path":"/reference/plot.SMCMallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot SMC Posterior Distributions — plot.SMCMallows","text":"","code":"set.seed(1) # UPDATING A MALLOWS MODEL WITH NEW COMPLETE RANKINGS # Assume we first only observe the first four rankings in the potato_visual # dataset data_first_batch <- potato_visual[1:4, ]  # We start by fitting a model using Metropolis-Hastings mod_init <- compute_mallows(   data = setup_rank_data(data_first_batch),   compute_options = set_compute_options(nmc = 10000))  # Convergence seems good after no more than 2000 iterations assess_convergence(mod_init)  burnin(mod_init) <- 2000  # Next, assume we receive four more observations data_second_batch <- potato_visual[5:8, ]  # We can now update the model using sequential Monte Carlo mod_second <- update_mallows(   model = mod_init,   new_data = setup_rank_data(rankings = data_second_batch),   smc_options = set_smc_options(resampler = \"systematic\")   )  # This model now has a collection of particles approximating the posterior # distribution after the first and second batch # We can use all the posterior summary functions as we do for the model # based on compute_mallows(): plot(mod_second)  plot(mod_second, parameter = \"rho\", items = 1:4)  compute_posterior_intervals(mod_second) #>   parameter   mean median           hpdi central_interval #> 1     alpha 11.420 11.358 [9.118,13.980]   [9.154,14.096]  # Next, assume we receive the third and final batch of data. We can update # the model again data_third_batch <- potato_visual[9:12, ] mod_final <- update_mallows(   model = mod_second, new_data = setup_rank_data(rankings = data_third_batch))  # We can plot the same things as before plot(mod_final)  compute_consensus(mod_final) #>      cluster ranking item cumprob #> 1  Cluster 1       1  P12   0.999 #> 2  Cluster 1       2  P13   0.999 #> 3  Cluster 1       3   P9   0.983 #> 4  Cluster 1       4  P10   0.587 #> 5  Cluster 1       5  P17   0.915 #> 6  Cluster 1       6   P7   0.741 #> 7  Cluster 1       7  P14   0.999 #> 8  Cluster 1       8  P16   0.957 #> 9  Cluster 1       9   P5   0.467 #> 10 Cluster 1      10   P1   0.876 #> 11 Cluster 1      11  P11   0.969 #> 12 Cluster 1      12  P19   0.998 #> 13 Cluster 1      13  P18   0.554 #> 14 Cluster 1      14  P20   0.999 #> 15 Cluster 1      15   P6   0.955 #> 16 Cluster 1      16   P4   0.549 #> 17 Cluster 1      17   P2   0.778 #> 18 Cluster 1      18  P15   1.000 #> 19 Cluster 1      19   P3   1.000 #> 20 Cluster 1      20   P8   1.000  # UPDATING A MALLOWS MODEL WITH NEW OR UPDATED PARTIAL RANKINGS # The sequential Monte Carlo algorithm works for data with missing ranks as # well. This both includes the case where new users arrive with partial ranks, # and when previously seen users arrive with more complete data than they had # previously. # We illustrate for top-k rankings of the first 10 users in potato_visual potato_top_10 <- ifelse(potato_visual[1:10, ] > 10, NA_real_,                         potato_visual[1:10, ]) potato_top_12 <- ifelse(potato_visual[1:10, ] > 12, NA_real_,                         potato_visual[1:10, ]) potato_top_14 <- ifelse(potato_visual[1:10, ] > 14, NA_real_,                         potato_visual[1:10, ])  # We need the rownames as user IDs (user_ids <- 1:10) #>  [1]  1  2  3  4  5  6  7  8  9 10  # First, users provide top-10 rankings mod_init <- compute_mallows(   data = setup_rank_data(rankings = potato_top_10, user_ids = user_ids),   compute_options = set_compute_options(nmc = 10000))  # Convergence seems fine. We set the burnin to 2000. assess_convergence(mod_init)  burnin(mod_init) <- 2000  # Next assume the users update their rankings, so we have top-12 instead. mod1 <- update_mallows(   model = mod_init,   new_data = setup_rank_data(rankings = potato_top_12, user_ids = user_ids),   smc_options = set_smc_options(resampler = \"stratified\") )  plot(mod1)   # Then, assume we get even more data, this time top-14 rankings: mod2 <- update_mallows(   model = mod1,   new_data = setup_rank_data(rankings = potato_top_14, user_ids = user_ids) )  plot(mod2)   # Finally, assume a set of new users arrive, who have complete rankings. potato_new <- potato_visual[11:12, ] # We need to update the user IDs, to show that these users are different (user_ids <- 11:12) #> [1] 11 12  mod_final <- update_mallows(   model = mod2,   new_data = setup_rank_data(rankings = potato_new, user_ids = user_ids) )  plot(mod_final)   # We can also update models with pairwise preferences # We here start by running MCMC on the first 20 assessors of the beach data # A realistic application should run a larger number of iterations than we # do in this example. set.seed(3) dat <- subset(beach_preferences, assessor <= 20) mod <- compute_mallows(   data = setup_rank_data(     preferences = beach_preferences),   compute_options = set_compute_options(nmc = 3000, burnin = 1000) )  # Next we provide assessors 21 to 24 one at a time. for(i in 21:24){   mod <- update_mallows(     model = mod,     new_data = setup_rank_data(       preferences = subset(beach_preferences, assessor == i),       user_ids = i, shuffle_unranked = TRUE),     smc_options = set_smc_options(latent_sampling_lag = 0)   ) }  # Compared to running full MCMC, there is a downward bias in the scale # parameter. This can be alleviated by increasing the number of particles, # MCMC steps, and the latent sampling lag. plot(mod)  compute_consensus(mod) #>      cluster ranking    item cumprob #> 1  Cluster 1       1  Item 6   0.430 #> 2  Cluster 1       2  Item 9   0.752 #> 3  Cluster 1       3  Item 3   0.488 #> 4  Cluster 1       4 Item 11   0.808 #> 5  Cluster 1       5 Item 15   0.526 #> 6  Cluster 1       6 Item 10   0.560 #> 7  Cluster 1       7  Item 1   0.708 #> 8  Cluster 1       8  Item 5   0.705 #> 9  Cluster 1       9 Item 13   0.696 #> 10 Cluster 1      10  Item 7   0.553 #> 11 Cluster 1      11  Item 8   0.426 #> 12 Cluster 1      12 Item 12   0.536 #> 13 Cluster 1      13  Item 4   0.738 #> 14 Cluster 1      14 Item 14   0.885 #> 15 Cluster 1      15  Item 2   1.000"},{"path":"/reference/plot_elbow.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Within-Cluster Sum of Distances — plot_elbow","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"Plot within-cluster sum distances corresponding cluster consensus different number clusters. function useful selecting number mixture.","code":""},{"path":"/reference/plot_elbow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"","code":"plot_elbow(...)"},{"path":"/reference/plot_elbow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"... One objects returned compute_mallows(), separated comma, list objects. Typically, object run different number mixtures, specified n_clusters argument compute_mallows(). Alternatively object returned compute_mallows_mixtures().","code":""},{"path":"/reference/plot_elbow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"boxplot number clusters horizontal axis -cluster sum distances vertical axis.","code":""},{"path":[]},{"path":"/reference/plot_elbow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Within-Cluster Sum of Distances — plot_elbow","text":"","code":"# SIMULATED CLUSTER DATA set.seed(1) n_clusters <- seq(from = 1, to = 5) models <- compute_mallows_mixtures(   n_clusters = n_clusters, data = setup_rank_data(cluster_data),   compute_options = set_compute_options(nmc = 2000, include_wcd = TRUE))  # There is good convergence for 1, 2, and 3 cluster, but not for 5. # Also note that there seems to be label switching around the 7000th iteration # for the 2-cluster solution. assess_convergence(models)  # We can create an elbow plot, suggesting that there are three clusters, exactly # as simulated. burnin(models) <- 1000 plot_elbow(models)   # We now fit a model with three clusters mixture_model <- compute_mallows(   data = setup_rank_data(cluster_data),   model_options = set_model_options(n_clusters = 3),   compute_options = set_compute_options(nmc = 2000))  # The trace plot for this model looks good. It seems to converge quickly. assess_convergence(mixture_model)  # We set the burnin to 500 burnin(mixture_model) <- 500  # We can now look at posterior quantities # Posterior of scale parameter alpha plot(mixture_model)  plot(mixture_model, parameter = \"rho\", items = 4:5)  # There is around 33 % probability of being in each cluster, in agreemeent # with the data simulating mechanism plot(mixture_model, parameter = \"cluster_probs\")  # We can also look at a cluster assignment plot plot(mixture_model, parameter = \"cluster_assignment\")   # DETERMINING THE NUMBER OF CLUSTERS IN THE SUSHI EXAMPLE DATA if (FALSE) {   # Let us look at any number of clusters from 1 to 10   # We use the convenience function compute_mallows_mixtures   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(     n_clusters = n_clusters, data = setup_rank_data(sushi_rankings),     compute_options = set_compute_options(include_wcd = TRUE))   # models is a list in which each element is an object of class BayesMallows,   # returned from compute_mallows   # We can create an elbow plot   burnin(models) <- 1000   plot_elbow(models)   # We then select the number of cluster at a point where this plot has   # an \"elbow\", e.g., n_clusters = 5.    # Having chosen the number of clusters, we can now study the final model   # Rerun with 5 clusters   mixture_model <- compute_mallows(     rankings = sushi_rankings,     model_options = set_model_options(n_clusters = 5),     compute_options = set_compute_options(include_wcd = TRUE))   # Delete the models object to free some memory   rm(models)   # Set the burnin   burnin(mixture_model) <- 1000   # Plot the posterior distributions of alpha per cluster   plot(mixture_model)   # Compute the posterior interval of alpha per cluster   compute_posterior_intervals(mixture_model, parameter = \"alpha\")   # Plot the posterior distributions of cluster probabilities   plot(mixture_model, parameter = \"cluster_probs\")   # Plot the posterior probability of cluster assignment   plot(mixture_model, parameter = \"cluster_assignment\")   # Plot the posterior distribution of \"tuna roll\" in each cluster   plot(mixture_model, parameter = \"rho\", items = \"tuna roll\")   # Compute the cluster-wise CP consensus, and show one column per cluster   cp <- compute_consensus(mixture_model, type = \"CP\")   cp$cumprob <- NULL   stats::reshape(cp, direction = \"wide\", idvar = \"ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(cp$cluster))))    # Compute the MAP consensus, and show one column per cluster   map <- compute_consensus(mixture_model, type = \"MAP\")   map$probability <- NULL   stats::reshape(map, direction = \"wide\", idvar = \"map_ranking\",                  timevar = \"cluster\", varying = list(as.character(unique(map$cluster))))    # RUNNING IN PARALLEL   # Computing Mallows models with different number of mixtures in parallel leads to   # considerably speedup   library(parallel)   cl <- makeCluster(detectCores() - 1)   n_clusters <- seq(from = 1, to = 10)   models <- compute_mallows_mixtures(     n_clusters = n_clusters,     rankings = sushi_rankings,     compute_options = set_compute_options(include_wcd = TRUE),     cl = cl)   stopCluster(cl) }"},{"path":"/reference/plot_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"Plot posterior probability, per item, ranked among top-\\(k\\) assessor. plot useful data take form pairwise preferences.","code":""},{"path":"/reference/plot_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"","code":"plot_top_k(model_fit, k = 3)"},{"path":"/reference/plot_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"model_fit object type BayesMallows, returned compute_mallows(). k Integer specifying k top-\\(k\\).","code":""},{"path":[]},{"path":"/reference/plot_top_k.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Top-k Rankings with Pairwise Preferences — plot_top_k","text":"","code":"set.seed(1) # We use the example dataset with beach preferences. Se the documentation to # compute_mallows for how to assess the convergence of the algorithm # We need to save the augmented data, so setting this option to TRUE model_fit <- compute_mallows(   data = setup_rank_data(preferences = beach_preferences),   compute_options = set_compute_options(     nmc = 1000, burnin = 500, save_aug = TRUE)) # By default, the probability of being top-3 is plotted # The default plot gives the probability for each assessor plot_top_k(model_fit)  # We can also plot the probability of being top-5, for each item plot_top_k(model_fit, k = 5)  # We get the underlying numbers with predict_top_k probs <- predict_top_k(model_fit) # To find all items ranked top-3 by assessors 1-3 with probability more than 80 %, # we do subset(probs, assessor %in% 1:3 & prob > 0.8) #>     assessor    item  prob #> 301        1  Item 6 1.000 #> 303        3  Item 6 0.994 #> 483        3  Item 9 1.000 #> 601        1 Item 11 1.000  # We can also plot for clusters model_fit <- compute_mallows(   data = setup_rank_data(preferences = beach_preferences),   model_options = set_model_options(n_clusters = 3),   compute_options = set_compute_options(     nmc = 1000, burnin = 500, save_aug = TRUE)   ) # The modal ranking in general differs between clusters, but the plot still # represents the posterior distribution of each user's augmented rankings. plot_top_k(model_fit)"},{"path":"/reference/potato_true_ranking.html","id":null,"dir":"Reference","previous_headings":"","what":"True ranking of the weights of 20 potatoes. — potato_true_ranking","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"True ranking weights 20 potatoes.","code":""},{"path":"/reference/potato_true_ranking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"","code":"potato_true_ranking"},{"path":"/reference/potato_true_ranking.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"object class numeric length 20.","code":""},{"path":"/reference/potato_true_ranking.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"True ranking of the weights of 20 potatoes. — potato_true_ranking","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/potato_visual.html","id":null,"dir":"Reference","previous_headings":"","what":"Potato weights assessed visually — potato_visual","title":"Potato weights assessed visually — potato_visual","text":"Result ranking potatoes weight, assessors allowed inspected potatoes visually. 12 assessors ranked 20 potatoes.","code":""},{"path":"/reference/potato_visual.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Potato weights assessed visually — potato_visual","text":"","code":"potato_visual"},{"path":"/reference/potato_visual.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Potato weights assessed visually — potato_visual","text":"object class matrix (inherits array) 12 rows 20 columns.","code":""},{"path":"/reference/potato_visual.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Potato weights assessed visually — potato_visual","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/potato_weighing.html","id":null,"dir":"Reference","previous_headings":"","what":"Potato weights assessed by hand — potato_weighing","title":"Potato weights assessed by hand — potato_weighing","text":"Result ranking potatoes weight, assessors allowed lift potatoes. 12 assessors ranked 20 potatoes.","code":""},{"path":"/reference/potato_weighing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Potato weights assessed by hand — potato_weighing","text":"","code":"potato_weighing"},{"path":"/reference/potato_weighing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Potato weights assessed by hand — potato_weighing","text":"object class matrix (inherits array) 12 rows 20 columns.","code":""},{"path":"/reference/potato_weighing.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Potato weights assessed by hand — potato_weighing","text":"Liu Q, Crispino M, Scheel , Vitelli V, Frigessi (2019). “Model-Based Learning Preference Data.” Annual Review Statistics Application, 6(1). doi:10.1146/annurev-statistics-031017-100213 .","code":""},{"path":[]},{"path":"/reference/predict_top_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"Predict posterior probability, per item, ranked among top-\\(k\\) assessor. useful data take form pairwise preferences.","code":""},{"path":"/reference/predict_top_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"","code":"predict_top_k(model_fit, k = 3)"},{"path":"/reference/predict_top_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"model_fit object type BayesMallows, returned compute_mallows(). k Integer specifying k top-\\(k\\).","code":""},{"path":"/reference/predict_top_k.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"dataframe columns assessor, item, prob, row states probability given assessor rates given item among top-\\(k\\).","code":""},{"path":[]},{"path":"/reference/predict_top_k.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Top-k Rankings with Pairwise Preferences — predict_top_k","text":"","code":"set.seed(1) # We use the example dataset with beach preferences. Se the documentation to # compute_mallows for how to assess the convergence of the algorithm # We need to save the augmented data, so setting this option to TRUE model_fit <- compute_mallows(   data = setup_rank_data(preferences = beach_preferences),   compute_options = set_compute_options(     nmc = 1000, burnin = 500, save_aug = TRUE)) # By default, the probability of being top-3 is plotted # The default plot gives the probability for each assessor plot_top_k(model_fit)  # We can also plot the probability of being top-5, for each item plot_top_k(model_fit, k = 5)  # We get the underlying numbers with predict_top_k probs <- predict_top_k(model_fit) # To find all items ranked top-3 by assessors 1-3 with probability more than 80 %, # we do subset(probs, assessor %in% 1:3 & prob > 0.8) #>     assessor    item  prob #> 301        1  Item 6 1.000 #> 303        3  Item 6 0.994 #> 483        3  Item 9 1.000 #> 601        1 Item 11 1.000  # We can also plot for clusters model_fit <- compute_mallows(   data = setup_rank_data(preferences = beach_preferences),   model_options = set_model_options(n_clusters = 3),   compute_options = set_compute_options(     nmc = 1000, burnin = 500, save_aug = TRUE)   ) # The modal ranking in general differs between clusters, but the plot still # represents the posterior distribution of each user's augmented rankings. plot_top_k(model_fit)"},{"path":"/reference/print.BayesMallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for BayesMallows Objects — print.BayesMallows","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"default print method BayesMallows object.","code":""},{"path":"/reference/print.BayesMallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"","code":"# S3 method for BayesMallows print(x, ...)  # S3 method for BayesMallowsMixtures print(x, ...)  # S3 method for SMCMallows print(x, ...)"},{"path":"/reference/print.BayesMallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for BayesMallows Objects — print.BayesMallows","text":"x object type BayesMallows, returned compute_mallows(). ... arguments passed print (used).","code":""},{"path":[]},{"path":"/reference/rmallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Mallows distribution. — rmallows","title":"Sample from the Mallows distribution. — rmallows","text":"Sample Mallows distribution arbitrary distance metric using Metropolis-Hastings algorithm.","code":""},{"path":"/reference/rmallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Mallows distribution. — rmallows","text":"","code":"rmallows(   rho0,   alpha0,   n_samples,   burnin,   thinning,   leap_size = 1L,   metric = \"footrule\" )"},{"path":"/reference/rmallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Mallows distribution. — rmallows","text":"rho0 Vector specifying latent consensus ranking. alpha0 Scalar specifying scale parameter. n_samples Integer specifying number random samples generate. burnin Integer specifying number iterations discard burn-. thinning Integer specifying number MCMC iterations perform time random rank vector sampled. leap_size Integer specifying step size leap--shift proposal distribution. metric Character string specifying distance measure use. Available options \"footrule\" (default), \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\".","code":""},{"path":"/reference/rmallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample from the Mallows distribution. — rmallows","text":"references Rd macro \\insertAllCites help page.","code":""},{"path":"/reference/sample_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Samples from the Mallows Rank Model — sample_mallows","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"Generate random samples Mallows Rank Model (Mallows 1957)  consensus ranking \\(\\rho\\) scale parameter \\(\\alpha\\). samples obtained running Metropolis-Hastings algorithm described Appendix C Vitelli et al. (2018) .","code":""},{"path":"/reference/sample_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"","code":"sample_mallows(   rho0,   alpha0,   n_samples,   leap_size = max(1L, floor(n_items/5)),   metric = \"footrule\",   diagnostic = FALSE,   burnin = ifelse(diagnostic, 0, 1000),   thinning = ifelse(diagnostic, 1, 1000),   items_to_plot = NULL,   max_lag = 1000L )"},{"path":"/reference/sample_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"rho0 Vector specifying latent consensus ranking Mallows rank model. alpha0 Scalar specifying scale parameter Mallows rank model. n_samples Integer specifying number random samples generate. diagnostic = TRUE, number must larger 1. leap_size Integer specifying step size leap--shift proposal distribution. metric Character string specifying distance measure use. Available options \"footrule\" (default), \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". See also rmm function PerMallows package (Irurozki et al. 2016)  sampling Mallows model Cayley, Hamming, Kendall, Ulam distances. diagnostic Logical specifying whether output convergence diagnostics. TRUE, diagnostic plot printed, together returned samples. burnin Integer specifying number iterations discard burn-. Defaults 1000 diagnostic = FALSE, else 0. thinning Integer specifying number MCMC iterations perform time random rank vector sampled. Defaults 1000 diagnostic = FALSE, else 1. items_to_plot Integer vector used diagnostic = TRUE, order specify items plot diagnostic output. provided, 5 items picked random. max_lag Integer specifying maximum lag use computation autocorrelation. Defaults 1000L. argument passed stats::acf. used diagnostic = TRUE.","code":""},{"path":"/reference/sample_mallows.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"Irurozki E, Calvo B, Lozano JA (2016). “PerMallows: R Package Mallows Generalized Mallows Models.” Journal Statistical Software, 71(12), 1--30. doi:10.18637/jss.v071.i12 . Mallows CL (1957). “Non-Null Ranking Models. .” Biometrika, 44(1/2), 114--130. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/sample_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random Samples from the Mallows Rank Model — sample_mallows","text":"","code":"# Sample 100 random rankings from a Mallows distribution with footrule distance set.seed(1) # Number of items n_items <- 15 # Set the consensus ranking rho0 <- seq(from = 1, to = n_items, by = 1) # Set the scale alpha0 <- 10 # Number of samples n_samples <- 100 # We first do a diagnostic run, to find the thinning and burnin to use # We set n_samples to 1000, in order to run 1000 diagnostic iterations. test <- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,                        n_samples = 1000, burnin = 1, thinning = 1) #> Items not provided by user. Picking 5 at random.  #> [1] \"Press [enter] to see the next plot\"  # When items_to_plot is not set, 5 items are picked at random. We can change this. # We can also reduce the number of lags computed in the autocorrelation plots test <- sample_mallows(rho0 = rho0, alpha0 = alpha0, diagnostic = TRUE,                        n_samples = 1000, burnin = 1, thinning = 1,                        items_to_plot = c(1:3, 10, 15), max_lag = 500)  #> [1] \"Press [enter] to see the next plot\"  # From the autocorrelation plot, it looks like we should use # a thinning of at least 200. We set thinning = 1000 to be safe, # since the algorithm in any case is fast. The Markov Chain # seems to mix quickly, but we set the burnin to 1000 to be safe. # We now run sample_mallows again, to get the 100 samples we want: samples <- sample_mallows(rho0 = rho0, alpha0 = alpha0, n_samples = 100,                           burnin = 1000, thinning = 1000) # The samples matrix now contains 100 rows with rankings of 15 items. # A good diagnostic, in order to confirm that burnin and thinning are set high # enough, is to run compute_mallows on the samples model_fit <- compute_mallows(   setup_rank_data(samples),   compute_options = set_compute_options(nmc = 10000)) # The highest posterior density interval covers alpha0 = 10. burnin(model_fit) <- 2000 compute_posterior_intervals(model_fit, parameter = \"alpha\") #>   parameter  mean median           hpdi central_interval #> 1     alpha 9.840  9.838 [9.330,10.360]   [9.338,10.373]"},{"path":"/reference/sample_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from prior distribution — sample_prior","title":"Sample from prior distribution — sample_prior","text":"Function obtain samples prior distributions Bayesian Mallows model. Intended given update_mallows().","code":""},{"path":"/reference/sample_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from prior distribution — sample_prior","text":"","code":"sample_prior(n, n_items, priors = set_priors())"},{"path":"/reference/sample_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from prior distribution — sample_prior","text":"n integer specifying number samples take. n_items integer specifying number items ranked. priors object class \"BayesMallowsPriors\" returned set_priors().","code":""},{"path":"/reference/sample_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from prior distribution — sample_prior","text":"object class \"BayesMallowsPriorSample\", containing n independent samples \\(\\alpha\\) \\(\\rho\\).","code":""},{"path":[]},{"path":"/reference/sample_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from prior distribution — sample_prior","text":"","code":"# We can use a collection of particles from the prior distribution as # initial values for the sequential Monte Carlo algorithm. # Here we start by drawing 1000 particles from the priors, using default # parameters. prior_samples <- sample_prior(1000, ncol(sushi_rankings)) # Next, we provide the prior samples to update_mallws(), together # with the first five rows of the sushi dataset model1 <- update_mallows(   model = prior_samples,   new_data = setup_rank_data(sushi_rankings[1:5, ])) plot(model1)   # We keep adding more data model2 <- update_mallows(   model = model1,   new_data = setup_rank_data(sushi_rankings[6:10, ])) plot(model2)   model3 <- update_mallows(   model = model2,   new_data = setup_rank_data(sushi_rankings[11:15, ])) plot(model3)"},{"path":"/reference/set_compute_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify options for computation — set_compute_options","title":"Specify options for computation — set_compute_options","text":"Set parameters related Metropolis-Hastings algorithm.","code":""},{"path":"/reference/set_compute_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify options for computation — set_compute_options","text":"","code":"set_compute_options(   nmc = 2000,   burnin = NULL,   alpha_prop_sd = 0.1,   rho_proposal = c(\"ls\", \"swap\"),   leap_size = 1,   aug_method = c(\"uniform\", \"pseudo\"),   pseudo_aug_metric = c(\"footrule\", \"spearman\"),   swap_leap = 1,   alpha_jump = 1,   aug_thinning = 1,   clus_thinning = 1,   rho_thinning = 1,   include_wcd = FALSE,   save_aug = FALSE,   save_ind_clus = FALSE )"},{"path":"/reference/set_compute_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify options for computation — set_compute_options","text":"nmc Integer specifying number iteration Metropolis-Hastings algorithm run. Defaults 2000. See assess_convergence() tools check convergence Markov chain. burnin Integer defining number samples discard. Defaults NULL, means burn-set. alpha_prop_sd Numeric value specifying \\(\\sigma\\) parameter lognormal proposal distribution used \\(\\alpha\\) Metropolis-Hastings algorithm. logarithm proposed samples standard deviation given alpha_prop_sd. Defaults 0.1. rho_proposal Character string specifying proposal distribution modal ranking \\(\\rho\\). Defaults \"ls\", means leap--shift algorithm Vitelli et al. (2018)  used. option \"swap\", means swap proposal Crispino et al. (2019)  used instead. leap_size Integer specifying step size distribution defined rho_proposal proposing new latent ranks \\(rho\\). Defaults 1. aug_method Augmentation proposal use missing data. One \"pseudo\" \"uniform\". Defaults \"uniform\", means new augmented rankings proposed sampling uniformly set available ranks, see Section 4 Vitelli et al. (2018) . Setting argument \"pseudo\" instead, means pseudo-likelihood proposal defined Chapter 5 Stein (2023)  used instead. pseudo_aug_metric String defining metric used pseudo-likelihood proposal. used aug_method = \"pseudo\". Can either \"footrule\" \"spearman\", defaults \"footrule\". swap_leap Integer specifying leap size swap proposal used proposing latent ranks case non-transitive pairwise preference data. Note leap size swap proposal used proposal modal ranking \\(\\rho\\) given leap_size argument . alpha_jump Integer specifying many times sample \\(\\rho\\) sampling \\(\\alpha\\). words, many times jump \\(\\alpha\\) sampling \\(\\rho\\), possibly parameters like augmented ranks \\(\\tilde{R}\\) cluster assignments \\(z\\). Setting alpha_jump high number can speed computation time, reducing number times partition function Mallows model needs computed. Defaults 1. aug_thinning Integer specifying thinning saving augmented data. used save_aug = TRUE. Defaults 1. clus_thinning Integer specifying thinning applied cluster assignments cluster probabilities. Defaults 1. rho_thinning Integer specifying thinning rho performed Metropolis- Hastings algorithm. Defaults 1. compute_mallows save every rho_thinningth value \\(\\rho\\). include_wcd Logical indicating whether store within-cluster distances computed Metropolis-Hastings algorithm. Defaults FALSE. Setting include_wcd = TRUE useful deciding number mixture components include, required plot_elbow(). save_aug Logical specifying whether save augmented rankings every aug_thinningth iteration, case missing data pairwise preferences. Defaults FALSE. Saving augmented data useful predicting rankings assessor give items yet ranked, required plot_top_k(). save_ind_clus Whether save individual cluster probabilities step. results csv files cluster_probs1.csv, cluster_probs2.csv, ..., saved calling directory. option may slow code considerably, necessary detecting label switching using Stephen's algorithm.","code":""},{"path":"/reference/set_compute_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify options for computation — set_compute_options","text":"object class \"BayesMallowsComputeOptions\", provided compute_options argument compute_mallows(), compute_mallows_mixtures(), update_mallows().","code":""},{"path":"/reference/set_compute_options.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Specify options for computation — set_compute_options","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 . Stein (2023). Sequential Inference Mallows Model. Ph.D. thesis, Lancaster University. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/set_initial_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Set initial values of scale parameter and modal ranking — set_initial_values","title":"Set initial values of scale parameter and modal ranking — set_initial_values","text":"Set initial values used Metropolis-Hastings algorithm.","code":""},{"path":"/reference/set_initial_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set initial values of scale parameter and modal ranking — set_initial_values","text":"","code":"set_initial_values(rho_init = NULL, alpha_init = 1)"},{"path":"/reference/set_initial_values.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set initial values of scale parameter and modal ranking — set_initial_values","text":"rho_init Numeric vector specifying initial value latent consensus ranking \\(\\rho\\). Defaults NULL, means initial value set randomly. rho_init provided n_clusters > 1, mixture component \\(\\rho_{c}\\) gets initial value. alpha_init Numeric value specifying initial value scale parameter \\(\\alpha\\). Defaults 1. n_clusters > 1, mixture component \\(\\alpha_{c}\\) gets initial value. chains run parallel, providing argument cl = cl, alpha_init can vector length length(cl), element becomes initial value given chain.","code":""},{"path":"/reference/set_initial_values.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set initial values of scale parameter and modal ranking — set_initial_values","text":"object class \"BayesMallowsInitialValues\", provided initial_values argument compute_mallows() compute_mallows_mixtures().","code":""},{"path":[]},{"path":"/reference/set_model_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set options for Bayesian Mallows model — set_model_options","title":"Set options for Bayesian Mallows model — set_model_options","text":"Specify various model options Bayesian Mallows model.","code":""},{"path":"/reference/set_model_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set options for Bayesian Mallows model — set_model_options","text":"","code":"set_model_options(   metric = c(\"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\"),   n_clusters = 1,   error_model = c(\"none\", \"bernoulli\") )"},{"path":"/reference/set_model_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set options for Bayesian Mallows model — set_model_options","text":"metric character string specifying distance metric use Bayesian Mallows Model. Available options \"footrule\", \"spearman\", \"cayley\", \"hamming\", \"kendall\", \"ulam\". distance given metric also used compute within-cluster distances, include_wcd = TRUE. n_clusters Integer specifying number clusters, .e., number mixture components use. Defaults 1L, means clustering performed. See compute_mallows_mixtures() convenience function computing several models varying numbers mixtures. error_model Character string specifying model use inconsistent rankings. Defaults \"none\", means inconsistent rankings allowed. moment, available option \"bernoulli\", means Bernoulli error model used. See Crispino et al. (2019)  definition Bernoulli model.","code":""},{"path":"/reference/set_model_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set options for Bayesian Mallows model — set_model_options","text":"object class \"BayesMallowsModelOptions\", provided model_options argument compute_mallows(), compute_mallows_mixtures(), update_mallows().","code":""},{"path":"/reference/set_model_options.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set options for Bayesian Mallows model — set_model_options","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 .","code":""},{"path":[]},{"path":"/reference/set_priors.html","id":null,"dir":"Reference","previous_headings":"","what":"Set prior parameters for Bayesian Mallows model — set_priors","title":"Set prior parameters for Bayesian Mallows model — set_priors","text":"Set values related prior distributions Bayesian Mallows model.","code":""},{"path":"/reference/set_priors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set prior parameters for Bayesian Mallows model — set_priors","text":"","code":"set_priors(gamma = 1, lambda = 0.001, psi = 10, kappa = c(1, 3))"},{"path":"/reference/set_priors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set prior parameters for Bayesian Mallows model — set_priors","text":"gamma Strictly positive numeric value specifying shape parameter gamma prior distribution \\(\\alpha\\). Defaults 1, thus recovering exponential prior distribution used (Vitelli et al. 2018) . lambda Strictly positive numeric value specifying rate parameter gamma prior distribution \\(\\alpha\\). Defaults 0.001. n_cluster > 1, mixture component \\(\\alpha_{c}\\) prior distribution. psi Positive integer specifying concentration parameter \\(\\psi\\) Dirichlet prior distribution used cluster probabilities \\(\\tau_{1}, \\tau_{2}, \\dots, \\tau_{C}\\), \\(C\\) value n_clusters. Defaults 10L. n_clusters = 1, argument used. kappa Hyperparameters truncated Beta prior used error probability \\(\\theta\\) Bernoulli error model. prior form \\(\\pi(\\theta) = \\theta^{\\kappa_{1}} (1 - \\theta)^{\\kappa_{2}}\\). Defaults c(1, 3), means \\(\\theta\\) priori expected closer zero 0.5. See (Crispino et al. 2019)  details.","code":""},{"path":"/reference/set_priors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set prior parameters for Bayesian Mallows model — set_priors","text":"object class \"BayesMallowsPriors\", provided priors argument compute_mallows(), compute_mallows_mixtures(), update_mallows().","code":""},{"path":"/reference/set_priors.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set prior parameters for Bayesian Mallows model — set_priors","text":"Crispino M, Arjas E, Vitelli V, Barrett N, Frigessi (2019). “Bayesian Mallows approach nontransitive pair comparison data: human sounds?” Annals Applied Statistics, 13(1), 492--519. doi:10.1214/18-aoas1203 . Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/set_smc_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Set SMC compute options — set_smc_options","title":"Set SMC compute options — set_smc_options","text":"Sets SMC compute options used update_mallows.BayesMallows().","code":""},{"path":"/reference/set_smc_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set SMC compute options — set_smc_options","text":"","code":"set_smc_options(   n_particles = 1000,   mcmc_steps = 5,   resampler = c(\"stratified\", \"systematic\", \"residual\", \"multinomial\"),   latent_sampling_lag = NA_integer_ )"},{"path":"/reference/set_smc_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set SMC compute options — set_smc_options","text":"n_particles Integer specifying number particles. mcmc_steps Number MCMC steps applied resample-move step. resampler Character string defining resampling method use. One \"stratified\", \"systematic\", \"residual\", \"multinomial\". Defaults \"stratified\". multinomial resampling used Stein (2023) , stratified, systematic, residual resampling typically give lower Monte Carlo error (Douc Cappe 2005; Hol et al. 2006; Naesseth et al. 2019) . latent_sampling_lag Parameter specifying number timesteps go back resampling latent ranks move step. See Section 6.2.3 (Kantas et al. 2015)  details. \\(L\\) notation corresponds latent_sampling_lag. See Details. Defaults NA, means latent ranks previous timesteps moved. set 0, move step applied latent ranks.","code":""},{"path":"/reference/set_smc_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set SMC compute options — set_smc_options","text":"object class \"SMCOptions\".","code":""},{"path":"/reference/set_smc_options.html","id":"lag-parameter-in-move-step","dir":"Reference","previous_headings":"","what":"Lag parameter in move step","title":"Set SMC compute options — set_smc_options","text":"parameter latent_sampling_lag corresponds \\(L\\) (Kantas et al. 2015) . use package can explained terms Algorithm 12 (Stein 2023) . relevant line algorithm : \\(j = 1 : M_{t}\\) M-H step: update \\(\\tilde{\\mathbf{R}}_{j}^{()}\\) proposal \\(\\tilde{\\mathbf{R}}_{j}' \\sim q(\\tilde{\\mathbf{R}}_{j}^{()} |   \\mathbf{R}_{j}, \\boldsymbol{\\rho}_{t}^{()}, \\alpha_{t}^{()})\\).end Let \\(L\\) denote value latent_sampling_lag. parameter, modify algorithm becomes \\(j = M_{t-L+1} : M_{t}\\) M-H step: update \\(\\tilde{\\mathbf{R}}_{j}^{()}\\) proposal \\(\\tilde{\\mathbf{R}}_{j}' \\sim q(\\tilde{\\mathbf{R}}_{j}^{()} |   \\mathbf{R}_{j}, \\boldsymbol{\\rho}_{t}^{()}, \\alpha_{t}^{()})\\).end means \\(L=0\\) move step performed latent ranks, whereas \\(L=1\\) means move step applied parameters entering given timestep. default, latent_sampling_lag = NA means \\(L=t\\) timestep, hence latent ranks part move step timestep.","code":""},{"path":"/reference/set_smc_options.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set SMC compute options — set_smc_options","text":"Douc R, Cappe O (2005). “Comparison resampling schemes particle filtering.” ISPA 2005. Proceedings 4th International Symposium Image Signal Processing Analysis,  2005.. doi:10.1109/ispa.2005.195385 , http://dx.doi.org/10.1109/ISPA.2005.195385. Hol JD, Schon TB, Gustafsson F (2006). “Resampling Algorithms Particle Filters.” 2006 IEEE Nonlinear Statistical Signal Processing Workshop. doi:10.1109/nsspw.2006.4378824 , http://dx.doi.org/10.1109/NSSPW.2006.4378824. Kantas N, Doucet , Singh SS, Maciejowski J, Chopin N (2015). “Particle Methods Parameter Estimation State-Space Models.” Statistical Science, 30(3). ISSN 0883-4237, doi:10.1214/14-sts511 , http://dx.doi.org/10.1214/14-STS511. Naesseth CA, Lindsten F, Schön TB (2019). “Elements Sequential Monte Carlo.” Foundations Trends® Machine Learning, 12(3), 187–306. ISSN 1935-8245, doi:10.1561/2200000074 , http://dx.doi.org/10.1561/2200000074. Stein (2023). Sequential Inference Mallows Model. Ph.D. thesis, Lancaster University.","code":""},{"path":[]},{"path":"/reference/setup_rank_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup rank data — setup_rank_data","title":"Setup rank data — setup_rank_data","text":"Prepare rank preference data analyses.","code":""},{"path":"/reference/setup_rank_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup rank data — setup_rank_data","text":"","code":"setup_rank_data(   rankings = NULL,   preferences = NULL,   user_ids = numeric(),   observation_frequency = NULL,   validate_rankings = TRUE,   na_action = c(\"augment\", \"fail\", \"omit\"),   cl = NULL,   shuffle_unranked = FALSE,   random = FALSE,   random_limit = 8L,   timepoint = NULL,   n_items = NULL )"},{"path":"/reference/setup_rank_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup rank data — setup_rank_data","text":"rankings matrix ranked items, size n_assessors x n_items. See create_ranking() ordered set items need converted rankings. preferences provided, rankings optional initial value rankings. rankings column names, assumed names items. NA values rankings treated missing data automatically augmented; change behavior, see na_action argument set_model_options(). vector length n_items silently converted matrix length 1 x n_items, names (), used column names. preferences data frame one row per pairwise comparison, columns assessor, top_item, bottom_item. column contains following: assessor numeric vector containing assessor index. bottom_item numeric vector containing index item disfavored pairwise comparison. top_item numeric vector containing index item preferred pairwise comparison. two assessors five items, assessor 1 prefers item 1 item 2 item 1 item 5, assessor 2 prefers item 3 item 5, following df: user_ids Optional numeric vector user IDs. used update_mallows(). provided, new data can consist updated partial rankings users already dataset, described Section 6 Stein (2023) . observation_frequency vector observation frequencies (weights) apply row rankings. can speed computation large number assessors share rank pattern. Defaults NULL, means row rankings multiplied 1. provided, observation_frequency must number elements rows rankings, rankings NULL. See compute_observation_frequency() convenience function computing . validate_rankings Logical specifying whether rankings provided (generated preferences) validated. Defaults TRUE. Turning check reduce computing time large number items assessors. na_action Character specifying deal NA values rankings matrix, provided. Defaults \"augment\", means missing values automatically filled using Bayesian data augmentation scheme described Vitelli et al. (2018) . options argument \"fail\", means error message printed algorithm stops NAs rankings, \"omit\" simply deletes rows NAs . cl Optional computing cluster used parallelization generating transitive closure based preferences, returned parallel::makeCluster(). Defaults NULL. shuffle_unranked Logical specifying whether randomly permute unranked items initial ranking. shuffle_unranked=TRUE random=FALSE, unranked items assessor randomly permuted. Otherwise, first ordering returned igraph::topo_sort() returned. random Logical specifying whether use random initial ranking. Defaults FALSE. Setting TRUE means possible orderings consistent stated pairwise preferences generated assessor, one picked random. random_limit Integer specifying maximum number items allowed possible orderings computed, .e., random=TRUE. Defaults 8L. timepoint Integer vector specifying timepoint. Defaults NULL, means vector ones, one observation, generated. Used update_mallows() identify data given iteration sequential Monte Carlo algorithm. NULL, must contain one integer row rankings. n_items Integer specifying number items. Defaults NULL, means number items inferred rankings preferences. Setting n_items manually can useful pairwise preference data SMC algorithm, .e., rankings NULL preferences non-NULL, contains small number pairwise preferences subset users items.","code":""},{"path":"/reference/setup_rank_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup rank data — setup_rank_data","text":"object class \"BayesMallowsData\", provided data argument compute_mallows().","code":""},{"path":"/reference/setup_rank_data.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Setup rank data — setup_rank_data","text":"Setting random=TRUE means possible orderings assessor's preferences generated, one picked random. can useful experiencing convergence issues, e.g., MCMC algorithm mix properly. However, finding possible orderings combinatorial problem, may computationally hard. result may even possible fit memory, may cause R session crash. using option, please try increase size problem incrementally, starting smaller subsets complete data. example given . assumed items labeled starting 1. example, single comparison following form provided, assumed total 30 items (n_items=30), initial ranking permutation 30 items consistent preference 29<30. reality two items, relabeled 1 2, follows:","code":""},{"path":"/reference/setup_rank_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Setup rank data — setup_rank_data","text":"Stein (2023). Sequential Inference Mallows Model. Ph.D. thesis, Lancaster University. Vitelli V, Sørensen, Crispino M, Arjas E, Frigessi (2018). “Probabilistic Preference Learning Mallows Rank Model.” Journal Machine Learning Research, 18(1), 1--49. https://jmlr.org/papers/v18/15-481.html.","code":""},{"path":[]},{"path":"/reference/sushi_rankings.html","id":null,"dir":"Reference","previous_headings":"","what":"Sushi rankings — sushi_rankings","title":"Sushi rankings — sushi_rankings","text":"Complete rankings 10 types sushi 5000 assessors (Kamishima 2003) .","code":""},{"path":"/reference/sushi_rankings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sushi rankings — sushi_rankings","text":"","code":"sushi_rankings"},{"path":"/reference/sushi_rankings.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sushi rankings — sushi_rankings","text":"object class matrix (inherits array) 5000 rows 10 columns.","code":""},{"path":"/reference/sushi_rankings.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sushi rankings — sushi_rankings","text":"Kamishima T (2003). “Nantonac Collaborative Filtering: Recommendation Based Order Responses.” Proceedings Ninth ACM SIGKDD International Conference Knowledge Discovery Data Mining, 583--588.","code":""},{"path":[]},{"path":"/reference/update_mallows.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a Bayesian Mallows model with new users — update_mallows","title":"Update a Bayesian Mallows model with new users — update_mallows","text":"Update Bayesian Mallows model estimated using Metropolis-Hastings algorithm compute_mallows() using sequential Monte Carlo algorithm described Stein (2023) .","code":""},{"path":"/reference/update_mallows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a Bayesian Mallows model with new users — update_mallows","text":"","code":"update_mallows(model, new_data, ...)  # S3 method for BayesMallowsPriorSamples update_mallows(   model,   new_data,   model_options = set_model_options(),   smc_options = set_smc_options(),   compute_options = set_compute_options(),   priors = model$priors,   pfun_estimate = NULL,   ... )  # S3 method for BayesMallows update_mallows(   model,   new_data,   model_options = set_model_options(),   smc_options = set_smc_options(),   compute_options = set_compute_options(),   priors = model$priors,   ... )  # S3 method for SMCMallows update_mallows(model, new_data, ...)"},{"path":"/reference/update_mallows.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a Bayesian Mallows model with new users — update_mallows","text":"model model object class \"BayesMallows\" returned compute_mallows(), object class \"SMCMallows\" returned function, object class \"BayesMallowsPriorSamples\" returned sample_prior(). new_data object class \"BayesMallowsData\" returned setup_rank_data(). object contain new data provided. ... Optional arguments. Currently used. model_options object class \"BayesMallowsModelOptions\" returned set_model_options(). smc_options object class \"SMCOptions\" returned set_smc_options(). compute_options object class \"BayesMallowsComputeOptions\" returned set_compute_options(). priors object class \"BayesMallowsPriors\" returned set_priors(). Defaults priors used model. pfun_estimate Object returned estimate_partition_function(). Defaults NULL, used footrule, Spearman, Ulam distances cardinalities available, cf. get_cardinalities(). used specialization objects type \"BayesMallowsPriorSamples\".","code":""},{"path":"/reference/update_mallows.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update a Bayesian Mallows model with new users — update_mallows","text":"updated model, class \"SMCMallows\".","code":""},{"path":[]},{"path":"/reference/update_mallows.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update a Bayesian Mallows model with new users — update_mallows","text":"","code":"set.seed(1) # UPDATING A MALLOWS MODEL WITH NEW COMPLETE RANKINGS # Assume we first only observe the first four rankings in the potato_visual # dataset data_first_batch <- potato_visual[1:4, ]  # We start by fitting a model using Metropolis-Hastings mod_init <- compute_mallows(   data = setup_rank_data(data_first_batch),   compute_options = set_compute_options(nmc = 10000))  # Convergence seems good after no more than 2000 iterations assess_convergence(mod_init)  burnin(mod_init) <- 2000  # Next, assume we receive four more observations data_second_batch <- potato_visual[5:8, ]  # We can now update the model using sequential Monte Carlo mod_second <- update_mallows(   model = mod_init,   new_data = setup_rank_data(rankings = data_second_batch),   smc_options = set_smc_options(resampler = \"systematic\")   )  # This model now has a collection of particles approximating the posterior # distribution after the first and second batch # We can use all the posterior summary functions as we do for the model # based on compute_mallows(): plot(mod_second)  plot(mod_second, parameter = \"rho\", items = 1:4)  compute_posterior_intervals(mod_second) #>   parameter   mean median           hpdi central_interval #> 1     alpha 11.420 11.358 [9.118,13.980]   [9.154,14.096]  # Next, assume we receive the third and final batch of data. We can update # the model again data_third_batch <- potato_visual[9:12, ] mod_final <- update_mallows(   model = mod_second, new_data = setup_rank_data(rankings = data_third_batch))  # We can plot the same things as before plot(mod_final)  compute_consensus(mod_final) #>      cluster ranking item cumprob #> 1  Cluster 1       1  P12   0.999 #> 2  Cluster 1       2  P13   0.999 #> 3  Cluster 1       3   P9   0.983 #> 4  Cluster 1       4  P10   0.587 #> 5  Cluster 1       5  P17   0.915 #> 6  Cluster 1       6   P7   0.741 #> 7  Cluster 1       7  P14   0.999 #> 8  Cluster 1       8  P16   0.957 #> 9  Cluster 1       9   P5   0.467 #> 10 Cluster 1      10   P1   0.876 #> 11 Cluster 1      11  P11   0.969 #> 12 Cluster 1      12  P19   0.998 #> 13 Cluster 1      13  P18   0.554 #> 14 Cluster 1      14  P20   0.999 #> 15 Cluster 1      15   P6   0.955 #> 16 Cluster 1      16   P4   0.549 #> 17 Cluster 1      17   P2   0.778 #> 18 Cluster 1      18  P15   1.000 #> 19 Cluster 1      19   P3   1.000 #> 20 Cluster 1      20   P8   1.000  # UPDATING A MALLOWS MODEL WITH NEW OR UPDATED PARTIAL RANKINGS # The sequential Monte Carlo algorithm works for data with missing ranks as # well. This both includes the case where new users arrive with partial ranks, # and when previously seen users arrive with more complete data than they had # previously. # We illustrate for top-k rankings of the first 10 users in potato_visual potato_top_10 <- ifelse(potato_visual[1:10, ] > 10, NA_real_,                         potato_visual[1:10, ]) potato_top_12 <- ifelse(potato_visual[1:10, ] > 12, NA_real_,                         potato_visual[1:10, ]) potato_top_14 <- ifelse(potato_visual[1:10, ] > 14, NA_real_,                         potato_visual[1:10, ])  # We need the rownames as user IDs (user_ids <- 1:10) #>  [1]  1  2  3  4  5  6  7  8  9 10  # First, users provide top-10 rankings mod_init <- compute_mallows(   data = setup_rank_data(rankings = potato_top_10, user_ids = user_ids),   compute_options = set_compute_options(nmc = 10000))  # Convergence seems fine. We set the burnin to 2000. assess_convergence(mod_init)  burnin(mod_init) <- 2000  # Next assume the users update their rankings, so we have top-12 instead. mod1 <- update_mallows(   model = mod_init,   new_data = setup_rank_data(rankings = potato_top_12, user_ids = user_ids),   smc_options = set_smc_options(resampler = \"stratified\") )  plot(mod1)   # Then, assume we get even more data, this time top-14 rankings: mod2 <- update_mallows(   model = mod1,   new_data = setup_rank_data(rankings = potato_top_14, user_ids = user_ids) )  plot(mod2)   # Finally, assume a set of new users arrive, who have complete rankings. potato_new <- potato_visual[11:12, ] # We need to update the user IDs, to show that these users are different (user_ids <- 11:12) #> [1] 11 12  mod_final <- update_mallows(   model = mod2,   new_data = setup_rank_data(rankings = potato_new, user_ids = user_ids) )  plot(mod_final)   # We can also update models with pairwise preferences # We here start by running MCMC on the first 20 assessors of the beach data # A realistic application should run a larger number of iterations than we # do in this example. set.seed(3) dat <- subset(beach_preferences, assessor <= 20) mod <- compute_mallows(   data = setup_rank_data(     preferences = beach_preferences),   compute_options = set_compute_options(nmc = 3000, burnin = 1000) )  # Next we provide assessors 21 to 24 one at a time. for(i in 21:24){   mod <- update_mallows(     model = mod,     new_data = setup_rank_data(       preferences = subset(beach_preferences, assessor == i),       user_ids = i, shuffle_unranked = TRUE),     smc_options = set_smc_options(latent_sampling_lag = 0)   ) }  # Compared to running full MCMC, there is a downward bias in the scale # parameter. This can be alleviated by increasing the number of particles, # MCMC steps, and the latent sampling lag. plot(mod)  compute_consensus(mod) #>      cluster ranking    item cumprob #> 1  Cluster 1       1  Item 6   0.430 #> 2  Cluster 1       2  Item 9   0.752 #> 3  Cluster 1       3  Item 3   0.488 #> 4  Cluster 1       4 Item 11   0.808 #> 5  Cluster 1       5 Item 15   0.526 #> 6  Cluster 1       6 Item 10   0.560 #> 7  Cluster 1       7  Item 1   0.708 #> 8  Cluster 1       8  Item 5   0.705 #> 9  Cluster 1       9 Item 13   0.696 #> 10 Cluster 1      10  Item 7   0.553 #> 11 Cluster 1      11  Item 8   0.426 #> 12 Cluster 1      12 Item 12   0.536 #> 13 Cluster 1      13  Item 4   0.738 #> 14 Cluster 1      14 Item 14   0.885 #> 15 Cluster 1      15  Item 2   1.000"},{"path":"/news/index.html","id":"bayesmallows-210","dir":"Changelog","previous_headings":"","what":"BayesMallows 2.1.0","title":"BayesMallows 2.1.0","text":"CRAN release: 2024-03-13 SMC method update_mallows() now supports pairwise preferences, new users providing pairwise preferences existing users updating preferences. Acceptance ratios now tracked Metropolis-Hastings algorithm used compute_mallows() move step inside sequential Monte Carlo algorithm used update_mallows() compute_mallows_sequentially(). Use function get_acceptance_ratios() access . BREAKING CHANGE: Burnin now explicitly set using ‘burnin(model) <- value’ already set compute_options. alleviates need ‘burnin’ argument functions assessing posterior distribution abstracts away implementation user. See ‘?burnin’ ‘?burnin<-’ details. now option proposing modal ranking rho. can defined setting rho_proposal=“swap” set_compute_options(). leap-- shift distribution still default. Fixed bug heat_plot() model estimated rho_thinning > 1, causing probabilities unnormalized. Issue #381. Thanks Marta Crispino discovering bug. Added stratified, systematic, residual resampling sequential Monte Carlo algorithm. distributions general preferred multinomial resampling, available option now. move step SMC algorithm now allows user-defined lag sampling latent ranks, specified “latent_sampling_lag” argument set_smc_options(). Prior precision parameter alpha now gamma distribution. now exponential distribution assumed. Since exponential special case gamma shape parameter equal 1 (default), breaking change. However, adds flexibility comes specifying prior. setup_rank_data() now accepts single vector rankings, silently converting vector matrix single row. Sequential Monte Carlo algorithm can now start sample prior distribution, see sample_prior() function example. Added support parallelism --hood oneTBB.","code":""},{"path":"/news/index.html","id":"bayesmallows-201","dir":"Changelog","previous_headings":"","what":"BayesMallows 2.0.1","title":"BayesMallows 2.0.1","text":"CRAN release: 2024-01-25 Edits C++ code fixing memory leaks. Edits unit tests caused issues CRAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-200","dir":"Changelog","previous_headings":"","what":"BayesMallows 2.0.0","title":"BayesMallows 2.0.0","text":"CRAN release: 2024-01-15 Large refactoring several breaking changes. See vignettes documentation details.","code":""},{"path":"/news/index.html","id":"bayesmallows-150","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.5.0","title":"BayesMallows 1.5.0","text":"CRAN release: 2023-11-25 Bug plot.BayesMallows posterior distribution ‘parameter = “rho”’ fixed. Thanks Lorenzo Zuccato points issue. (https://github.com/ocbe-uio/BayesMallows/issues/342) Argument obs_freq internal function rmallows() removed, used. Thanks Lorenzo Zuccato pointing (https://github.com/ocbe-uio/BayesMallows/issues/337). Argument save_clus compute_mallows() removed, used. compute_mallows() now supports parallel chains, providing ‘cl’ argument. See vignette “MCMC Parallel Chains” tutorial. Documentation functions now grouped families. lik_db_mix() now deprecated favor get_mallows_loglik() Unusued argument removed internal function augment_pairwise(). Thanks Lorenzo Zuccato making us aware (https://github.com/ocbe-uio/BayesMallows/issues/313).","code":""},{"path":"/news/index.html","id":"bayesmallows-140","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.4.0","title":"BayesMallows 1.4.0","text":"CRAN release: 2023-10-04 Bug fix: psi argument compute_mallows() compute_mallows_mixtures(), specifying concentration parameter Dirichlet prior, now forwarded underlying run_mcmc() function. Previously, argument effect, default psi=10 used regardless input. Thanks Lorenzo Zuccato discovering bug. SMC functions now accept exact partition functions available. Removed SMC functions deprecated version 1.2.0 (#301) Website deployed https://ocbe-uio.github.io/BayesMallows. Reordering authors, Waldir Leoncio appears second list.","code":""},{"path":"/news/index.html","id":"bayesmallows-132","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.2","title":"BayesMallows 1.3.2","text":"CRAN release: 2023-08-24 Fixed LTO compilation notes CRAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-131","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.1","title":"BayesMallows 1.3.1","text":"CRAN release: 2023-08-21 Fixed package documentation issue CRAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-130","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.3.0","title":"BayesMallows 1.3.0","text":"CRAN release: 2023-03-10 Added heat_plot() function (#255) Replaced deprecated ggplot2::aes_ function ggplot2::aes. Refactoring SMC functions (#257) Improved validation documentation SMC post-processing functions (#262)","code":""},{"path":"/news/index.html","id":"bayesmallows-122","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.2","title":"BayesMallows 1.2.2","text":"CRAN release: 2023-02-03 Added plot.SMCMallows() method Changed default values argument order several SMC functions (see PR #269) Modifications internal C++ code avoid CRAN NOTEs.","code":""},{"path":"/news/index.html","id":"bayesmallows-121","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.1","title":"BayesMallows 1.2.1","text":"CRAN release: 2022-11-04 PerMallows package removed Imports risk removed CRAN. means Ulam distance 95 items, user compute importance sampling estimate. Refactoring data augmentation function SMC Mallows. Improved documentation sample_dataset","code":""},{"path":"/news/index.html","id":"bayesmallows-120","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.2.0","title":"BayesMallows 1.2.0","text":"CRAN release: 2022-05-24 Fixed bug caused assess_convergence() fail ‘parameter = “cluster_probs”’. Fixed bug smc_mallows_new_users_partial() smc_mallows_new_users_partial_alpha_fixed(). metropolis_hastings_aug_ranking_pseudo() deprecated. Please use metropolis_hastings_aug_ranking() instead, pseudo=TRUE. smc_mallows_new_users_partial_alpha_fixed(), smc_mallows_new_users_complete(), smc_mallows_new_users_partial() deprecated. Please use smc_mallows_new_users() instead, set type= argument “complete”, “partial”, “partial_alpha_fixed”. smc_mallows_new_item_rank_alpha_fixed() deprecated. Please use smc_mallows_new_item_rank() instead, argument alpha_fixed=TRUE. Fixed unexpected behavior leap--shift proposal distribution SMC Mallows, causing function propose current rank vector nonzero probability. BayesMallows longer depends ‘dplyr’. Quite extensive internal refactoring C++ code. Function lik_db_mix renamed get_mallows_loglik. lik_db_mix still exists deprecated. initial rankings provided, compute_mallows() compute_mallows_mixtures() use independent initial rho cluster. Previously single initial rho used cluster. potentially improve convergence, lead different results n_clusters>=2 given random number seed.","code":""},{"path":"/news/index.html","id":"bayesmallows-112","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.2","title":"BayesMallows 1.1.2","text":"CRAN release: 2022-04-11 Fixed issue stats::reshape causing error R-oldrel. Fixed issue checking class objects, now consistently use inherits(). Internal C++ fixes comply CRAN checks.","code":""},{"path":"/news/index.html","id":"bayesmallows-111","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.1","title":"BayesMallows 1.1.1","text":"CRAN release: 2022-04-01 Fixed C++ errors leading CRAN issues.","code":""},{"path":"/news/index.html","id":"bayesmallows-110","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.1.0","title":"BayesMallows 1.1.0","text":"CRAN release: 2021-12-03 Major update, introducing whole new class methods using sequential Monte Carlo. Also reducing number dependencies.","code":""},{"path":"/news/index.html","id":"bayesmallows-1049001","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4.9001","title":"BayesMallows 1.0.4.9001","text":"major update, new functions estimating Bayesian Mallows model using sequential Monte Carlo. methods described vignette titled “SMC-Mallows Tutorial”.","code":""},{"path":"/news/index.html","id":"bayesmallows-1049000","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4.9000","title":"BayesMallows 1.0.4.9000","text":"Removed large number dependencies converting base R code. make package easier install across range systems, less vulnerable changes packages.","code":""},{"path":"/news/index.html","id":"bayesmallows-104","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.4","title":"BayesMallows 1.0.4","text":"CRAN release: 2021-11-17 Incorporates changes since 1.0.3, also remove PLMIX Imports.","code":""},{"path":"/news/index.html","id":"bayesmallows-1039001","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3.9001","title":"BayesMallows 1.0.3.9001","text":"Fixed bug caused plot_top_k fail plotting clusters. Improved default value rel_widths argument plot_top_k. Wrote unit tests check bugs don’t appear .","code":""},{"path":"/news/index.html","id":"bayesmallows-1039000","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3.9000","title":"BayesMallows 1.0.3.9000","text":"Fixed bug caused importance sampling fail running parallel. Fixed issue error message trying plot error probability compute_mallows set compute error probability. Increased number unit tests.","code":""},{"path":"/news/index.html","id":"bayesmallows-103","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.3","title":"BayesMallows 1.0.3","text":"CRAN release: 2021-10-14 Fixed critical bug caused results wrong one mixture component compute_mallows() compute_mallows_mixtures(). Thanks Anja Stein discovering bug.","code":""},{"path":"/news/index.html","id":"bayesmallows-102","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.2","title":"BayesMallows 1.0.2","text":"CRAN release: 2021-06-04 Function generate_initial_ranking() now two additional options generating random initial rankings. can help convergence problems, allowing MCMC algorithm run range different starting points.","code":""},{"path":"/news/index.html","id":"bayesmallows-101","dir":"Changelog","previous_headings":"","what":"BayesMallows 1.0.1","title":"BayesMallows 1.0.1","text":"CRAN release: 2021-02-23 Fixes bug lik_db_mix expected_dist, scaling parameter used different parametrization rest package. functions package now use consistent parametrization Mallows model, stated vignette.","code":""},{"path":"/news/index.html","id":"bayesmallows-050","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.5.0","title":"BayesMallows 0.5.0","text":"CRAN release: 2020-08-28 Function compute_consensus now includes option computing consensus augmented ranks.","code":""},{"path":"/news/index.html","id":"bayesmallows-044","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.4","title":"BayesMallows 0.4.4","text":"CRAN release: 2020-08-07 Fixed bug predict_top_k plot_top_k using aug_thinning > 1.","code":""},{"path":"/news/index.html","id":"bayesmallows-043","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.3","title":"BayesMallows 0.4.3","text":"CRAN release: 2020-06-20 Updated README vignette.","code":""},{"path":"/news/index.html","id":"bayesmallows-042","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.2","title":"BayesMallows 0.4.2","text":"CRAN release: 2020-03-23 Updating unit test make sure BayesMallows compatible dplyr version 1.0.0.","code":""},{"path":"/news/index.html","id":"bayesmallows-041","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.1","title":"BayesMallows 0.4.1","text":"CRAN release: 2019-09-05 Improvement plotting functions, noted .","code":""},{"path":"/news/index.html","id":"bayesmallows-0409002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9002","title":"BayesMallows 0.4.0.9002","text":"plot.BayesMallows plot_elbow longer print titles automatically.","code":""},{"path":"/news/index.html","id":"bayesmallows-0409001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9001","title":"BayesMallows 0.4.0.9001","text":"assess_convergence longer prints legends clusters, cluster number essentially arbitrary.","code":""},{"path":"/news/index.html","id":"bayesmallows-0409000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0.9000","title":"BayesMallows 0.4.0.9000","text":"Added CITATION. Updated test random number seed.","code":""},{"path":"/news/index.html","id":"bayesmallows-040","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.4.0","title":"BayesMallows 0.4.0","text":"CRAN release: 2019-02-22 Implements fixes since version 0.3.1 . Fixed typo y-axis label elbow plot. Fixed issue caused cluster probabilities differ across platforms, despite using seed. https://stackoverflow.com/questions/54822702","code":""},{"path":"/news/index.html","id":"bayesmallows-0319005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9005","title":"BayesMallows 0.3.1.9005","text":"Fixed bug caused compute_mallows work (without giving errors) rankings contained missing values. Fixed bug caused compute_mallows fail preferences integer columns.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9004","title":"BayesMallows 0.3.1.9004","text":"Changed name save_individual_cluster_probs save_ind_clus, save typing.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9003","title":"BayesMallows 0.3.1.9003","text":"Added user prompt asking user really wants save csv files, save_individual_cluster_probs = TRUE compute_mallows. Added alpha_max, truncation exponential prior alpha, user option compute_mallows.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9002","title":"BayesMallows 0.3.1.9002","text":"Added functionality checking label switching. See ?label_switching info.","code":""},{"path":"/news/index.html","id":"bayesmallows-0319001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1.9001","title":"BayesMallows 0.3.1.9001","text":"internal function compute_importance_sampling_estimate updated avoid numerical overflow. Previously, importance sampling failed 200 items. Now works way 10,000 items.","code":""},{"path":"/news/index.html","id":"bayesmallows-031","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.1","title":"BayesMallows 0.3.1","text":"CRAN release: 2019-02-01 update parts C++ code, avoid failing sanitizer checks clang-UBSAN gcc-UBSAN.","code":""},{"path":"/news/index.html","id":"bayesmallows-030","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.3.0","title":"BayesMallows 0.3.0","text":"CRAN release: 2019-01-30 See bullet points , since 0.2.0.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209006","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9006","title":"BayesMallows 0.2.0.9006","text":"generate_transitive_closure, generate_initial_ranking, generate_constraints now able run parallel. Large changes underlying code base make maintainable affect user.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9005","title":"BayesMallows 0.2.0.9005","text":"estimate_partition_function now option run parallel, leading significant speed-.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9004","title":"BayesMallows 0.2.0.9004","text":"Implemented Bernoulli error model. Set error_model = \"bernoulli\" compute_mallows order use . Examples come later.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9003","title":"BayesMallows 0.2.0.9003","text":"Added parallelization option compute_mallows_mixtures added parallel Suggests field.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9002","title":"BayesMallows 0.2.0.9002","text":"Deprecated functions compute_cp_consensus compute_map_consensus removed. Use compute_consensus instead.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9001","title":"BayesMallows 0.2.0.9001","text":"Clusters now factor variables sorted according cluster number. Hence, plot legends, “Cluster 10” comes “Cluster 9”, rather “Cluster 1” used now, character. plot.BayesMallows longer contains print statements forces display plots. Instead plots returned function. Using p <- plot(fit) hence longer display plot, whereas using plot(fit) without assigning object, displays plot. now plot always shown rho alpha.","code":""},{"path":"/news/index.html","id":"bayesmallows-0209000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0.9000","title":"BayesMallows 0.2.0.9000","text":"compute_mallows sample_mallows now support Ulam distance, argument metric = \"ulam\". Slimmed vignette significantly, order avoid clang-UBSAN error caused running vignette (caused Rcpp, cf. issue). long vignette longer needed case, since functions well documented executable examples.","code":""},{"path":"/news/index.html","id":"bayesmallows-020","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.2.0","title":"BayesMallows 0.2.0","text":"CRAN release: 2018-11-30 New release CRAN, contains updates 0.1.1, described .","code":""},{"path":"/news/index.html","id":"bayesmallows-0119009","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9009","title":"BayesMallows 0.1.1.9009","text":"Rankcluster package removed dependencies.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119008","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9008","title":"BayesMallows 0.1.1.9008","text":"Fixed bug Cayley distance. distance, computational shortcut p. 8 Vitelli et al. (2018), JMLR, work. However, still used. Now, Cayley distance always computed complete rank vectors. Fixed bug default argument leap_size compute_mallows. used floor(n_items / 5), evaluates zero n_items <= 4. Updated max(1L, floor(n_items / 5)). Added Hamming distance (metric = \"hamming\") option compute_mallows sample_mallows.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119007","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9007","title":"BayesMallows 0.1.1.9007","text":"Updated generate_initial_ranking, generate_transitive_closure, sample_mallows avoid errors package tibble version 2.0.0 released. update purely internal.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119006","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9006","title":"BayesMallows 0.1.1.9006","text":"Objects class BayesMallows BayesMallowsMixtures now default print functions, hence avoiding excessive amounts informations printed console user happens write name object press Return. compute_mallows_mixtures longer sets include_wcd = TRUE default. user can choose argument. compute_mallows new argument save_clus, can set FALSE saving cluster assignments.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119005","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9005","title":"BayesMallows 0.1.1.9005","text":"assess_convergence now automatically plots mixtures. compute_mallows_mixtures now returns object class BayesMallowsMixtures.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119004","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9004","title":"BayesMallows 0.1.1.9004","text":"assess_convergence now adds prefix Assessor plots parameter = \"Rtilde\". predict_top_k now exported function. Previously internal.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119003","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9003","title":"BayesMallows 0.1.1.9003","text":"compute_posterior_intervals now default parameter = \"alpha\". now, argument default. Argument type plot.BayesMallows assess_convergence renamed parameter, consistent.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119002","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9002","title":"BayesMallows 0.1.1.9002","text":"Argument save_augment_data compute_mallows renamed save_aug. compute_mallows fills implied ranks assessor one missing rank. avoids unnecessary augmentation MCMC. generate_ranking generate_ordering now work missing ranks.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119001","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9001","title":"BayesMallows 0.1.1.9001","text":"Argument cluster_assignment_thinning compute_mallows renamed clus_thin.","code":""},{"path":"/news/index.html","id":"bayesmallows-0119000","dir":"Changelog","previous_headings":"","what":"BayesMallows 0.1.1.9000","title":"BayesMallows 0.1.1.9000","text":"Change interface computing consensus ranking. Now, CP MAP consensus computed compute_consensus function, argument type equal either \"CP\" \"MAP\".","code":""}]
