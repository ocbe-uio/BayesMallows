---
title: "MCMC with Parallel Chains"
output: 
  rmarkdown::html_vignette:
    fig_width: 6
    fig_height: 4
bibliography: ../inst/REFERENCES.bib
link-citations: yes
vignette: >
  %\VignetteIndexEntry{MCMC with Parallel Chains}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---





```r
library(BayesMallows)
```

This vignette describes how to run Markov chain Monte Carlo with parallel chains. For an introduction to the "BayesMallows" package, please see @sorensen2020.


## Why Parallel Chains?

Modern computers have multiple cores, and on computing clusters one can get access to hundreds of cores easily. By running Markov Chains in parallel on $K$ cores, ideally from different starting points, we achieve at least the following:

1. The time you have to wait to get the required number of post-burnin samples scales like $1/K$.

2. You can check convergence by comparing chains.



## Parallel Chains with Complete Rankings

In "BayesMallows" we use the "parallel" package for parallel computation. Parallelization is obtained by starting a cluster and providing it as an argument. Since the limit to parallelism for vignettes being built on CRAN is 2, we here start a cluster spanning two cores, but in real applications this number should typically be larger (the output from `parallel::detectCores()` can be a good guide). Note that we also give one initial value of the dispersion parameter $\alpha$ to each chain.


```r
library(parallel)
cl <- makeCluster(2)
fit <- compute_mallows(
  data = setup_rank_data(rankings = potato_visual), 
  compute_options = set_compute_options(nmc = 5000),
  cl = cl
)
stopCluster(cl)
```

We can assess convergence in the usual way:


```r
assess_convergence(fit)
```

![plot of chunk unnamed-chunk-3](unnamed-chunk-3-1.png)

We can also assess convergence for the latent ranks $\boldsymbol{\rho}$. Since the initial value of $\boldsymbol{\rho}$ is sampled uniformly, the two chains automatically get different initial values.


```r
assess_convergence(fit, parameter = "rho", items = 1:3)
```

![plot of chunk unnamed-chunk-4](unnamed-chunk-4-1.png)

Based on the convergence plots, we set the burnin to 700. 


```r
fit$burnin <- 700
```

We can now use all the tools for assessing the posterior distributions as usual. The post-burnin samples for all parallel chains are simply combined, as they should be.

Below is a plot of the posterior distribution of $\alpha$.


```r
plot(fit)
```

![plot of chunk unnamed-chunk-6](unnamed-chunk-6-1.png)

Next is a plot of the posterior distribution of $\boldsymbol{\rho}$.



```r
plot(fit, parameter = "rho", items = 4:7)
```

![plot of chunk unnamed-chunk-7](unnamed-chunk-7-1.png)

## Parallel Chains with Pairwise Preferences


A case where parallel chains might be more strongly needed is with incomplete data, e.g., arising from pairwise preferences. In this case the MCMC algorithm needs to perform data augmentation, which tends to be both slow and sticky. We illustrate this with the beach preference data, again referring to @sorensen2020 for a more thorough introduction to the aspects not directly related to parallelism.



```r
beach_data <- setup_rank_data(preferences = beach_preferences)
```

We run two parallel chains, letting the package generate random initial rankings, but again providing a vector of initial values for $\alpha$.


```r
cl <- makeCluster(2)
fit <- compute_mallows(
  data = beach_data,
  compute_options = set_compute_options(nmc = 4000, save_aug = TRUE),
  initial_values = set_initial_values(alpha_init = runif(2, 1, 4)),
  cl = cl
)
stopCluster(cl)
```

### Trace Plots

The convergence plots shows some long-range autocorrelation, but otherwise it seems to mix relatively well.


```r
assess_convergence(fit)
```

![plot of chunk unnamed-chunk-10](unnamed-chunk-10-1.png)

Here is the convergence plot for $\boldsymbol{\rho}$:


```r
assess_convergence(fit, parameter = "rho", items = 4:6)
```

![plot of chunk unnamed-chunk-11](unnamed-chunk-11-1.png)

To avoid overplotting, it's a good idea to pick a low number of assessors and chains. We here look at items 1-3 of assessors 1 and 2.


```r
assess_convergence(fit,
  parameter = "Rtilde",
  items = 1:3, assessors = 1:2
)
```

![plot of chunk unnamed-chunk-12](unnamed-chunk-12-1.png)

### Posterior Quantities

Based on the trace plots, the chains seem to be mixing well. We set the burnin to 700 again.


```r
fit$burnin <- 700
```

We can now study the posterior distributions. Here is the posterior for $\alpha$. Note that by increasing the `nmc` argument to `compute_mallows` above, the density would appear smoother. In this vignette we have kept it low to reduce the run time.


```r
plot(fit)
```

![plot of chunk unnamed-chunk-14](unnamed-chunk-14-1.png)

We can also look at the posterior for $\boldsymbol{\rho}$.


```r
plot(fit, parameter = "rho", items = 6:9)
```

![plot of chunk unnamed-chunk-15](unnamed-chunk-15-1.png)

We can also compute posterior intervals in the usual way:


```r
compute_posterior_intervals(fit, parameter = "alpha")
#>   parameter  mean median conf_level          hpdi central_interval
#> 1     alpha 4.821  4.822       95 % [4.264,5.403]    [4.254,5.396]
```


```r
compute_posterior_intervals(fit, parameter = "rho")
#>       item parameter mean median conf_level    hpdi central_interval
#> 1   Item 1       rho    7      7       95 %     [7]            [6,7]
#> 2   Item 2       rho   15     15       95 %    [15]          [14,15]
#> 3   Item 3       rho    3      3       95 %   [3,4]            [3,4]
#> 4   Item 4       rho   12     11       95 % [11,13]          [11,14]
#> 5   Item 5       rho    9      8       95 %  [8,10]           [8,10]
#> 6   Item 6       rho    2      2       95 %   [1,2]            [1,2]
#> 7   Item 7       rho    9      9       95 %  [8,10]           [8,10]
#> 8   Item 8       rho   12     12       95 % [11,13]          [11,13]
#> 9   Item 9       rho    1      1       95 %   [1,2]            [1,2]
#> 10 Item 10       rho    6      6       95 %   [5,6]            [5,7]
#> 11 Item 11       rho    4      4       95 %   [3,4]            [3,5]
#> 12 Item 12       rho   13     13       95 % [12,14]          [12,14]
#> 13 Item 13       rho   10     10       95 %  [9,10]           [9,10]
#> 14 Item 14       rho   13     14       95 % [11,14]          [11,15]
#> 15 Item 15       rho    5      5       95 %   [4,6]            [4,6]
```

And we can compute the consensus ranking:


```r
compute_consensus(fit)
#>    ranking    item   cumprob
#> 1        1  Item 9 0.8837879
#> 2        2  Item 6 1.0000000
#> 3        3  Item 3 0.7478788
#> 4        4 Item 11 0.9595455
#> 5        5 Item 15 0.9487879
#> 6        6 Item 10 0.9746970
#> 7        7  Item 1 1.0000000
#> 8        8  Item 5 0.5031818
#> 9        9  Item 7 0.9213636
#> 10      10 Item 13 1.0000000
#> 11      11  Item 4 0.5492424
#> 12      12  Item 8 0.8048485
#> 13      13 Item 12 0.5416667
#> 14      14 Item 14 0.9748485
#> 15      15  Item 2 1.0000000
```


```r
compute_consensus(fit, type = "MAP")
#>    map_ranking    item probability
#> 1            1  Item 9        0.14
#> 2            2  Item 6        0.14
#> 3            3  Item 3        0.14
#> 4            4 Item 11        0.14
#> 5            5 Item 15        0.14
#> 6            6 Item 10        0.14
#> 7            7  Item 1        0.14
#> 8            8  Item 5        0.14
#> 9            9  Item 7        0.14
#> 10          10 Item 13        0.14
#> 11          11  Item 4        0.14
#> 12          12  Item 8        0.14
#> 13          13 Item 12        0.14
#> 14          14 Item 14        0.14
#> 15          15  Item 2        0.14
```

We can compute the probability of being top-$k$, here for $k=4$:


```r
plot_top_k(fit, k = 4)
```

![plot of chunk unnamed-chunk-20](unnamed-chunk-20-1.png)



# References
