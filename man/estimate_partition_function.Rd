% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate_partition_function.R
\name{estimate_partition_function}
\alias{estimate_partition_function}
\title{Estimate Partition Function}
\usage{
estimate_partition_function(
  method = "importance_sampling",
  alpha_vector,
  n_items,
  metric,
  nmc,
  degree,
  n_iterations,
  K,
  cl = NULL,
  seed = NULL
)
}
\arguments{
\item{method}{Character string specifying the method to use in order to
estimate the logarithm of the partition function. Available options are
\code{"importance_sampling"} and \code{"asymptotic"}.}

\item{alpha_vector}{Numeric vector of \eqn{\alpha} values over which to
compute the importance sampling estimate.}

\item{n_items}{Integer specifying the number of items.}

\item{metric}{Character string specifying the distance measure to use.
Available options are \code{"footrule"} and \code{"spearman"} when
\code{method = "asymptotic"} and in addition \code{"cayley"}, \code{"hamming"},
\code{"kendall"}, and \code{"ulam"} when \code{method = "importance_sampling"}.}

\item{nmc}{Integer specifying the number of Monte Carlo samples to use in the
importance sampling. Only used when \code{method = "importance_sampling"}.}

\item{degree}{Integer specifying the degree of the polynomial used to
estimate \eqn{\log(\alpha)} from the grid of values provided by the
importance sampling estimate.}

\item{n_iterations}{Integer specifying the number of iterations to use in the
asymptotic approximation of the partition function. Only used when
\code{method = "asymptotic"}.}

\item{K}{Integer specifying the parameter \eqn{K} in the
asymptotic approximation of the partition function. Only used when
\code{method = "asymptotic"}.}

\item{cl}{Optional computing cluster used for parallelization, returned
from \code{\link[parallel:makeCluster]{parallel::makeCluster()}}. Defaults to \code{NULL}. Only used when
\code{method = "importance_sampling"}.}

\item{seed}{Optional random number seed.}
}
\value{
A vector of length \code{degree} which can be supplied to the
\code{logz_estimate} argument of \code{\link[=compute_mallows]{compute_mallows()}}.
}
\description{
Estimate the logarithm of the partition function of the Mallows rank model. Choose
between the importance sampling algorithm described in
\insertCite{vitelli2018}{BayesMallows} and the IPFP algorithm for computing an
asymptotic approximation described in \insertCite{mukherjee2016}{BayesMallows}.
}
\examples{
# IMPORTANCE SAMPLING
# Let us estimate logZ(alpha) for 20 items with Spearman distance
# We create a grid of alpha values from 0 to 10
alpha_vector <- seq(from = 0, to = 10, by = 0.5)
n_items <- 20
metric <- "spearman"
degree <- 10

# We start with 1e3 Monte Carlo samples
fit1 <- estimate_partition_function(method = "importance_sampling",
                                      alpha_vector = alpha_vector,
                                      n_items = n_items, metric = metric,
                                      nmc = 1e3, degree = degree)
# A vector of polynomial regression coefficients is returned
fit1

# Now let us recompute with 1e4 Monte Carlo samples
fit2 <- estimate_partition_function(method = "importance_sampling",
                                    alpha_vector = alpha_vector,
                                    n_items = n_items, metric = metric,
                                    nmc = 1e4, degree = degree)

# ASYMPTOTIC APPROXIMATION
# We can also compute an estimate using the asymptotic approximation
K <- 20
n_iterations <- 50

fit3 <- estimate_partition_function(method = "asymptotic",
                                    alpha_vector = alpha_vector,
                                    n_items = n_items, metric = metric,
                                    n_iterations = n_iterations,
                                    K = K, degree = degree)

# We write a little function for storing the estimates in a dataframe
powers <- seq(from = 0, to = degree, by = 1)

compute_fit <- function(fit, type){
  do.call(rbind, lapply(alpha_vector, function(alpha){
    data.frame(
      type = type,
      alpha = alpha,
      logz_estimate = sum(alpha^powers * fit)
    )
  }))
}



estimates <- rbind(
  compute_fit(fit1, type = "Importance Sampling 1e3"),
  compute_fit(fit2, type = "Importance Sampling 1e4"),
  compute_fit(fit3, type = "Asymptotic")
  )

# We can now plot the two estimates side-by-side
library(ggplot2)
ggplot(estimates, aes(x = alpha, y = logz_estimate, color = type)) +
  geom_line()
# We see that the two importance sampling estimates, which are unbiased,
# overlap. The asymptotic approximation seems a bit off. It can be worthwhile
# to try different values of n_iterations and K.

# When we are happy, we can provide the coefficient vector in the
# logz_estimate argument to compute_mallows
# Say we choose to use the importance sampling estimate with 1e4 Monte Carlo samples:
model_fit <- compute_mallows(
  setup_rank_data(potato_visual),
  model = set_model_options(metric = "spearman"),
  logz_estimate = fit2)

}
\references{
\insertAllCited{}
}
\seealso{
Other preprocessing: 
\code{\link{get_transitive_closure}()},
\code{\link{set_compute_options}()},
\code{\link{set_initial_values}()},
\code{\link{set_model_options}()},
\code{\link{set_priors}()},
\code{\link{setup_rank_data}()}
}
\concept{preprocessing}
