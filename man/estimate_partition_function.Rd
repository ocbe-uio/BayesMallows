% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate_partition_function.R
\name{estimate_partition_function}
\alias{estimate_partition_function}
\title{Estimate Partition Function}
\usage{
estimate_partition_function(
  method = c("importance_sampling", "asymptotic"),
  alpha_vector,
  n_items,
  metric,
  n_iterations,
  K = 20,
  cl = NULL
)
}
\arguments{
\item{method}{Character string specifying the method to use in order to
estimate the logarithm of the partition function. Available options are
\code{"importance_sampling"} and \code{"asymptotic"}.}

\item{alpha_vector}{Numeric vector of \eqn{\alpha} values over which to
compute the importance sampling estimate.}

\item{n_items}{Integer specifying the number of items.}

\item{metric}{Character string specifying the distance measure to use.
Available options are \code{"footrule"} and \code{"spearman"} when \code{method = "asymptotic"} and in addition \code{"cayley"}, \code{"hamming"}, \code{"kendall"}, and
\code{"ulam"} when \code{method = "importance_sampling"}.}

\item{n_iterations}{Integer specifying the number of iterations to use. When
\code{method = "importance_sampling"}, this is the number of Monte Carlo samples
to generate. When \code{method = "asymptotic"}, on the other hand, it represents
the number of iterations of the IPFP algorithm.}

\item{K}{Integer specifying the parameter \eqn{K} in the asymptotic
approximation of the partition function. Only used when \code{method = "asymptotic"}. Defaults to 20.}

\item{cl}{Optional computing cluster used for parallelization, returned from
\code{\link[parallel:makeCluster]{parallel::makeCluster()}}. Defaults to \code{NULL}. Only used when \code{method = "importance_sampling"}.}
}
\value{
A vector of length \code{degree} which can be supplied to the
\code{logz_estimate} argument of \code{\link[=compute_mallows]{compute_mallows()}}.
}
\description{
Estimate the logarithm of the partition function of the Mallows rank model.
Choose between the importance sampling algorithm described in
\insertCite{vitelli2018}{BayesMallows} and the IPFP algorithm for computing
an asymptotic approximation described in
\insertCite{mukherjee2016}{BayesMallows}. Note that exact partition functions
can be computed efficiently for Cayley, Hamming and Kendall distances with
any number of items, for footrule distances with up to 50 items, Spearman
distance with up to 20 items, and Ulam distance with up to 60 items. This
function is thus intended for the complement of these cases. See
\code{\link[=get_cardinalities]{get_cardinalities()}} for details.
}
\examples{
# IMPORTANCE SAMPLING
# Let us estimate logZ(alpha) for 20 items with Spearman distance
# We create a grid of alpha values from 0 to 10
alpha_vector <- seq(from = 0, to = 10, by = 0.5)
n_items <- 20
metric <- "spearman"

# We start with 1e3 Monte Carlo samples
fit1 <- estimate_partition_function(method = "importance_sampling",
                                      alpha_vector = alpha_vector,
                                      n_items = n_items, metric = metric,
                                      n_iterations = 1e3)
# A vector of polynomial regression coefficients is returned
fit1

# Now let us recompute with 2e3 Monte Carlo samples
fit2 <- estimate_partition_function(method = "importance_sampling",
                                    alpha_vector = alpha_vector,
                                    n_items = n_items, metric = metric,
                                    n_iterations = 2e3)

# ASYMPTOTIC APPROXIMATION
# We can also compute an estimate using the asymptotic approximation
fit3 <- estimate_partition_function(method = "asymptotic",
                                    alpha_vector = alpha_vector,
                                    n_items = n_items, metric = metric,
                                    n_iterations = 50)

# We write a little function for storing the estimates in a dataframe
powers <- seq(from = 0, by = 1, length.out = length(fit1))

compute_fit <- function(fit, type){
  do.call(rbind, lapply(alpha_vector, function(alpha){
    data.frame(
      type = type,
      alpha = alpha,
      logz_estimate = sum(alpha^powers * fit)
    )
  }))
}

estimates <- rbind(
  compute_fit(fit1, type = "Importance Sampling 1e3"),
  compute_fit(fit2, type = "Importance Sampling 2e3"),
  compute_fit(fit3, type = "Asymptotic")
  )

# We can now plot the two estimates side-by-side
library(ggplot2)
ggplot(estimates, aes(x = alpha, y = logz_estimate, color = type)) +
  geom_line()
# We see that the two importance sampling estimates, which are unbiased,
# overlap. The asymptotic approximation seems a bit off. It can be worthwhile
# to try different values of n_iterations and K.

# When we are happy, we can provide the coefficient vector in the
# logz_estimate argument to compute_mallows
# Say we choose to use the importance sampling estimate with 1e4 Monte Carlo samples:
model_fit <- compute_mallows(
  setup_rank_data(potato_visual),
  model_options = set_model_options(metric = "spearman"),
  compute_options = set_compute_options(nmc = 200),
  logz_estimate = fit2)

}
\references{
\insertAllCited{}
}
\seealso{
Other partition function: 
\code{\link{footrule_cardinalities}},
\code{\link{get_cardinalities}()}
}
\concept{partition function}
