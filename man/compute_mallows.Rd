% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_mallows.R
\name{compute_mallows}
\alias{compute_mallows}
\title{Preference Learning with the Mallows Rank Model}
\usage{
compute_mallows(rankings = NULL, preferences = NULL,
  metric = "footrule", n_clusters = 1L, nmc = 2000L,
  leap_size = NULL, rho_init = NULL, rho_thinning = 1L,
  alpha_prop_sd = 0.1, alpha_init = 1, alpha_jump = 1L,
  lambda = 0.001, psi = 10L, include_wcd = (n_clusters > 1),
  save_augmented_data = FALSE, aug_thinning = 1L,
  logz_estimate = NULL, verbose = FALSE, skip_postprocessing = FALSE)
}
\arguments{
\item{rankings}{A matrix of ranked items, of size \code{n_assessors x
n_items}. See \code{\link{create_ranking}} if you have an ordered set of
items that need to be converted to rankings. If \code{preferences} is
provided, \code{rankings} is an optional initial value of the rankings,
generated by \code{\link{generate_initial_ranking}}. If \code{rankings} has
column names, these are assumed to be the names of the items.}

\item{preferences}{A dataframe with pairwise comparisons, with 3 columns,
named \code{assessor}, \code{bottom_item}, and \code{top_item}, and one row
for each stated preference. Given a set of pairwise preferences, generate a
transitive closure using \code{\link{generate_transitive_closure}}. This
will give \code{preferences} the class \code{"BayesMallowsTC"}. Otherwise,
\code{compute_mallows} will call \code{\link{generate_transitive_closure}}
will be called on \code{preferences} before computations are done. In the
current version, the pairwise preferences are assumed to be mutually
compatible.}

\item{metric}{A character string specifying the distance metric to use in the
Bayesian Mallows Model. Available options are \code{"footrule"},
\code{"spearman"}, \code{"kendall"}, and \code{"cayley"}. The distance
given by \code{metric} is also used to compute within-cluster distances,
when \code{include_wcd = TRUE}.}

\item{n_clusters}{Integer specifying the number of clusters, i.e., the number
of mixture components to use. Defaults to \code{1L}, which means no
clustering is performed. See \code{\link{compute_mallows_mixtures}} for a
convenience function for computing several models with varying numbers of
mixtures.}

\item{nmc}{Integer specifying the number of iteration of the
Metropolis-Hastings algorithm. Defaults to \code{2000L}. See
\code{\link{assess_convergence}} for tools to check convergence of the
Markov chain.}

\item{leap_size}{Integer specifying the step size of the leap-and-shift
proposal distribution. Defaults to NULL, which means that it is set based
on the data to \code{floor(n_items / 5)}.}

\item{rho_init}{Numeric vector specifying the initial value of the latent
consensus ranking \eqn{\rho}. Defaults to NULL, which means that the
initial value is set randomly. If \code{rho_init} is provided when
\code{n_clusters > 1}, each mixture component \eqn{\rho_{c}} gets the same
initial value.}

\item{rho_thinning}{Integer specifying the thinning of \code{rho} to be
performed in the Metropolis- Hastings algorithm. Defaults to \code{1L}.
\code{compute_mallows} save every \code{rho_thinning}th value of \eqn{\rho}.
See \insertCite{link2011;textual}{BayesMallows} for a discussion of when it
is appropriate to use thinning, and when it is not.}

\item{alpha_prop_sd}{Numeric value specifying the standard deviation of the
lognormal proposal distribution used for \eqn{\alpha} in the
Metropolis-Hastings algorithm. Defaults to \code{0.1}.}

\item{alpha_init}{Numeric value specifying the initial value of the scale
parameter \eqn{\alpha}. Defaults to \code{1}. When \code{n_clusters > 1},
each mixture component \eqn{\alpha_{c}} gets the same initial value.}

\item{alpha_jump}{Integer specifying how many times to sample \eqn{\rho}
between each sampling of \eqn{\alpha}. In other words, how many times to
jump over \eqn{\alpha} while sampling \eqn{\rho}, and possibly other
parameters like augmented ranks \eqn{\tilde{R}} or cluster assignments
\eqn{z}. Setting \code{alpha_jump} to a high number can speed up
computation time, by reducing the number of times the partition function
for the Mallows model needs to be computed.}

\item{lambda}{Numeric value specifying the rate parameter of the exponential
prior distribution of \eqn{\alpha}, \eqn{\pi(\alpha) = \lambda
\exp{(-\lambda \alpha)}}. Defaults to \code{0.1}. When \code{n_cluster >
1}, each mixture component \eqn{\alpha_{c}} has the same prior
distribution.}

\item{psi}{Integer specifying the concentration parameter \eqn{\psi} of the
Dirichlet prior distribution used for the cluster probabilities
\eqn{\tau_{1}, \tau_{2}, \dots, \tau_{C}}, where \eqn{C} is the value of
\code{n_clusters}. Defaults to \code{10L}. When \code{n_clusters = 1}, this
argument is not used.}

\item{include_wcd}{Logical indicating whether to store the within-cluster
distances computing during the Metropolis-Hastings algorithm. Defaults to
\code{TRUE} if \code{n_clusters > 1} and otherwise \code{FALSE}. Setting
\code{include_wcd = TRUE} is useful when deciding the number of mixture
components to include, and is required by \code{\link{plot_elbow}}.}

\item{save_augmented_data}{Logical specifying whether or not to save the
augmented rankings every \code{aug_thinning}th iteration, for the case of
missing data or pairwise preferences. Defaults to \code{FALSE}. Saving
augmented data is useful for predicting the rankings each assessor would
give to the items not yet ranked, and is required by
\code{\link{plot_top_k}}.}

\item{aug_thinning}{Integer specifying the thinning for saving augmented
data. Only used when \code{save_augmented_data = TRUE}. Defaults to 1L.}

\item{logz_estimate}{Estimate of the partition function, computed with
\code{\link{estimate_partition_function}}.}

\item{verbose}{Logical specifying whether to print out the progress of the
Metropolis-Hastings algorithm. If \code{TRUE}, a notification is printed
every 1000th iteration.}

\item{skip_postprocessing}{Logical specifying whether to skip the postprocessing
of the output of the Metropolis-Hastings algorithm. This can be useful for
very large datasets, which cause the postprocessing to crash. Note that when
\code{skip_postprocessing=TRUE}, the functions for studying the posterior
distributions will not work unless the internal function
\code{\link{tidy_mcmc}} has been run.}
}
\value{
A list of class BayesMallows.
}
\description{
Compute the posterior distributions of the parameters of the Bayesian Mallows
Rank Model \insertCite{mallows1957,vitelli2018}{BayesMallows}, given rankings
or preferences stated by a set of assessors. \code{compute_mallows} always
returns posterior distributions of the latent consensus ranking \eqn{\rho}
and the scale parameter \eqn{\alpha}. Several distance measures are
supported, and the preferences can take the form of complete or incomplete
rankings, as well as pairwise preferences. \code{compute_mallows} can also
compute mixtures of Mallows models, for clustering of assessors with similar
preferences.
}
\examples{
# ANALYSIS OF COMPLETE RANKINGS
# The example datasets potato_visual and potato_weighing contain complete
# rankings of 20 items, by 12 assessors. We first analyse these using the Mallows
# model:
model_fit <- compute_mallows(potato_visual)

# We study the trace plot of the parameters
assess_convergence(model_fit, type = "alpha")
\dontrun{assess_convergence(model_fit, type = "rho")}

# Based on these plots, we set burnin = 1000.
model_fit$burnin <- 1000
# Next, we use the generic plot function to study the posterior distributions
# of alpha and rho
plot(model_fit, type = "alpha")
\dontrun{plot(model_fit, type = "rho", items = 10:15)}

# We can also compute the CP consensus posterior ranking
compute_cp_consensus(model_fit)

# And we can compute the posterior intervals:
# First we compute the interval for alpha
compute_posterior_intervals(model_fit, parameter = "alpha")
# Then we compute the interval for all the items
\dontrun{compute_posterior_intervals(model_fit, parameter = "rho")}

# ANALYSIS OF PAIRWISE PREFERENCES
\dontrun{
  # The example dataset beach_preferences contains pairwise
  # preferences between beaches stated by 60 assessors. There
  # is a total of 15 beaches in the dataset.
  # In order to use it, we first generate all the orderings
  # implied by the pairwise preferences.
  beach_tc <- generate_transitive_closure(beach_preferences)
  # We also generate an inital rankings
  beach_rankings <- generate_initial_ranking(beach_tc, n_items = 15)
  # We then run the Bayesian Mallows rank model
  # We save the augmented data for diagnostics purposes.
  model_fit <- compute_mallows(rankings = beach_rankings,
                               preferences = beach_tc,
                               save_augmented_data = TRUE,
                               verbose = TRUE)
  # We can assess the convergence of the scale parameter
  assess_convergence(model_fit)
  # We can assess the convergence of latent rankings. Here we
  # show beaches 1-5.
  assess_convergence(model_fit, type = "rho", items = 1:5)
  # We can also look at the convergence of the augmented rankings for
  # each assessor.
  assess_convergence(model_fit, type = "Rtilde",
                     items = c(2, 4), assessors = c(1, 2))
  # Notice how, for assessor 1, the lines cross each other, while
  # beach 2 consistently has a higher rank value (lower preference) for
  # assessor 2. We can see why by looking at the implied orderings in
  # beach_tc
  library(dplyr)
  beach_tc \%>\%
    filter(assessor \%in\% c(1, 2),
           bottom_item \%in\% c(2, 4) & top_item \%in\% c(2, 4))
  # Assessor 1 has no implied ordering between beach 2 and beach 4,
  # while assessor 2 has the implied ordering that beach 4 is preferred
  # to beach 2. This is reflected in the trace plots.
}

# CLUSTERING OF ASSESSORS WITH SIMILAR PREFERENCES
\dontrun{
  # The example dataset sushi_rankings contains 5000 complete
  # rankings of 10 types of sushi
  # We start with computing a 3-cluster solution
  model_fit <- compute_mallows(sushi_rankings, n_clusters = 3,
                               nmc = 10000, verbose = TRUE)
  # We then assess convergence of the scale parameter alpha
  assess_convergence(model_fit)
  # Next, we assess convergence of the cluster probabilities
  assess_convergence(model_fit, type = "cluster_probs")
  # Based on this, we set burnin = 1000
  # We now plot the posterior density of the scale parameters alpha in
  # each mixture:
  model_fit$burnin <- 1000
  plot(model_fit, type = "alpha")
  # We can also compute the posterior density of the cluster probabilities
  plot(model_fit, type = "cluster_probs")
  # We can also plot the posterior cluster assignment. In this case,
  # the assessors are sorted according to their maximum a posteriori cluster estimate.
  plot(model_fit, type = "cluster_assignment")
  # We can also assign each assessor to a cluster
  cluster_assignments <- assign_cluster(model_fit, soft = FALSE)
  }

# DETERMINING THE NUMBER OF CLUSTERS
\dontrun{
  # Continuing with the sushi data, we can determine the number of cluster
  # Let us look at any number of clusters from 1 to 10
  # We use the convenience function compute_mallows_mixtures
  n_clusters <- seq(from = 1, to = 10)
  models <- compute_mallows_mixtures(n_clusters = n_clusters, rankings = sushi_rankings,
                                     nmc = 6000, alpha_jump = 10)
  # models is a list in which each element is an object of class BayesMallows,
  # returned from compute_mallows
  # We can create an elbow plot
  plot_elbow(models, burnin = 1000)
  # We then select the number of cluster at a point where this plot has
  # an "elbow", e.g., at 6 clusters.
}

}
\references{
\insertAllCited{}
}
\seealso{
\code{\link{compute_mallows_mixtures}} for a function that computes
  separate Mallows models for varying number of clusters.
}
